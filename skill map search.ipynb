{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills map searcher\n",
    "Search related chapter base on text entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from xlsx file. I loaded xlsx file and split it into inputs, labels. Finally, I also split inputs to generate more training datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 1093)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from xlsx file\n",
    "wb = load_workbook('skill_map_data.xlsx')\n",
    "##  print(wb.get_sheet_names())\n",
    "ws = wb.get_sheet_by_name('raw data - Chapter and Text')\n",
    "\n",
    "raw_data = []\n",
    "for row in ws.iter_rows():\n",
    "    raw_data_row = {\n",
    "        \"week_day\" : row[0].value,\n",
    "        \"chapter\" : row[1].value,\n",
    "        \"lesson\" : row[2].value,\n",
    "        \"section\" : row[3].value,\n",
    "        \"text\" : row[4].value\n",
    "        }\n",
    "    raw_data.append(raw_data_row)\n",
    "\n",
    "raw_data = raw_data[2:] # remove table name and header\n",
    "assert(len(raw_data) < 100) # normally we don't have 100+ sections\n",
    "\n",
    "# Split raw_data into inputs and labels\n",
    "inputs = [row['text'] for row in raw_data]\n",
    "assert(len(raw_data) == len(inputs))\n",
    "\n",
    "## concated week_day, chapter, lesson, section into one label\n",
    "labels = [' '.join([\n",
    "            str(row['week_day']), ' ',\n",
    "            row['chapter'], ' ',\n",
    "            row['lesson'], ' ',\n",
    "            row['section']\n",
    "        ]) for row in raw_data]\n",
    "\n",
    "assert(len(raw_data) == len(labels))\n",
    "\n",
    "# Split inputs to generate more training datas\n",
    "seq_len = 200 # length for split long text\n",
    "seq_inputs = []\n",
    "seq_labels = []\n",
    "count = 0\n",
    "for i, input in enumerate(inputs):\n",
    "    if len(input) > seq_len:\n",
    "        for j in range(int(len(input)/seq_len + 0.5)):\n",
    "            seq_input = input[j*seq_len:(j+1)*seq_len]\n",
    "            seq_inputs.append(seq_input)\n",
    "            seq_labels.append(labels[i])\n",
    "            count += 1\n",
    "    else:\n",
    "        seq_inputs.append(input)\n",
    "        seq_labels.append(labels[i])\n",
    "\n",
    "len(seq_inputs), len(seq_labels)\n",
    "# seq_labels[998], seq_inputs[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = seq_inputs\n",
    "labels = seq_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the concepts behind recurrent networks and see how a character-wise recurrent network is implemented in\",\n",
       " \"One of the coolest deep learning results from last year was the Google Translate update. They've been a leader in machine learning for a while, but implementing a deep neural network for translation b\",\n",
       " 'rought the service nearly to the level of human translators. With translation, the correct word to use depends on the context, and all the other words in the sentence, and even in the paragraph. Much ',\n",
       " \"of the information contained in language is in the sequence of the words. So far, we've been working with what are called feed forward networks. The input is fed into the network and it propagates for\",\n",
       " \"ward through the hidden layers to the output layer. In feed forward networks, there is no sense of order in the inputs. Here's a simple idea then, let's build order into our network. First, we'll spli\",\n",
       " \"t up the data into parts. The text, this can be words or individual characters like I've done here with the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's \",\n",
       " \"great. Our goal here is to predict the next character in the word steep. To keep it simple, assume our entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'l\",\n",
       " 'l pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, does',\n",
       " \"n't have enough information to determine which character to predict. To solve this problem, we'll need to include information about the sequence of characters. We can do this by routing the hidden lay\",\n",
       " 'er output from the previous step back into the hidden layer. That box there means value from the previous sequence, or time step. Now, when the network sees an E, it knows it saw an S and a T before, ',\n",
       " 'so the next character should be another E. This architecture is known as a recurrent neural network, or RNN. Now, the total input to the hidden layer is just the sum of the layered combinations from t',\n",
       " 'he input layer and the previous hidden layer, ht minus 1. We can view our recurrent network as one big graph by unrolling it. Now, we have a feed forward network for each character but connected throu',\n",
       " \"gh the hidden layers. Each hidden node receives inputs from an input node and the hidden node from the previous step. To make it a bit more understandable, let's plug in some numbers here. This diagra\",\n",
       " \"m is borrowed from Andrej Karpathy, with a few alterations. Here, we're one hot encoding the input characters. So, 1000 is an S, 0100 is a T, and so on. There are three units in the hidden layer, and \",\n",
       " \"the output layer is showing the logits. You'd pass the logits into a softmax function to get predictions and train with a cross interview loss. This is the basic architecture for our character wise RN\",\n",
       " 'Before we saw that you can include information from a sequence of data using a recurrent connection on the hidden layer. This connection goes through these weights, Whh. If we enroll the network, we s',\n",
       " 'ee that the hidden layer sub t is a function of the previous hidden state multiplied by those weights. And the output from that layer is again multiplied by Whh. For every step you have in the network',\n",
       " \", you're multiplying by the weights again and again. And when you're doing backprop, that's even more multiplications. This leads to a problem where the gradients going through the network either get \",\n",
       " \"really small and vanish or get really large and explode. This happens because if you're multiplying by some number a bunch of times, you're going to get two results, except in a couple special cases. \",\n",
       " \"If that number is less that 1, you'll end up at 0. If it's greater than 1, you'll head towards infinity. This happens to gradients in a normal RNN. They'll either vanish or explode. This makes it diff\",\n",
       " 'icult for RNNs to learn long range interactions. Luckily, there is a solution. We can think of recurrent networks as a bunch of cells with inputs and outputs. Inside the cell you have your network lay',\n",
       " 'ers, such as the sigmoid layer labeled with a sigma here. To solve the problem of the vanishing gradients, we can use more complicated cells called long short-term memory, LSTMs for short. Okay, LSTM ',\n",
       " \"cells are pretty complicated at first glance. But if you break it down into parts, they aren't too difficult to understand. The key addition here is the cell state labeled C, I'll get into this next. \",\n",
       " 'In this cell, there are four network layers shown as yellow boxes. Each of them with their own weights. The layers labeled with sigmas are sigmoids. And tanh is the hyperbolic tangent function. tanh i',\n",
       " 's similar to a sigmoid in that it squashes inputs. But the output is between -1 and 1 instead of 0 and 1. The red circles are point-wise or element-wise operations. That is, they operate on matrices e',\n",
       " 'lement by element. The main improvement here is through the cell state. The cell state goes through the LSTM cell with little interaction, allowing information to flow easily through the cells. The ce',\n",
       " 'll state is modified only through these element-wise operations which function as gates. And the hidden state is now calculated from the cell state, then passed on. The first gate is the forget gate. ',\n",
       " 'The values coming out of the sigmoid layer are between 0 and 1. Then, they are multiplied element-wise with the cell state. So values from this layer close to 0 will shut off certain elements in the c',\n",
       " 'ell state. Effectively forgetting that information going forward. Conversely, values close to 1 will allow information to pass through unchanged. This is helpful, because the network can learn to forg',\n",
       " 'et information that causes incorrect predictions. On the other hand, long range information that is helpful is allowed to flow through freely. The next bit updates the cell state from the input and pr',\n",
       " 'evious hidden state. The tanh layer output is added to the cell state, again, gated by a sigmoid layer. In this way, the cell state can be updated in the step and passed along to the next cell. Here, ',\n",
       " \"the cell state is used to produce the hidden state which is sent to the next hidden cell as well as to higher layers. It's the arrow pointing up here. The cell state is passed through another tanh the\",\n",
       " 'n gated again with another sigmoid layer. All these sigmoid gates let the network learn which information to keep and which information to get rid of. Putting all this together, the LSTM cell consists',\n",
       " ' of a cell state with a bunch of gates used to update it, and leak it out to the hidden state. This is just the basic LSTM. There are multiple variations and a lot of ongoing experimentation into impr',\n",
       " 'oving these. They are also easily stacked into deeper layers. You just send the output from one cell to the input of another. Okay, so now you might be wondering how all of this fixes our gradient pro',\n",
       " 'blem. Since the cell state is allowed to flow through the hidden layers with only this linear sum operation. Gradients can easily move through the network without being diminished. You also get gradie',\n",
       " \"nts added into the network through the LSTM cells but they're just added to the gradients flowing through. I'm not going to get into the math behind all this. I'll link you to some resources such as a\",\n",
       " \" great lecture by Andre Carpathy to help you out there. LSTMs are the basic unit of recurrent networks these days. You don't have to completely understand the inner workings. But knowing how they work\",\n",
       " \"RNN Resources\\nHere are a few great resources for you to learn more about recurrent neural networks. We'll also continue to cover RNNs over the coming weeks.\\n\\t•\\tAndrej Karpathy's lecture on RNNs and LS\",\n",
       " 'TMs from CS231n\\n\\t•\\tA great blog post by Christopher Olah on how LSTMs work.\\n\\t•\\tBuilding an RNN from the ground up, this is a little more advanced, but has an implementation in TensorFlow.',\n",
       " \"Hey, welcome back, so previously, I talked to you about the concepts behind RNNs and LSTMs. But now I'm actually going to walk through an implementation of these things in TensorFlow. So here I'm buil\",\n",
       " \"ding a character-wise RNN. So what that means is that it looks at the characters in text and it can generate new characters. So it generates new texts character by character. And I'm going to train th\",\n",
       " \"is on Anna Karenina, which is one of my favorite books of all time, it's super good. So then the idea is that train it on Anna Karenina, and then it can generate new text from the book. And it's going\",\n",
       " \" to learn names in the structure and all these cool things. So this network is based off a blog post by Andrej Karpathy. You should check out this link here. I'll share this notebook with you so you c\",\n",
       " \"an work through it and also check out these links. So this was a really good blog post of his, he has an implementation that was in Torch and it's on GitHub. When I was building this, I also checked o\",\n",
       " \"ut some other code, which you can find at this link, r2rt, and this dude's GitHub repo. He made a character-wise RNN also. So below here is a diagram that was made by Andrej Karpathy. And it basically\",\n",
       " ' shows the general architecture of what I built here. So the idea is to encode each of your characters with one-hot encoding. So this is your input layer, so you just have your one-hot encoded charact',\n",
       " 'ers. This goes to the hidden layer which is then passed to the next character in the sequence, right. The output of this hidden layer also goes to the output layer where you have your logits. And you ',\n",
       " 'use these logits with Softmax to predict what the next character will be. And then your target characters are just the next character in the sequence, right. So this is the general architecture of thi',\n",
       " \"s network, and you'll see how I implement this in TensorFlow later. Great, so here I'm going to import modules. This is where I'm loading the text. So Anna Karenina is in the public domain, so you jus\",\n",
       " \"t find the text online. Here I'm making the vocab, so this is basically a set of all the characters. And I take those characters and make a dictionary where I can convert the characters to integers. A\",\n",
       " \"nd I have a dictionary that goes backwards from integers to characters. And this is just a giant array of all the characters that we have. So here I'm doing vocab_to_int. So I am making this character\",\n",
       " \" array, so all the characters are converted to integers. So this is the first 100 characters you see. This is the first line of the book. And this is what it looks like when it's converted to integers\",\n",
       " \", cool. So now that we have our data in a form we can use, we need to form batches, right, that we're going to pass into our network. We also wannna split it into a training set and validation set and\",\n",
       " ' also into the input features and the targets that we want. Basically I wrote this function to do just that. So it takes this character vector, this array that I have here, in the batch size and the n',\n",
       " \"umber of steps. So the number of steps is the sequence length of characters that we're going to pass into our network. And so the longer the sequence is, then the further back it can look for correlat\",\n",
       " \"ions between characters. So it's just kind of the more steps you have, the better performance your network will have. But it also makes it longer to train, of course. Okay, so the cool thing about thi\",\n",
       " \"s is that our target's y. It's exactly the same as the input characters, just shifted one over, right. So like in hello, our first character is f and the first target is e. So you can just have your x\",\n",
       " \" as just grab a bunch characters and y is just everything just shifted over by 1. Here what I'm doing is dropping the last few characters to make sure that when I'm going through and I'm making my bat\",\n",
       " \"ches, that all the batches are full. So I'm just kind of leaving off the tail of this character array. Great, so now that I have my x and my y, I need to split these into batches. So what I'm going to\",\n",
       " ' do is I take split(x, batch_size). What this does is it makes a number of different arrays equal to batch size. And then when I stack them, what this does is it makes rows, it stacks them vertically.',\n",
       " ' And so you have rows equal to batch size, and then columns equal to all the rest of the data. So basically what we can do now with this is that we can put a little window on top of this. And the heig',\n",
       " 'ht of this window is going to be the batch size and then the width of the window is going to be the number of steps in the sequence. So you can just move this window along this big 2D array and just k',\n",
       " \"ind of slide it over and get your batches. So here's just splitting out the training and validation sets. So pretty standard, you look at this. If we do a batch size of 10 and our step size are 200, t\",\n",
       " \"hen we see the train x, the shape is 10, so it's 10 rows for the batches, pretty normal. And we can look at all the batches for the first 10 steps, so this would be a window into your data. So if your\",\n",
       " ' sequence steps was doing 10 steps, right, so this would be the first batch. And for the second batch, you would just shift it over 10 and you get the next batch. And this is basically what that get_b',\n",
       " 'atch is, exactly what I just described. Where you are just sliding this window along your data that you see here. [BLANK_AUDIO] So now this is where I actually build the network. So this is all done i',\n",
       " \"n TensorFlow, now I'm going to go through it now. So here just kind of typical, your placeholders for the inputs and the targets. And this keep probability is a placeholder that I'll use for drop out.\",\n",
       " \" So this is how many of the different connections I'll be keeping in the drop out layers. In this network, I am passing in characters, so actually integers, because I converted them to integers in inp\",\n",
       " \"uts and targets. And then here, I'm encoding them to one_hot vectors. And this is the RNN layer, it is actually pretty simple to do this part. So you just say I'm going to make a BasicLSTMCell, and th\",\n",
       " \"e (lstm_size) is the number of hidden units in these cells. And then you just wrap it in Dropout, so it just does Dropout for you, basically. And then it's super easy to stack these up. You can do Mul\",\n",
       " 'tiRNNCells. So this automatically routes the output from one layer of an LCM to the next layer of the LCMs. So you can just stack however many tall, how many deep LCM cells you want. So drop is from t',\n",
       " \"he DropoutWrapper and then you just multiply it by the number of layers, and so you get that. Then here we are setting the cell state, so that we're just setting it to zero, so it's the initial state.\",\n",
       " \" So it just starts all zeros and this is what we're going to be training as we go through. All right, now this part runs the data through the RNN. So what we're doing here is we're splitting. So first\",\n",
       " \" we're splitting tf.split, our x, our input data into number of steps. So what this is doing, it's making just a column of sequences, right? So it's t = 1, i = 1, so this is your first time step. You \",\n",
       " \"just get a vector, just a column vector that is the length of your batches. So you're just kind of the stepping through one time point at a time, and you're making this list. And so every element of t\",\n",
       " \"his list is one step in your sequence. And then you're passing each of this steps in the sequence through your recurrent neural networks, which you have defined here. And all that squeeze does is it j\",\n",
       " 'ust removes any dimensions that are only of size 1. So basically, what we have here is just a list of column vectors, and the length of this column is the size of the batch. So in this line is actuall',\n",
       " \"y where we run through our network. So we're passing the inputs into our network, I'm just taking cell, and setting the initial state. And what this does automatically for us is that it passes the sta\",\n",
       " \"te from each sequence step to the next one. So just does all that for us, it's really convenient. And at the end, we get out the final state and we get out the final outputs. And the outputs are the o\",\n",
       " \"utputs of the hidden layer for each step in the sequence, and this is just a big list. So here, we'll keep the final state for later, okay. So like I was saying, the outputs is a big list of the outpu\",\n",
       " \"t of each hidden layer at each step. So to make this easier, we're just going to concatenate it into one big list, just get all these outputs and mash them together into one array. Then what I do here\",\n",
       " ' is I reshape it so that each row is one output, so lstm_size, so this the size of the number of hidden layers in our cells. So I can reshape it into the size of the lstms, and each row is a different',\n",
       " ' output, so every step has an output. And the number of columns, the width of this output array is the number of hidden units in the lstm cells. So now that I have all that done, I can send all of the',\n",
       " \" hidden outputs through some weights and biases and do softmax, basically. So here I'm just calculating my logits. So you just do your matrix multiplication of the output and the softmax weights and a\",\n",
       " \"dd the softmax biases. And it pass through softmax to the predictions. So here I'm calculating the loss now. So, again, I look to reshaping my targets so that they match the same kind of form as our l\",\n",
       " \"ogits. because remember logits, each row of this array is one output for one step. So we're going to compare each output of a step to the target at that step. And that's exactly what we're doing here.\",\n",
       " \" So we're doing softmax_cross_entropy_with_logits. So our logits and our labels here calculate the cost. So we just reduce the means, so just calculate the mean for the losses for each of the steps in\",\n",
       " ' the sequence. So here you might remember me talking about how gradients can explode in a recurrent neural network. The easiest way to fix this is that you just clip the gradients. So if the gradient ',\n",
       " \"is above 5, you just set it to 5, and that's kind of what this block of code does. It's a pretty simple thing to do, that's nice. And here I'm just exporting the nodes that I want, because the whole t\",\n",
       " \"hing is just a function right, so I just run the function. Then I export the nodes that I want, so that I can use them later during training. Right, and here are the hyperparameters I'm setting. So ba\",\n",
       " \"tch_size and number of steps in the sequence that I'm looking at, the lstm_size. So this is the number of hidden units in each lstm layer, and this is the numbers of layers I'm using. So then the more\",\n",
       " \" data you have, the more parameters you need. And so you cam up your parameters by including more layers and by increasing the size of your hidden layers. Okay, and now we're training. So the idea her\",\n",
       " \"e is that we have all of our epochs, so I'm just going to use 20, and this is just for saving. Don't worry about it too much. So here this is pretty typical, we just get our batches of data, create yo\",\n",
       " \"ur feed dictionary. So here, my inputs are x, my targets are y. And for training the dropout keep probability, I'm going to keep it at 0.5. new_state is being initialized, so I'm taking the initial_st\",\n",
       " \"ate and I'm passing it as the initial_state for the rnn. So what's going to happen is that for every batch, I'm going to get the state after a batch, and I'm just going to route it back into the next \",\n",
       " \"batch. So in this way, you can keep your state going through each batch as you're going through the training. Session run, so getting the cost, getting the final_state, which is I'm going to use the n\",\n",
       " 'ew_state for the initial_state for the next batch. And then the optimizer, and so this is what is actually reducing the loss for the network and changing the parameters. And then down here is just the',\n",
       " \" validation part, pretty simple. So again, I'm going to provide this notebook for you so you can look through it and see how it all works, and just kind of work it out for yourself. So this is some of\",\n",
       " \" the output for how it trains. And I'm running this on a NVIDIA 1070, and I get 0.13 seconds per batch at these settings. Okay, so I trained it and now we can actually see what the results are, and th\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in inputs if c not in punctuation])\n",
    "\n",
    "all_text = ' '.join(inputs)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40391, 218416, 1093)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(all_text), len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the concepts behind recurrent networks and see how a character-wise recurrent network is implemented in One of the coolest deep learning results from last year was the Google Translate update. They've been a leader in machine learning for a while, but implementing a deep neural network for translation b rought the service nearly to the level of human translators. With translation, the correct word to use depends on the context, and all the other words in the sentence, and even in the paragraph. Much  of the information contained in language is in the sequence of the words. So far, we've been working with what are called feed forward networks. The input is fed into the network and it propagates for ward through the hidden layers to the output layer. In feed forward networks, there is no sense of order in the inputs. Here's a simple idea then, let's build order into our network. First, we'll spli t up the data into parts. The text, this can be words or individual characters like I've done here with the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's  great. Our goal here is to predict the next character in the word steep. To keep it simple, assume our entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'l l pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, does n't have enough information to determine which character to predict. To solve this problem, we'll need to include information about the sequence of characters. We can do this by routing the hidden lay er output from the previous step back into the hidden layer. That box there means value from the previous sequence, or time step. Now, when the network sees an E, it knows it saw an S and a T\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recurrent',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'data.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'lesson,',\n",
       " \"you'll\",\n",
       " 'learn',\n",
       " 'the',\n",
       " 'concepts',\n",
       " 'behind',\n",
       " 'recurrent',\n",
       " 'networks',\n",
       " 'and',\n",
       " 'see',\n",
       " 'how',\n",
       " 'a',\n",
       " 'character-wise',\n",
       " 'recurrent',\n",
       " 'network',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coolest',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'results',\n",
       " 'from',\n",
       " 'last',\n",
       " 'year',\n",
       " 'was',\n",
       " 'the',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'update.',\n",
       " \"They've\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while,',\n",
       " 'but',\n",
       " 'implementing',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'for',\n",
       " 'translation',\n",
       " 'b',\n",
       " 'rought',\n",
       " 'the',\n",
       " 'service',\n",
       " 'nearly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'level',\n",
       " 'of',\n",
       " 'human',\n",
       " 'translators.',\n",
       " 'With',\n",
       " 'translation,',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'word',\n",
       " 'to',\n",
       " 'use',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'the',\n",
       " 'context,',\n",
       " 'and',\n",
       " 'all',\n",
       " 'the',\n",
       " 'other',\n",
       " 'words',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sentence,',\n",
       " 'and',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paragraph.',\n",
       " 'Much']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "inputs_ints = []\n",
    "for each in inputs:\n",
    "    inputs_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_set = set(labels)\n",
    "labels_array = [i for i, label in enumerate(labels_set)]\n",
    "labels_np = np.array(labels_array)\n",
    "labels_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, create an array features that contains the data we'll pass to the network. The data should come from review_ints, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128]. For reviews longer than 200, use on the first 200 words as the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out that inputs with 0 length\n",
    "inputs_ints = [each for each in inputs_ints if len(each) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(inputs), seq_len), dtype=int)\n",
    "for i, row in enumerate(inputs_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1031,   10, 1484],\n",
       "       [ 561,    3,    1, ...,    3, 2349, 1611],\n",
       "       [1178,   12,  564, ...,    3,  518,   26],\n",
       "       ..., \n",
       "       [  84,   11,    7, ...,  341,   30,    7],\n",
       "       [4158, 3044,    1, ...,  890,  950,   43],\n",
       "       [ 217, 5199,   90, ...,    1,   99, 1459]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(36, 200) \n",
      "Validation set: \t(5, 200) \n",
      "Test set: \t\t(5, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac= 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
