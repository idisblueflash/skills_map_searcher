{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills map searcher\n",
    "Search related chapter base on text entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from openpyxl import load_workbook\n",
    "from collections import namedtuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from xlsx file. I loaded xlsx file and split it into inputs, labels. Finally, I also split inputs to generate more training datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2188, 2188)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from xlsx file\n",
    "wb = load_workbook('skill_map_data.xlsx')\n",
    "##  print(wb.get_sheet_names())\n",
    "ws = wb.get_sheet_by_name('raw data - Chapter and Text')\n",
    "\n",
    "raw_data = []\n",
    "for row in ws.iter_rows():\n",
    "    raw_data_row = {\n",
    "        \"week_day\" : row[0].value,\n",
    "        \"chapter\" : row[1].value,\n",
    "        \"lesson\" : row[2].value,\n",
    "        \"section\" : row[3].value,\n",
    "        \"text\" : row[4].value\n",
    "        }\n",
    "    raw_data.append(raw_data_row)\n",
    "\n",
    "raw_data = raw_data[2:] # remove table name and header\n",
    "assert(len(raw_data) < 100) # normally we don't have 100+ sections\n",
    "\n",
    "# Split raw_data into inputs and labels\n",
    "inputs = [row['text'] for row in raw_data]\n",
    "assert(len(raw_data) == len(inputs))\n",
    "\n",
    "## concated week_day, chapter, lesson, section into one label\n",
    "labels = [' '.join([\n",
    "            str(row['week_day']), ' ',\n",
    "            row['chapter'], ' ',\n",
    "            row['lesson'], ' ',\n",
    "            row['section']\n",
    "        ]) for row in raw_data]\n",
    "\n",
    "assert(len(raw_data) == len(labels))\n",
    "\n",
    "# Split inputs to generate more training datas\n",
    "seq_len = 100 # length for split long text\n",
    "seq_inputs = []\n",
    "seq_labels = []\n",
    "count = 0\n",
    "for i, input in enumerate(inputs):\n",
    "    if len(input) > seq_len:\n",
    "        for j in range(int(len(input)/seq_len + 0.5)):\n",
    "            seq_input = input[j*seq_len:(j+1)*seq_len]\n",
    "            seq_inputs.append(seq_input)\n",
    "            seq_labels.append(labels[i])\n",
    "            count += 1\n",
    "    else:\n",
    "        seq_inputs.append(input)\n",
    "        seq_labels.append(labels[i])\n",
    "\n",
    "len(seq_inputs), len(seq_labels)\n",
    "# seq_labels[998], seq_inputs[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = seq_inputs\n",
    "labels = seq_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the\",\n",
       " ' concepts behind recurrent networks and see how a character-wise recurrent network is implemented in',\n",
       " \"One of the coolest deep learning results from last year was the Google Translate update. They've bee\",\n",
       " 'n a leader in machine learning for a while, but implementing a deep neural network for translation b',\n",
       " 'rought the service nearly to the level of human translators. With translation, the correct word to u',\n",
       " 'se depends on the context, and all the other words in the sentence, and even in the paragraph. Much ',\n",
       " \"of the information contained in language is in the sequence of the words. So far, we've been working\",\n",
       " ' with what are called feed forward networks. The input is fed into the network and it propagates for',\n",
       " 'ward through the hidden layers to the output layer. In feed forward networks, there is no sense of o',\n",
       " \"rder in the inputs. Here's a simple idea then, let's build order into our network. First, we'll spli\",\n",
       " \"t up the data into parts. The text, this can be words or individual characters like I've done here w\",\n",
       " \"ith the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's \",\n",
       " 'great. Our goal here is to predict the next character in the word steep. To keep it simple, assume o',\n",
       " \"ur entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'l\",\n",
       " 'l pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we ',\n",
       " 'pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, does',\n",
       " \"n't have enough information to determine which character to predict. To solve this problem, we'll ne\",\n",
       " 'ed to include information about the sequence of characters. We can do this by routing the hidden lay',\n",
       " 'er output from the previous step back into the hidden layer. That box there means value from the pre',\n",
       " 'vious sequence, or time step. Now, when the network sees an E, it knows it saw an S and a T before, ',\n",
       " 'so the next character should be another E. This architecture is known as a recurrent neural network,',\n",
       " ' or RNN. Now, the total input to the hidden layer is just the sum of the layered combinations from t',\n",
       " 'he input layer and the previous hidden layer, ht minus 1. We can view our recurrent network as one b',\n",
       " 'ig graph by unrolling it. Now, we have a feed forward network for each character but connected throu',\n",
       " 'gh the hidden layers. Each hidden node receives inputs from an input node and the hidden node from t',\n",
       " \"he previous step. To make it a bit more understandable, let's plug in some numbers here. This diagra\",\n",
       " \"m is borrowed from Andrej Karpathy, with a few alterations. Here, we're one hot encoding the input c\",\n",
       " 'haracters. So, 1000 is an S, 0100 is a T, and so on. There are three units in the hidden layer, and ',\n",
       " \"the output layer is showing the logits. You'd pass the logits into a softmax function to get predict\",\n",
       " 'ions and train with a cross interview loss. This is the basic architecture for our character wise RN',\n",
       " \"N, but there are some improvements we'll need to make, which I'll talk about next.\",\n",
       " 'Before we saw that you can include information from a sequence of data using a recurrent connection ',\n",
       " 'on the hidden layer. This connection goes through these weights, Whh. If we enroll the network, we s',\n",
       " 'ee that the hidden layer sub t is a function of the previous hidden state multiplied by those weight',\n",
       " 's. And the output from that layer is again multiplied by Whh. For every step you have in the network',\n",
       " \", you're multiplying by the weights again and again. And when you're doing backprop, that's even mor\",\n",
       " 'e multiplications. This leads to a problem where the gradients going through the network either get ',\n",
       " \"really small and vanish or get really large and explode. This happens because if you're multiplying \",\n",
       " \"by some number a bunch of times, you're going to get two results, except in a couple special cases. \",\n",
       " \"If that number is less that 1, you'll end up at 0. If it's greater than 1, you'll head towards infin\",\n",
       " \"ity. This happens to gradients in a normal RNN. They'll either vanish or explode. This makes it diff\",\n",
       " 'icult for RNNs to learn long range interactions. Luckily, there is a solution. We can think of recur',\n",
       " 'rent networks as a bunch of cells with inputs and outputs. Inside the cell you have your network lay',\n",
       " 'ers, such as the sigmoid layer labeled with a sigma here. To solve the problem of the vanishing grad',\n",
       " 'ients, we can use more complicated cells called long short-term memory, LSTMs for short. Okay, LSTM ',\n",
       " \"cells are pretty complicated at first glance. But if you break it down into parts, they aren't too d\",\n",
       " \"ifficult to understand. The key addition here is the cell state labeled C, I'll get into this next. \",\n",
       " 'In this cell, there are four network layers shown as yellow boxes. Each of them with their own weigh',\n",
       " 'ts. The layers labeled with sigmas are sigmoids. And tanh is the hyperbolic tangent function. tanh i',\n",
       " 's similar to a sigmoid in that it squashes inputs. But the output is between -1 and 1 instead of 0 a',\n",
       " 'nd 1. The red circles are point-wise or element-wise operations. That is, they operate on matrices e',\n",
       " 'lement by element. The main improvement here is through the cell state. The cell state goes through ',\n",
       " 'the LSTM cell with little interaction, allowing information to flow easily through the cells. The ce',\n",
       " 'll state is modified only through these element-wise operations which function as gates. And the hid',\n",
       " 'den state is now calculated from the cell state, then passed on. The first gate is the forget gate. ',\n",
       " 'The values coming out of the sigmoid layer are between 0 and 1. Then, they are multiplied element-wi',\n",
       " 'se with the cell state. So values from this layer close to 0 will shut off certain elements in the c',\n",
       " 'ell state. Effectively forgetting that information going forward. Conversely, values close to 1 will',\n",
       " ' allow information to pass through unchanged. This is helpful, because the network can learn to forg',\n",
       " 'et information that causes incorrect predictions. On the other hand, long range information that is ',\n",
       " 'helpful is allowed to flow through freely. The next bit updates the cell state from the input and pr',\n",
       " 'evious hidden state. The tanh layer output is added to the cell state, again, gated by a sigmoid lay',\n",
       " 'er. In this way, the cell state can be updated in the step and passed along to the next cell. Here, ',\n",
       " 'the cell state is used to produce the hidden state which is sent to the next hidden cell as well as ',\n",
       " \"to higher layers. It's the arrow pointing up here. The cell state is passed through another tanh the\",\n",
       " 'n gated again with another sigmoid layer. All these sigmoid gates let the network learn which inform',\n",
       " 'ation to keep and which information to get rid of. Putting all this together, the LSTM cell consists',\n",
       " ' of a cell state with a bunch of gates used to update it, and leak it out to the hidden state. This ',\n",
       " 'is just the basic LSTM. There are multiple variations and a lot of ongoing experimentation into impr',\n",
       " 'oving these. They are also easily stacked into deeper layers. You just send the output from one cell',\n",
       " ' to the input of another. Okay, so now you might be wondering how all of this fixes our gradient pro',\n",
       " 'blem. Since the cell state is allowed to flow through the hidden layers with only this linear sum op',\n",
       " 'eration. Gradients can easily move through the network without being diminished. You also get gradie',\n",
       " \"nts added into the network through the LSTM cells but they're just added to the gradients flowing th\",\n",
       " \"rough. I'm not going to get into the math behind all this. I'll link you to some resources such as a\",\n",
       " ' great lecture by Andre Carpathy to help you out there. LSTMs are the basic unit of recurrent networ',\n",
       " \"ks these days. You don't have to completely understand the inner workings. But knowing how they work\",\n",
       " 'RNN Resources\\nHere are a few great resources for you to learn more about recurrent neural networks. ',\n",
       " \"We'll also continue to cover RNNs over the coming weeks.\\n\\t•\\tAndrej Karpathy's lecture on RNNs and LS\",\n",
       " 'TMs from CS231n\\n\\t•\\tA great blog post by Christopher Olah on how LSTMs work.\\n\\t•\\tBuilding an RNN from ',\n",
       " 'the ground up, this is a little more advanced, but has an implementation in TensorFlow.',\n",
       " 'Hey, welcome back, so previously, I talked to you about the concepts behind RNNs and LSTMs. But now ',\n",
       " \"I'm actually going to walk through an implementation of these things in TensorFlow. So here I'm buil\",\n",
       " 'ding a character-wise RNN. So what that means is that it looks at the characters in text and it can ',\n",
       " \"generate new characters. So it generates new texts character by character. And I'm going to train th\",\n",
       " \"is on Anna Karenina, which is one of my favorite books of all time, it's super good. So then the ide\",\n",
       " \"a is that train it on Anna Karenina, and then it can generate new text from the book. And it's going\",\n",
       " ' to learn names in the structure and all these cool things. So this network is based off a blog post',\n",
       " \" by Andrej Karpathy. You should check out this link here. I'll share this notebook with you so you c\",\n",
       " 'an work through it and also check out these links. So this was a really good blog post of his, he ha',\n",
       " \"s an implementation that was in Torch and it's on GitHub. When I was building this, I also checked o\",\n",
       " \"ut some other code, which you can find at this link, r2rt, and this dude's GitHub repo. He made a ch\",\n",
       " 'aracter-wise RNN also. So below here is a diagram that was made by Andrej Karpathy. And it basically',\n",
       " ' shows the general architecture of what I built here. So the idea is to encode each of your characte',\n",
       " 'rs with one-hot encoding. So this is your input layer, so you just have your one-hot encoded charact',\n",
       " 'ers. This goes to the hidden layer which is then passed to the next character in the sequence, right',\n",
       " '. The output of this hidden layer also goes to the output layer where you have your logits. And you ',\n",
       " 'use these logits with Softmax to predict what the next character will be. And then your target chara',\n",
       " 'cters are just the next character in the sequence, right. So this is the general architecture of thi',\n",
       " \"s network, and you'll see how I implement this in TensorFlow later. Great, so here I'm going to impo\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in inputs if c not in punctuation])\n",
    "\n",
    "all_text = ' '.join(inputs)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41233, 220438, 2188)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(all_text), len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the  concepts behind recurrent networks and see how a character-wise recurrent network is implemented in One of the coolest deep learning results from last year was the Google Translate update. They've bee n a leader in machine learning for a while, but implementing a deep neural network for translation b rought the service nearly to the level of human translators. With translation, the correct word to u se depends on the context, and all the other words in the sentence, and even in the paragraph. Much  of the information contained in language is in the sequence of the words. So far, we've been working  with what are called feed forward networks. The input is fed into the network and it propagates for ward through the hidden layers to the output layer. In feed forward networks, there is no sense of o rder in the inputs. Here's a simple idea then, let's build order into our network. First, we'll spli t up the data into parts. The text, this can be words or individual characters like I've done here w ith the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's  great. Our goal here is to predict the next character in the word steep. To keep it simple, assume o ur entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'l l pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we  pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, does n't have enough information to determine which character to predict. To solve this problem, we'll ne ed to include information about the sequence of characters. We can do this by routing the hidden lay er output from the previous step back into the hidden layer. That box there means value from the pre vious sequence, or time step. Now, when the network sees an E, it knows it saw an\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recurrent',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'data.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'lesson,',\n",
       " \"you'll\",\n",
       " 'learn',\n",
       " 'the',\n",
       " 'concepts',\n",
       " 'behind',\n",
       " 'recurrent',\n",
       " 'networks',\n",
       " 'and',\n",
       " 'see',\n",
       " 'how',\n",
       " 'a',\n",
       " 'character-wise',\n",
       " 'recurrent',\n",
       " 'network',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coolest',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'results',\n",
       " 'from',\n",
       " 'last',\n",
       " 'year',\n",
       " 'was',\n",
       " 'the',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'update.',\n",
       " \"They've\",\n",
       " 'bee',\n",
       " 'n',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while,',\n",
       " 'but',\n",
       " 'implementing',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'for',\n",
       " 'translation',\n",
       " 'b',\n",
       " 'rought',\n",
       " 'the',\n",
       " 'service',\n",
       " 'nearly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'level',\n",
       " 'of',\n",
       " 'human',\n",
       " 'translators.',\n",
       " 'With',\n",
       " 'translation,',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'word',\n",
       " 'to',\n",
       " 'u',\n",
       " 'se',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'the',\n",
       " 'context,',\n",
       " 'and',\n",
       " 'all',\n",
       " 'the',\n",
       " 'other',\n",
       " 'words',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sentence,',\n",
       " 'and',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "inputs_ints = []\n",
    "for each in inputs:\n",
    "    inputs_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_set = set(labels)\n",
    "labels_array = [i for i, label in enumerate(labels_set)]\n",
    "labels_np = np.array(labels_array)\n",
    "assert(len(labels_np) == 46)\n",
    "labels = labels_np\n",
    "labels_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, create an array features that contains the data we'll pass to the network. The data should come from review_ints, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128]. For reviews longer than 200, use on the first 200 words as the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out that inputs with 0 length\n",
    "inputs_ints = [each for each in inputs_ints if len(each) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "features = np.zeros((len(inputs), seq_len), dtype=int)\n",
    "for i, row in enumerate(inputs_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2188, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1750, 100) \n",
      "Validation set: \t(219, 100) \n",
      "Test set: \t\t(219, 100)\n"
     ]
    }
   ],
   "source": [
    "split_frac= 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1750, 100), (46,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_batches(train_x, train_y, batch_size=100))\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[10000,46] labels_size=[46,46]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_2, Reshape_3)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-c891ee9250c8>\", line 11, in <module>\n    num_layers=num_layers)\n  File \"<ipython-input-40-4703d3d512ec>\", line 58, in build_rnn\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[10000,46] labels_size=[46,46]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_2, Reshape_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[10000,46] labels_size=[46,46]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_2, Reshape_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c891ee9250c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[10000,46] labels_size=[46,46]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_2, Reshape_3)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-c891ee9250c8>\", line 11, in <module>\n    num_layers=num_layers)\n  File \"<ipython-input-40-4703d3d512ec>\", line 58, in build_rnn\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/deeplearning/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[10000,46] labels_size=[46,46]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape_2, Reshape_3)]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "num_classes = len(labels)\n",
    "model = build_rnn(num_classes, \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batches(train_x, train_y, num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batches(val_x, val_y, num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
