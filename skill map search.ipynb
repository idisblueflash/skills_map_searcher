{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills map searcher\n",
    "Search related chapter base on text entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from xlsx file. I loaded xlsx file and split it into inputs, labels. Finally, I also split inputs to generate more training datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 1093)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from xlsx file\n",
    "wb = load_workbook('skill_map_data.xlsx')\n",
    "##  print(wb.get_sheet_names())\n",
    "ws = wb.get_sheet_by_name('raw data - Chapter and Text')\n",
    "\n",
    "raw_data = []\n",
    "for row in ws.iter_rows():\n",
    "    raw_data_row = {\n",
    "        \"week_day\" : row[0].value,\n",
    "        \"chapter\" : row[1].value,\n",
    "        \"lesson\" : row[2].value,\n",
    "        \"section\" : row[3].value,\n",
    "        \"text\" : row[4].value\n",
    "        }\n",
    "    raw_data.append(raw_data_row)\n",
    "\n",
    "raw_data = raw_data[2:] # remove table name and header\n",
    "assert(len(raw_data) < 100) # normally we don't have 100+ sections\n",
    "\n",
    "# Split raw_data into inputs and labels\n",
    "inputs = [row['text'] for row in raw_data]\n",
    "assert(len(raw_data) == len(inputs))\n",
    "\n",
    "## concated week_day, chapter, lesson, section into one label\n",
    "labels = [' '.join([\n",
    "            str(row['week_day']), ' ',\n",
    "            row['chapter'], ' ',\n",
    "            row['lesson'], ' ',\n",
    "            row['section']\n",
    "        ]) for row in raw_data]\n",
    "\n",
    "assert(len(raw_data) == len(labels))\n",
    "\n",
    "# Split inputs to generate more training datas\n",
    "seq_len = 200 # length for split long text\n",
    "seq_inputs = []\n",
    "seq_labels = []\n",
    "count = 0\n",
    "for i, input in enumerate(inputs):\n",
    "    if len(input) > seq_len:\n",
    "        for j in range(int(len(input)/seq_len + 0.5)):\n",
    "            seq_input = input[j*seq_len:(j+1)*seq_len]\n",
    "            seq_inputs.append(seq_input)\n",
    "            seq_labels.append(labels[i])\n",
    "            count += 1\n",
    "    else:\n",
    "        seq_inputs.append(input)\n",
    "        seq_labels.append(labels[i])\n",
    "\n",
    "len(seq_inputs), len(seq_labels)\n",
    "# seq_labels[998], seq_inputs[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the concepts behind recurrent networks and see how a character-wise recurrent network is implemented in TensorFlow.\",\n",
       " \"One of the coolest deep learning results from last year was the Google Translate update. They've been a leader in machine learning for a while, but implementing a deep neural network for translation brought the service nearly to the level of human translators. With translation, the correct word to use depends on the context, and all the other words in the sentence, and even in the paragraph. Much of the information contained in language is in the sequence of the words. So far, we've been working with what are called feed forward networks. The input is fed into the network and it propagates forward through the hidden layers to the output layer. In feed forward networks, there is no sense of order in the inputs. Here's a simple idea then, let's build order into our network. First, we'll split up the data into parts. The text, this can be words or individual characters like I've done here with the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's great. Our goal here is to predict the next character in the word steep. To keep it simple, assume our entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'll pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, doesn't have enough information to determine which character to predict. To solve this problem, we'll need to include information about the sequence of characters. We can do this by routing the hidden layer output from the previous step back into the hidden layer. That box there means value from the previous sequence, or time step. Now, when the network sees an E, it knows it saw an S and a T before, so the next character should be another E. This architecture is known as a recurrent neural network, or RNN. Now, the total input to the hidden layer is just the sum of the layered combinations from the input layer and the previous hidden layer, ht minus 1. We can view our recurrent network as one big graph by unrolling it. Now, we have a feed forward network for each character but connected through the hidden layers. Each hidden node receives inputs from an input node and the hidden node from the previous step. To make it a bit more understandable, let's plug in some numbers here. This diagram is borrowed from Andrej Karpathy, with a few alterations. Here, we're one hot encoding the input characters. So, 1000 is an S, 0100 is a T, and so on. There are three units in the hidden layer, and the output layer is showing the logits. You'd pass the logits into a softmax function to get predictions and train with a cross interview loss. This is the basic architecture for our character wise RNN, but there are some improvements we'll need to make, which I'll talk about next.\",\n",
       " \"Before we saw that you can include information from a sequence of data using a recurrent connection on the hidden layer. This connection goes through these weights, Whh. If we enroll the network, we see that the hidden layer sub t is a function of the previous hidden state multiplied by those weights. And the output from that layer is again multiplied by Whh. For every step you have in the network, you're multiplying by the weights again and again. And when you're doing backprop, that's even more multiplications. This leads to a problem where the gradients going through the network either get really small and vanish or get really large and explode. This happens because if you're multiplying by some number a bunch of times, you're going to get two results, except in a couple special cases. If that number is less that 1, you'll end up at 0. If it's greater than 1, you'll head towards infinity. This happens to gradients in a normal RNN. They'll either vanish or explode. This makes it difficult for RNNs to learn long range interactions. Luckily, there is a solution. We can think of recurrent networks as a bunch of cells with inputs and outputs. Inside the cell you have your network layers, such as the sigmoid layer labeled with a sigma here. To solve the problem of the vanishing gradients, we can use more complicated cells called long short-term memory, LSTMs for short. Okay, LSTM cells are pretty complicated at first glance. But if you break it down into parts, they aren't too difficult to understand. The key addition here is the cell state labeled C, I'll get into this next. In this cell, there are four network layers shown as yellow boxes. Each of them with their own weights. The layers labeled with sigmas are sigmoids. And tanh is the hyperbolic tangent function. tanh is similar to a sigmoid in that it squashes inputs. But the output is between -1 and 1 instead of 0 and 1. The red circles are point-wise or element-wise operations. That is, they operate on matrices element by element. The main improvement here is through the cell state. The cell state goes through the LSTM cell with little interaction, allowing information to flow easily through the cells. The cell state is modified only through these element-wise operations which function as gates. And the hidden state is now calculated from the cell state, then passed on. The first gate is the forget gate. The values coming out of the sigmoid layer are between 0 and 1. Then, they are multiplied element-wise with the cell state. So values from this layer close to 0 will shut off certain elements in the cell state. Effectively forgetting that information going forward. Conversely, values close to 1 will allow information to pass through unchanged. This is helpful, because the network can learn to forget information that causes incorrect predictions. On the other hand, long range information that is helpful is allowed to flow through freely. The next bit updates the cell state from the input and previous hidden state. The tanh layer output is added to the cell state, again, gated by a sigmoid layer. In this way, the cell state can be updated in the step and passed along to the next cell. Here, the cell state is used to produce the hidden state which is sent to the next hidden cell as well as to higher layers. It's the arrow pointing up here. The cell state is passed through another tanh then gated again with another sigmoid layer. All these sigmoid gates let the network learn which information to keep and which information to get rid of. Putting all this together, the LSTM cell consists of a cell state with a bunch of gates used to update it, and leak it out to the hidden state. This is just the basic LSTM. There are multiple variations and a lot of ongoing experimentation into improving these. They are also easily stacked into deeper layers. You just send the output from one cell to the input of another. Okay, so now you might be wondering how all of this fixes our gradient problem. Since the cell state is allowed to flow through the hidden layers with only this linear sum operation. Gradients can easily move through the network without being diminished. You also get gradients added into the network through the LSTM cells but they're just added to the gradients flowing through. I'm not going to get into the math behind all this. I'll link you to some resources such as a great lecture by Andre Carpathy to help you out there. LSTMs are the basic unit of recurrent networks these days. You don't have to completely understand the inner workings. But knowing how they work conceptually will be important.\",\n",
       " \"RNN Resources\\nHere are a few great resources for you to learn more about recurrent neural networks. We'll also continue to cover RNNs over the coming weeks.\\n\\t•\\tAndrej Karpathy's lecture on RNNs and LSTMs from CS231n\\n\\t•\\tA great blog post by Christopher Olah on how LSTMs work.\\n\\t•\\tBuilding an RNN from the ground up, this is a little more advanced, but has an implementation in TensorFlow.\",\n",
       " \"Hey, welcome back, so previously, I talked to you about the concepts behind RNNs and LSTMs. But now I'm actually going to walk through an implementation of these things in TensorFlow. So here I'm building a character-wise RNN. So what that means is that it looks at the characters in text and it can generate new characters. So it generates new texts character by character. And I'm going to train this on Anna Karenina, which is one of my favorite books of all time, it's super good. So then the idea is that train it on Anna Karenina, and then it can generate new text from the book. And it's going to learn names in the structure and all these cool things. So this network is based off a blog post by Andrej Karpathy. You should check out this link here. I'll share this notebook with you so you can work through it and also check out these links. So this was a really good blog post of his, he has an implementation that was in Torch and it's on GitHub. When I was building this, I also checked out some other code, which you can find at this link, r2rt, and this dude's GitHub repo. He made a character-wise RNN also. So below here is a diagram that was made by Andrej Karpathy. And it basically shows the general architecture of what I built here. So the idea is to encode each of your characters with one-hot encoding. So this is your input layer, so you just have your one-hot encoded characters. This goes to the hidden layer which is then passed to the next character in the sequence, right. The output of this hidden layer also goes to the output layer where you have your logits. And you use these logits with Softmax to predict what the next character will be. And then your target characters are just the next character in the sequence, right. So this is the general architecture of this network, and you'll see how I implement this in TensorFlow later. Great, so here I'm going to import modules. This is where I'm loading the text. So Anna Karenina is in the public domain, so you just find the text online. Here I'm making the vocab, so this is basically a set of all the characters. And I take those characters and make a dictionary where I can convert the characters to integers. And I have a dictionary that goes backwards from integers to characters. And this is just a giant array of all the characters that we have. So here I'm doing vocab_to_int. So I am making this character array, so all the characters are converted to integers. So this is the first 100 characters you see. This is the first line of the book. And this is what it looks like when it's converted to integers, cool. So now that we have our data in a form we can use, we need to form batches, right, that we're going to pass into our network. We also wannna split it into a training set and validation set and also into the input features and the targets that we want. Basically I wrote this function to do just that. So it takes this character vector, this array that I have here, in the batch size and the number of steps. So the number of steps is the sequence length of characters that we're going to pass into our network. And so the longer the sequence is, then the further back it can look for correlations between characters. So it's just kind of the more steps you have, the better performance your network will have. But it also makes it longer to train, of course. Okay, so the cool thing about this is that our target's y. It's exactly the same as the input characters, just shifted one over, right. So like in hello, our first character is f and the first target is e. So you can just have your x as just grab a bunch characters and y is just everything just shifted over by 1. Here what I'm doing is dropping the last few characters to make sure that when I'm going through and I'm making my batches, that all the batches are full. So I'm just kind of leaving off the tail of this character array. Great, so now that I have my x and my y, I need to split these into batches. So what I'm going to do is I take split(x, batch_size). What this does is it makes a number of different arrays equal to batch size. And then when I stack them, what this does is it makes rows, it stacks them vertically. And so you have rows equal to batch size, and then columns equal to all the rest of the data. So basically what we can do now with this is that we can put a little window on top of this. And the height of this window is going to be the batch size and then the width of the window is going to be the number of steps in the sequence. So you can just move this window along this big 2D array and just kind of slide it over and get your batches. So here's just splitting out the training and validation sets. So pretty standard, you look at this. If we do a batch size of 10 and our step size are 200, then we see the train x, the shape is 10, so it's 10 rows for the batches, pretty normal. And we can look at all the batches for the first 10 steps, so this would be a window into your data. So if your sequence steps was doing 10 steps, right, so this would be the first batch. And for the second batch, you would just shift it over 10 and you get the next batch. And this is basically what that get_batch is, exactly what I just described. Where you are just sliding this window along your data that you see here. [BLANK_AUDIO] So now this is where I actually build the network. So this is all done in TensorFlow, now I'm going to go through it now. So here just kind of typical, your placeholders for the inputs and the targets. And this keep probability is a placeholder that I'll use for drop out. So this is how many of the different connections I'll be keeping in the drop out layers. In this network, I am passing in characters, so actually integers, because I converted them to integers in inputs and targets. And then here, I'm encoding them to one_hot vectors. And this is the RNN layer, it is actually pretty simple to do this part. So you just say I'm going to make a BasicLSTMCell, and the (lstm_size) is the number of hidden units in these cells. And then you just wrap it in Dropout, so it just does Dropout for you, basically. And then it's super easy to stack these up. You can do MultiRNNCells. So this automatically routes the output from one layer of an LCM to the next layer of the LCMs. So you can just stack however many tall, how many deep LCM cells you want. So drop is from the DropoutWrapper and then you just multiply it by the number of layers, and so you get that. Then here we are setting the cell state, so that we're just setting it to zero, so it's the initial state. So it just starts all zeros and this is what we're going to be training as we go through. All right, now this part runs the data through the RNN. So what we're doing here is we're splitting. So first we're splitting tf.split, our x, our input data into number of steps. So what this is doing, it's making just a column of sequences, right? So it's t = 1, i = 1, so this is your first time step. You just get a vector, just a column vector that is the length of your batches. So you're just kind of the stepping through one time point at a time, and you're making this list. And so every element of this list is one step in your sequence. And then you're passing each of this steps in the sequence through your recurrent neural networks, which you have defined here. And all that squeeze does is it just removes any dimensions that are only of size 1. So basically, what we have here is just a list of column vectors, and the length of this column is the size of the batch. So in this line is actually where we run through our network. So we're passing the inputs into our network, I'm just taking cell, and setting the initial state. And what this does automatically for us is that it passes the state from each sequence step to the next one. So just does all that for us, it's really convenient. And at the end, we get out the final state and we get out the final outputs. And the outputs are the outputs of the hidden layer for each step in the sequence, and this is just a big list. So here, we'll keep the final state for later, okay. So like I was saying, the outputs is a big list of the output of each hidden layer at each step. So to make this easier, we're just going to concatenate it into one big list, just get all these outputs and mash them together into one array. Then what I do here is I reshape it so that each row is one output, so lstm_size, so this the size of the number of hidden layers in our cells. So I can reshape it into the size of the lstms, and each row is a different output, so every step has an output. And the number of columns, the width of this output array is the number of hidden units in the lstm cells. So now that I have all that done, I can send all of the hidden outputs through some weights and biases and do softmax, basically. So here I'm just calculating my logits. So you just do your matrix multiplication of the output and the softmax weights and add the softmax biases. And it pass through softmax to the predictions. So here I'm calculating the loss now. So, again, I look to reshaping my targets so that they match the same kind of form as our logits. because remember logits, each row of this array is one output for one step. So we're going to compare each output of a step to the target at that step. And that's exactly what we're doing here. So we're doing softmax_cross_entropy_with_logits. So our logits and our labels here calculate the cost. So we just reduce the means, so just calculate the mean for the losses for each of the steps in the sequence. So here you might remember me talking about how gradients can explode in a recurrent neural network. The easiest way to fix this is that you just clip the gradients. So if the gradient is above 5, you just set it to 5, and that's kind of what this block of code does. It's a pretty simple thing to do, that's nice. And here I'm just exporting the nodes that I want, because the whole thing is just a function right, so I just run the function. Then I export the nodes that I want, so that I can use them later during training. Right, and here are the hyperparameters I'm setting. So batch_size and number of steps in the sequence that I'm looking at, the lstm_size. So this is the number of hidden units in each lstm layer, and this is the numbers of layers I'm using. So then the more data you have, the more parameters you need. And so you cam up your parameters by including more layers and by increasing the size of your hidden layers. Okay, and now we're training. So the idea here is that we have all of our epochs, so I'm just going to use 20, and this is just for saving. Don't worry about it too much. So here this is pretty typical, we just get our batches of data, create your feed dictionary. So here, my inputs are x, my targets are y. And for training the dropout keep probability, I'm going to keep it at 0.5. new_state is being initialized, so I'm taking the initial_state and I'm passing it as the initial_state for the rnn. So what's going to happen is that for every batch, I'm going to get the state after a batch, and I'm just going to route it back into the next batch. So in this way, you can keep your state going through each batch as you're going through the training. Session run, so getting the cost, getting the final_state, which is I'm going to use the new_state for the initial_state for the next batch. And then the optimizer, and so this is what is actually reducing the loss for the network and changing the parameters. And then down here is just the validation part, pretty simple. So again, I'm going to provide this notebook for you so you can look through it and see how it all works, and just kind of work it out for yourself. So this is some of the output for how it trains. And I'm running this on a NVIDIA 1070, and I get 0.13 seconds per batch at these settings. Okay, so I trained it and now we can actually see what the results are, and they're pretty cool. So first off, I wrote this function here, so it's pick_top_n. So basically we have something like 65 different characters. And so every time we try to predict what the next character's going to be, it's going to be one of these 65. So to make it more reliable, what I did is I said, okay, well let's just look at the 5 most likely, or you can set this. Just look at the most likely characters and then pick from those, right? So I'm saying, basically, for 5, what are the 5 most likely characters to come next, and then choose from those. So with sampling, the whole idea is that you pass in a character, and then your network will predict for you what the next character will be. And then you take that character, pass it back in, and you get the next one. And you take that, you pass it back in, and you get the next one. And so you can just build up text this way. You're just continuously getting new characters, passing it back into your network and then it all just works out and just makes a cool thing. In here I'm also priming it, so when I'm doing this, I'm passing in characters, so in this case, Far. And it actually starts generating the state for you, so that the rest of the characters have something to be based on. You're starting to sequence going through your network. So again, read up on this, go through this notebook, and see how all of this all works. And this is what we get out of it. So this is after 3,560 iterations, so not epochs but iterations, so going through all the batches and the epochs. This network learned English and learned sentence structure and question marks and to put quotes around stuff. And put commas before a quote and a space, and it's really impressive. I just went from nothing and then it learned English, really cool. However, mostly it doesn't really make a lot of sense. It can get sentence structure and you can figure out words, but actually making sense as a sentence, it doesn't do that very well. He could not trouble to his wife, and there was anything in them of the side of his weaky in the creature at his forteren to him. I mean, most of the words make sense, but as you're going through it, the whole sentence doesn't really cohere. So it's pretty cool that it can learn English words and sentence structure and spaces and stuff. There's apostrophes s, like it knows this stuff, but it's not really understanding sentences and meaning. So now I'm showing you this is after the 200th iteration, showing you what the network knows as it's training, which is pretty interesting. So after 200 iterations, it actually knows where to put spaces, right? So it's, okay, I got a little word, space, little word, space, word, space. Yeah and it's pretty impressive, this is not all that much training and it's understanding that kind of stuff. It doesn't really know any words yet. It knows a few short words, and, out, as, the, etc. Then at 600 iterations, it's looking a little better. We're getting paragraph structure, it's learning longer words, or it's kind of making up longer words. And it knows more short words, so we've got some here, it's looking pretty good. So finally at 1,000 iterations, we see that it has learned how to use quotation marks, right? So quotation, so it's somebody speaking in the book or the dialogue and then a comma and another quotation. I'm going to provide this notebook for you, and this is not the final form. This is still just kind of me working on it. I'm going to keep it on our GitHub Repo and make updates to it and make it nice and easy to use. And feel free to take it and build off of it and make your own recurrent neural network.\",\n",
       " \"Character-wise RNN\\nYou can get the notebook with the character-wise RNN from our public GitHub repo.\\nTo clone the entire repository to your machine:\\ngit clone https://github.com/udacity/deep-learning.git\\nThis code requires TensorFlow 1.0, so make sure you upgrade if you're using an older version. Play with the network, improve it, train it on your own text. This thing is for you to build off of.\\nIf you find ways to improve it, make a pull request and we'll add it in.\",\n",
       " \"Siraj is going to predict the closing price of the S&P 500 using a special type of recurrent neural network called an LSTM network. He'll explain why we use recurrent nets for time series data, and why LSTMs boost our network's memory power.\",\n",
       " \"Can we actually predict stock prices with machine learning? Investors make educated guesses by analyzing data. They'll read the news, study the company history, industry trends, there are lots of data points that go into making a prediction. The prevailing theory is that stock prices are totally random and unpredictable. A blindfolded monkey throwing dart at a newspaper's financial pages could select a portfolio that would do just as well as one carefully selected by experts. But that raises the question, why didn't top firms like Morgan Stanley and Citi Group hire a quantitative analyst to build predictive models? We have this idea of a trading floor being filled with adrenaline-infused men with loose ties running around yelling something into a phone. But these days you're more likely to see rows of machine learning experts quietly sitting in front of computer screens. In fact, about 70% of all orders on Wall Street are now placed by software. We're now living in The Age of the Algorithm. Hello world, it's Raj. And today, we're going to build a deep learning model to predict stock prices. Records of prices for traded commodities go back thousands of years. Merchants along popular silk routes would keep records of traded goods to try and predict price trends, so that they could benefit from them. In finance, the field of quantitative analysis is about 25 years old and even now, it's still not fully accepted, understood or widely used. Just like Google plus. It's the study of how certain variable correlate with stock price behavior. One of the first attempts at this was made in the 70s by two British Statisticians named Box and Jenkins using mainframe computers. The only historical data they had access to were prices and volume. They call the model Arima and at the time it was slow and expensive to run, but by the 80s things started to get interesting. Spreadsheets were invented so that firms could model company's financial performance and automated data collection became a reality. And with improvements in computing power, models could analyze data much faster, it was a renaissance on Wall Street, people were excited about the possibilities. They started showing up at seminars and discussing their techniques. >> You should see what's going on at the bigger firms, I mean I know all the information. >> But all this quickly died down once people realized that what works is actually a very valuable secret. >> All right, get the fuck off my boat. >> Since then the most successful quants have gone underground. In the past few years we've seen lots of academic papers published using neural nets to predict stock prices with varying degrees of success. But until recently, the ability to build these models has been restricted to academics who spend their days writing very complex code. Now, with libraries like Tensorflow, anyone can build powerful predictive models trained on massive data sets. So let's build our own model using Keras with a Tensorflow backend. For our training data we'll be using the daily closing price of the S&P 500 from January 2000 to August 2016. This is a series of data points indexed in time order or a time series. Our goal will be to predict the closing price for any given date after training. We can load our data using a custom load data function. It essentially just reads our CSV file into an array of values and normalizes them, rather than feeding those values directly into our model, normalizing them improves convergence. We'll use this equation to normalize each value to reflect percentage changes from the starting point. So we'll divide each price by the initial price and subtract 1. When our model later makes a prediction, we'll denormalize the data using this formula to get a real world number out of it. To build our model, we'll first initialize it as sequential since it will be a linear stack of layers. Then we'll add our first layer which is an LSTM layer. So what is this? Let's back up for a bit. Recognize this beat? Sing the lyrics with me. You don't have to say what you did. I already know. I found out from him. It's easy to recall the words forward, but could we sing them backwards? [MUSIC] No, the reason for this is because we learn these words in a sequence. It's conditional memory. We can access a word, if we access the words before it. Memory matters when we have sequences, our thoughts have persistence but feed forward neural nets don't. They accept a fixed size vector as input like an image. So we couldn't use it to say predict the next frame in a movie because that would require a sequence of image vectors as inputs. Not just one since the probability of a certain event happening would depend on what happened every frame before it. We need a way to allow information to persist and that's why we'll use a recurrent neural net. Recurrent nets can accept sequences of vectors as inputs. So recall that for feed forward neural nets, the hidden layer's weights are based only on the input data. But in a recurrent net, the hidden layer is a combination of the input data at the current time step and the hidden layer at a previous time step. The hidden layer is constantly changing as it gets more inputs and the only way to reach these hidden states is with the correct sequence of inputs. This is how memory is incorporated in and we can model this process mathematically. So this hidden state at a given time step is a function of the input at that same time step modified by a weight matrix, like the ones used in feed forward nets. Added to the hidden state of the previous time step, multiplied by its own hidden state to hidden state matrix, otherwise known as a transition matrix. And because this feedback loop is occurring at every time step in the series, each hidden state has traces of not only the previous hidden state but also of all of those that preceded it. That's why we call it recurrent. In a way, we can think of it as copies of the same network each passing a message to the next. So that's the great thing about recurrent nets. They're able to connect previous data with the present task. But, we still have a problem. Take a look at this paragraph. It starts off with, I hope Senpai will notice me, and ends with, She is my friend, He is my Senpai. Let's say we wanted to train a model to predict this last work given all the other works. We need the context from the very beginning of the sequence to know that this word is probably Senpai, not something like buddy, or mate. In a regular recurrent net, memories become more subtle as they fade into the past since the error signal from later time steps doesn't make it far enough back in time to influence the network at early time steps during back propagation. Joshua Bengio called this the vanishing gradient problem in one of his most frequently cited papers titled, Learning Long-term Dependencies with Gradient Descent Is Difficult. Love the bluntness. A popular solution to this is a modification to recurrent nets called long short term memory. Normally, neurons are units that apply an activation function, like a sigmoid, to a linear combination of their inputs. In an LSTM recurrent net, we instead replace these neurons with what are called memory cells. Each cell has an input gate, an output gate and an internal state that feeds into itself across time steps with a constant weight of 1. This eliminates the banishing gradient problem since any gradient that flows into this self-recurring unit during back prop is preserved indefinitely, since errors multiplied by 1 still have the same value. Each gate is an activation function like Sigmoid. During the forward pass, the input gate learns when to let activation pass into the cell, and the output gate learns when to let activation pass out of it. During the backward pass, the output gate learns when to let error flow into the cell and the input gate learns when to let it flow out of the cell through the rest of the network. So despite everything else in a recurrent net staying the same, doing this more powerful update equation for our hidden state results in our network being able to remember long term dependencies. So for our LSTM layer, we'll set our input dimension to 1 and say we want 50 units in this layer. Setting return sequences to true means this layer's output is always fed into the next layer. All its activations can be seen as a sequence of predictions this first layer has made from the input sequence. We'll add 20% drop out to this layer, then initialize our second layer as another outlet LSTM with 100 units, and set return sequence to False on it since its output is only fed to the next layer at the end of the sequence. It doesn't output a prediction for the sequence, instead a prediction vector for the whole input sequence. We'll use the linear dense layer to aggregate the data from this prediction vector into one single value. Then we can compile our model using a popular loss function, called mean squared error, and use gradient descent as our optimizer, labeled RMS prop. We'll train our model with the fit function, then we can test it to see what it predicts for the next 50 steps, at several points in our graph and visualize it using MapPlot Live. It seemed that for a lot of the price movements, especially the big ones, there is quite the correlation between our model's prediction and the actual data. So, time to make some money and play some T.I.. But will our model be able to correctly predict the closing price 100% of the time? Hell to the no. It's an analytical tool to help us make educated guesses about the direction of the market that it slightly better than random. So to break it down, Recurrent nets can model sequential data since at each time step the hidden state is affected by the input and the previous hidden state. A solution to the vanishing gradient problem for recurrent nets is to use long short term memory cells to remember long term dependencies. And we can use LSTM networks to make predictions for time series data easily using Keras and Tensorflow. The winner of the coding challenge from the last video is Vishal Batchu. Vishal used transfer learning to create a classifier for cats and dogs. He chose a layer from a pretrained Tensorflow model and built his own custom convolutional net on top of it to make training much faster. Wizard of the week. And the runner up is Jie Xun See. I loved how he added a command line interface for users to input their images. The coding challenge for this video is to use three different inputs instead of just one to train your LSTM network to predict the price of Google's stock. Details are in the read me, post your GitHub link in the comments and I'll announce the winner in a week. Please subscribe for more videos like these and for now I gotta count my stacks of layers so thanks for watching.\",\n",
       " \"In this lesson, you'll learn about embeddings in neural networks by implement the word2vec model.\",\n",
       " \"Hi, it's Mat again!\\n\\nWord Embeddings\\nThis week, we'll be covering embeddings. This is a deep neural network method for representing data with a huge number of classes more efficiently. Embeddings greatly improve the ability of networks to learn from data of this sort by representing the data with lower dimensional vectors.\\nWord embeddings in particular are interesting because the networks are able to learn semantic relationships between words. For example, the embeddings will know that the male equivalent of a queen is a king.\\n\\nThese word embeddings are learned using a model called Word2vec. In this lesson, you'll implement Word2vec yourself.\\nWe've built a notebook with exercises and also provided our solutions. You can find the notebooks in our GitHub repo. Next up, I'll walk you through Word2vec and describe the exercises you'll be working on. Then I'll show you how I implemented Word2vec.\",\n",
       " \"Hey, welcome back. So this week we're going to be covering embeddings. And specifically, I'm going to have you work through this notebook to implement the word2vec algorithm. This algorithm came out a few years ago, I think like 2012. And it basically used a deep learning model to kind of learn semantic information about words and just use what's called embedding to make a much better representation of words that we can use in networks. So before you get started or while you're working through this I suggest checking out these different resources. So this is a really good conceptual overview from Chris McCormick. And here are the two original papers from Mikolov and other people like Google. I think they all worked with Jeff Dean. And here's an implementation that I borrowed some code from. And then finally, the TensorFlow word2vec tutorial which will probably help you out a lot as you're working on this. So this notebook is mostly going to be me walking you through implementing word2vec. I'm not going to go over in too much detail, conceptual stuff, because there's some really good resources here. So definitely, check this stuff out while you're working through the notebook. So word embeddings are really about dealing with things like language and words. Times when you end up with tens of thousands of different classes and if you have different classes than you'll typically will do this with having coding so you end up with these one high encoded vectors that are like tens of thousands of elements long. They're huge. And only one of the elements is set to one and all the rest are zero. So this is a really inefficient representation to use in our networks. So instead what word2vec does is it finds really efficient representations using vectors, basically. So, you can kind of take these 50,000 different words and then find a vector that is maybe like 200 to 300 elements long that actually represents each of these words. That's also super cool because the way these algorithms work is that the network actually learns semantics similarities between words, so for instance you'll end up with like black, white and red which are all colors, having vectors that are really near each other. So there are two architectures for implementing word2vec, so one is called CBOW, continuous bag of words. And the other skip-gram, so the idea with continues back of word is that you, can I you had this window around some target word and then you grab words around us. You grab like words from the context around some target word and use those words does input to your network and you try to predict to this target word. Skip gram it does the opposite of that so, you have a target word W, and you're trying to predict the words that are in the window around that word. So, what this does is it, you're trying to kind of predict the context around a word. And this is cool because if you have words that show up a lot in the same context. So again, like, black, white, and red, they're colors, right, so they're going to show up in a similar context. Then they're going to have similar representations as vectors. So, in this implementation, we're going to be using the skip-gram architecture because in testing, it performs better than the continuous bag of words. So, there are other resources out there that will help you implement CBOW. But we're just going to do Skip-gram here. Okay, so the first thing to do is employ the packages. I'm not going to run anything here. This is all going to be for you. Yeah, and the only thing is you're going to need to have installed for this notebook is numpy, tensorflow, and then later, matplotlib and sidekick learn And I'll be sure to include a packager or environment file or something to help you out here. For this, we're going to be using the text8 dataset, this is just a bunch kind of concatenated Wikipedia articles and it's been cleaned, so that it's kind of like a really good test set. To make sure everything is working. And my opinion, I think when you're implementing a network yourself for the first time is really important to use a dataset that you know works. because if you use a dataset that you don't know if it's going to work for this algorithm, then you're not really sure if you wrote your code wrong, or the dataset's wrong, so it's good to only have one unknown, if you're trying to experiment. Okay, so in these cells you are downloading the text8 dataset into a zip file. And then unzip it and then you can just remove the archive file once it's done, and then read it in. Okay, so here I am doing a preprocessing. So, this fixes up the data to make it easier to train. So, one thing is this converts a punctuation into tokens. So, for instance, if you have commas or periods, then it like convert those into words. In this dataset, there aren't actually any punctuation as far as I know. So, this doesn't make much of a difference, but in other things like if you're trying to do like build an RNN, where you're trying to generate new words, then having these tokens is going to be pretty important. I'm also removing all words that show up five or fewer times in the dataset. So this is going to reduce noise in the data, because in here there's a lot of words that are just like misspelled or if something's weird, they just don't show up very often and then it adds junk in the data that makes the algorithm work worse. So just get rid of all that stuff and then everything will be peachy. So, that's preprocessing, and then here this is just creating some lookup tables. So basically we want to be able to convert from our words to integers and integers back to our words. And this just creates a new dataset where I've converted all the words into integers. And this is what we're going to be passing into our network. [BLANK_AUDIO] So the first exercise for you is in subsampling. So the idea here is that a lot of words show up very often in the dataset. So the, of, for, and, and a, there's a lot of words that show up really frequently but don't actually provide any information about context, right. They're just kind of these words that connect other words. So what we're going to do is called subsampling. And the idea is that you go through your data set, you count up all the frequency of the words, your data set and then for words that show up more often, you just discard it. And this was implemented by Mikolov and they found that it greatly increased the quality of the vectors that they learned and improved training speed. So this is a pretty cool thing to do. So here's the, how you calculate the probability of dropping a word. And I'm going to leave it up to you to actually implement this. So the idea is to go through int words, which is like our dataset converted into integers. And then, for every single word, calculate the probability that you should discard it and then discard it. Then here, train words. This is going to be your final subsampled word list. You can kind of see what you can do here. Okay, and once you're done with the subsampling, the next thing to do is start making batches. So we're using the skip gram architecture. So what we want from our targets is we want to grab the words in a window around our input word. And the window is going to have a size C. So then what we need to do is we need to go through each of our words, so we need. Well okay. We need to write a function called get target. So the idea is like, you pass in a word and then you can convert that word into this window and then in that window you can grab words around it. So the idea here is to choose C equals 5 for instance. So that means our window's going to be five on the left, five on the right. And you randomly choose another R in that range. And then you choose R words from history R words from before your target word. R words in front of it. And then use those are correct labels. So here you're going to implement this function get target. It's going to take the list of words that we already have and an index. So, this is an index in this list of words. And then, it's going to find the window around this index, and then sample the words from that window, and then return the list of words that were sampled. So, this is an exercise I left up for you to do. Here I am creating batches. So, get batches creates a generator. So, if you don't know what a generator is, you kind of see this yield here. So, the idea with the generator is you're going through this loop, when you get to yield, it pauses. And then if you're iterating over this, like it pauses and then you do some calculations. And the next time this generator is called, then it does all this again. And when it gets to yield it returns this stuff and then pauses. And then just kind of like does this, go through the loop, pause, go through the loop, pause and so on. Generators are really cool because it lets you save memory, because you're not like basically calculating this giant list of batches, you're just getting one batch at a time, so I really like using them for doing batches. So another thing is that I haven't actually figured out how to pass in a random number of target words. And then predict a random number of labels, so what I'm doing here is I'm just making, each row is one input target pair. So if I have one input with multiple targets, then I just have multiple rows for that one input. So you can kind of read through this code and see how it works. [BLANK_AUDIO] Here are some images that I borrowed from Chris McCormick's blog. And we just kind of see the general structure of our network. In general, you're going to pass in a one hot encoded vector and of course, the size of this vector is going to be equal to the number of words we have. And then we have this hidden layer of linear neurons, so that means there's no activation function. Just linear. And then our output layer is softmax. We're going to use this to classify, like we've done plenty of times before. So these weights here are going to be our, this is called the embedding layer, and these are our embedding weights. We're going to talk about this more. So then what we want to do, so we're doing the skip gram architecture, right. So we pass in a vector, and then we are trying to, and then our targets our these words that come from the window around it. And what we want to do is we want to train up this hidden layer, we want to train these weights. And this is what we're really interested in, because this is going to give us our reduced dimensions a better representation. So when we're done training, we can just get rid of this Softmax classifier, we don't really care about that. We just want to use these weights in a different network. So if we're doing translations or something. We would first put all of our data in this network, train up these weights to get our embeddings. And then discard the softmax part, and then we're just left with these weights and with this hidden layer. And so this can be the very first step of our network. And then, the output of this goes to the rest of the translation network. Okay, so to start building this graph Create the inputs and labels placeholders. These should both be ints. The batch sizes can vary, so leave these as none. And the labels, when you initialize it, it should have a second dimension. So you list that as none or just one. Okay, so now we're going to talk about the embedding matrix. So this matrix has the sizes, the number of words, right? So the size of your one-hot encoded vectors by the number of neurons in the hidden layer. So this can get really huge, right? So if you have 10,000 words which is actually pretty small and 300 hidden units, then the size is like three million. So it's huge. But using one hot encoded vectors actually kind of saves us. So as you can see here, if you have one hot encoded vector, the only row, the only numbers that come out of your embedding matrix are the row that corresponds with the one in your one hot encoded vector. So, it's really easy to get the output of this matrix multiplication. And all you really do is finding the element just getting the integer of this element so for in this case. And then, you just find the fourth row in your matrix. That's the output your hidden layer. So, base of what will happens is that, your weight matrix actually just becomes a look up table. So now instead of passing in, one kind coded vector is we can just pass in an integer that corresponds to that word. And then that integer is just used in that lookup table to find the output of the hidden units. And it makes it much more efficient. We just kind of skipped this whole matrix multiplication stuff. And all we'll do is just grab a row out of here. It's really nice. So Tensorflow has a function called tf.nn.embedding_lookup. And it does it for us. So basically you pass in the embedding weights, and then you give it an integer, and then it just looks up that row and then you get that row back as a tensor. So here I'm going to leave it up to you to create the embedding matrix. You set the number of embedding features which is like the number of units in the hidden layer. And get the output from the hidden layer using embedding lookup. So this is up to you to do. Okay, so now we're going to talk about negative sampling. So again we are using these giant one hot and coded vectors right, tens of thousands of units. And for each input, you're using our softmax to make predictions, we calculate the loss and then from the loss, we make updates to, like these millions of weights that we have. So this is amazingly inefficient, because there's only one true target as compared to tens of thousands of negative targets. So the idea is that we can actually make an approximation of this loss by only sampling a very small subset of the negative results. So instead of updating all the weights, we're going to update the correct ones. And then we're going to update a small sample of the wrong ones. And this is called negative sampling. So Tensorflow has a convenient function that lets us do this called sampled softmax. So you can click on this link or the link here to get to documentation. I suggest looking this up to figure out how it works. So below is, I'll leave this up to you, it's another exercise. So we're going to create the Softmax weight matrix here, this is the weights from the hidden layer to the Softmax layer. And in the. And then here you're going to calculate the loss using the sampled softmax loss. And this is just where we calculate the cost and define our optimizer. So for validation, I'm not going to hold out a validation set for this. Instead, I'm using this code from implementation. Basically, the idea is that we're going to grab a few common words and few uncommon words and then we're going to actually like calculate the other vectors, the other words that are like close to these random words that we grab. So in this way like at the beginning of the training what you'll see is like just a bunch of random words that are near each other, but then at the end of the training, what you should see if everything is working right is that these words are grouping together that have the similar semantic meanings. So like before when I was talking about colors, you should see like red, black, and blue and white all kind of clustering together as we're going through the training. So here I'm going to be saving data in the checkpoints directory so if it's not created yet just go ahead and run the cell and it'll be created for you. Here are the training. So you don't have to implement this. This is just pretty normal training. We just set the number of epochs and our batch sizes, window size. So initiate all the global variables and go look at the batches, go through the batches and so on. And this part does the validation stuff. So basically, every 100 iterations, it's going to print out the training loss. And then every 1,000 iterations, every 1,000 batches, it'll print out the validation words. Here you can restore the model after you've trained it, if you need to. Okay, finally after you have everything trained you can visualize these word vectors using T-SNE on a two dimensional plane. So remember that our vectors have the size of whatever your hidden units are right? So if you have 200 to 300 dimensions for these vectors. So but to view them, we actually want to put them in these two-dimensional scatterplot, right? So somehow we have to convert the high dimensional vectors down to 2D, right? So what T-SNE does, is you can actually project these things down to two dimensions, but it's pretty unique in that it is able to preserve local structures. So that means that for vectors that are close to each other when we project it down the two dimensions those vectors will still be close to each other. So it's a really nice way to visualize high dimensional data. And there's this post from Christopher Olah that kind of goes through a bunch of different methods for doing this, and also T-SNE. And it's a good block post that I suggest you check out if you're interested in this stuff. Okay. So that is it for this walk through, this kind of introduction to this. I'Il leave it up to you to complete all these exercises. I'll have a solution video that you can watch to figure out how I did all this. So if you get stuck anywhere just check out the solution video and hopefully have fun in implementing word2vec.\",\n",
       " \"Here is my solution for the subsampling part. So what I did here is used a counter object to collect all of the word counts in my data set. So this basically just goes through all the words and int_words and just counts everything up. So we need to do this to get the frequency of the words that we're going to use for f here. And then we need to actually also get the total count of all the words to calculate the frequency. So it's going to be whatever count we have here divided by the total count as your frequency. So here I created a new dictionary using a dictionary comprehension, where I just iterate through word counts. So this counter, and then for every word I just divide by the total count, so this ends up being a dictionary of frequencies for each word in the data set. And here I am calculating the probability to drop or discard a word. So I basically just took this function here. So 1- square root of t / f, so 1- the square root of the threshold over the frequencies. And then, again, just iterating through the words and then calculating the probability for each word as we're going through in this dictionary comprehension. And here I'm using a list comprehension to go through all the words and int words through all the data that we have. And if the probability to drop the word is less than some random number, then I discard it. So this is basically saying, so keep the word. So go through all the words and int words and then keep the word if the probability is less than this. So what this is doing is basically kind of generating a random number, and then if the probability is less than that random number then we're going to keep it, if it's great than that random number then we're not going to keep it. So kind of the idea here, this is similar to rolling a dice. And so, say if the probability is 0.5, then you just roll a dice, right? And if you get greater than, if you get three, four, or if you get four, five or six, then you discard it. And if you get a one, two or three, you discard it. So, that's basically the way this works. It's just an easy way of kind of doing this probabilistic treatment for dropping these words, okay? So that is how I implemented the subsampling. And of course if you have any questions about this feel free to ask in the forums or in SLAC. Great, so here is my solution for the get_target function. So here I know I wanted to grab a random number, so basically here for each training word we will select randomly a number r in range 1 to C. So that's here so R is a random integer in a range of one to C where C is our window size. So then we're going to grab R words from before and we're going to grab R words from after. So index here so words is our big long list of words and index is where we are in that big long list. And so, to get the words before it, we're going to do index minus r. And then to get the words after it, we're going to get index plus r. And then at the very beginning of the list, if your index, so index minus r, it can be negative. And if it is negative, then it's going to kind of mess up the whole algorithm. So, if it is negative, just set it to zeroes, so that we just start at the beginning of the list. So then here, I'm grabbing all the words from before the index and grabbing the words from after the index using my start and stop. And then, so I'm putting this into a set to remove duplicate words, just in case sometimes you might end up with more than one word or the same word that shows up in this window. So just to be safe I just said set all right? And then just return the list of words. Easy. Okay and now here's my solution for building the graph. So here you see I created my inputs and my labels. So just used a placeholder integers so the batches can be dynamically sized. So I'm just going to leave this as none and none. Just left this as none. Setting it to one would also work. So we have our in puts and our labels. And then down here we have our embedding. So I set the number of embedding features to 200. I believe the higher this number is the better representation, the better your embedding actually works. But 200 is fine, and it worked for me, so I'm just going to leave this here. But also of course the higher number of features you have and the longer it takes to. So you have this playoff between do you want to take a longer time to train or do you want a better results? So it's kind of up to you about how you set that thing. Next, created the embedding matrix as a variable and then I initialize as a random uniform. Distribution from negative 1 to 1. And then, here, the size of this is the size of your vocabulary and the size of the embedding. So they're the size of the embedding features. And then, to do the lookup, we just pass in the embedding matrix and the inputs. And so this just does. It takes the integers from inputs, and does the lookup in the embedding matrix. And you get back your embedded feature vector. Really easy. Next is my implementation of negative sampling. So here I chose to use 100 negative labels and create the softmax weight here, so it's pretty normal. The truncated normal distribution to initialize and set the standard deviation to be some what low so that we don't run into gradient problems. And then the size of this is going to be the size of the vocabulary by the embedding feature size. And create the biases or the size of our vocab. Then the sample softmax_loss. So what this does is it takes your weights and then the biases and then the labels of your targets. And this is the embedding feature applet, so the output of the hidden layer in the number of samples. So here you go, this is the n_sampled, and then the number of classes you have. So this is like the n vocabulary, so this is, n_vocab is just the number of words we have in our vocabulary. So what this does, it just calculates the loss using negative sampling and that's pretty simple. So cost and optimizer, that's good. So, this is the result that I got after I trained it. So at the beginning, after about a thousand Iterations and you see my losses at, so my loss started at 5.65, now is down to 5.4 after 1,000 iterations, 1,000 batches. This is on my 1070 and it takes about a tenth of a second per batch. So this does take a bit to train. But it should be an hour, or two, if you have a decent GPU. So, you kind of, see our validation words. So, if you, kind of, look through here, this is just showing the words that are closest, the vectors that are closest to these words, and, for the most part, these are just, kind of, random and not really doing anything yet. But it is because, we just starting training it. So now, I am going to scroll down to at the end and we can see that our network is actually learning the semantic relationship between words. Okay, so this is the validation words at the end of my training. So, after 46,000 iterations and in the losses down below four now which is pretty nice. So one thing we see, so nearest to scale is diatonic and diatonic scales a term from music so it's probably shown up a bunch of times. Ice rink, yeah? Hockey, Sweden, USSR, they like, yeah, like hockey, like ice rinks. Here, like channel channels. So basically one cool thing about this is that, you can see here too report and reports. It actually kind of learns you know, channel and channels, they show up in the same context. So it kind of learns to put these near each other, and we see a lot of our small words like at, is, near a bunch of other small words. So, next I'm actually going to look at this train representations with ts and a and see how close all this things are okay. So this is the data. So this is the first 500 most common words. And I just kind of used TSNE to plot these down into a two-dimensional plane. So this is kind of small and is probably hard to see in the video but if you check out the notebook you should be able to see these as well. So basically, just show off a few of examples of this thing actually working. So over here we have Central, East, Western, South, West, North America. So North America are, is a common term. So is Soviet Union which we see here. We have World War battle, languages and language. Social, political, we get a bunch of single letters over here. And here we have black, red, white, light, school university, college. So, it seems like our network was able to learn a lot of these semantic commonalities between these words and kind of group them together. This is a pretty good indication that this worked well. It could be even better if I used a larger hidden layer, if I had more features to my embedding, and if I trained it longer. So again this is the thing you can just, as you're training you can go through and just keep trying to make your training loss lower and lower, but I decided to stop here because at least it was working fine. Okay and that is all for my solution videos. Be sure to grab this notebook yourself and so you can run all the code. If you didn't get your code to work, or if you just want to make sure you have something that works just go through and use this. Okay, cheers.\",\n",
       " \"Siraj will show you how to generate art with deep learning networks. He'll also talk about the history of computer generated art, and why deep learning models are so good at making art.\",\n",
       " \"How does a computer make art? The great artists of our time have all had their own distinct style. Da Vinci could evoke wonder and layer hidden messages in his works. Goya was able to create unmatched sense of dread. Dali blurred the line between reality and dreams. The lone artist, given the right medium, is able to create beauty. When the film camera was first invented it wasn't thought of as an artistic tool, just one able to capture reality. But when artists got their hands on it, it was the birth of a new era. In fact, every time we've created a technology, artists have found a way to use it as a creative tool, whether it was meant to track our hands or just to take calls. Recently, advances in machine learning have allowed us to generate amazing art pieces with a few lines of code. What if you could prototype an art piece 100 times faster, having your medium actually collaborate with you? If we can extend these biologically learned patterns with machine learned patterns, it'll become much more clear that it's not that machines are our artistic competitors, it's that we've upgraded our own creativity. Hello world, it's Sirag, and let's write a Python script to transform any image into the style of an artist that we choose. One of the first attempts at computational artistry was by a British artist named Harold Cohen in 1973. Inspired by his wife, the prominent Japanese poet Hiromi Ito, he created a program called Aaron, which created abstract drawings. Cohen hand coded base structures into it that were used to arrive at a form. It was a tree like structure and using heuristics, the program was able to take encoded rules and generate new combinations of what it knew. Incredibly, the paintings it generated ended up being displayed at museums across the world. >From London's Tate Modern to San Francisco's SoMa. Fast forward to 2015 when Google released Deep Dream, the Internet went crazy. [MUSIC] All glory to hypnotoad. They trained a convolutional net to classify images. Then used an optimization technique to enhance patterns in the input image, rather than its own weights, based on what it had learned. Soon afterwards, three German researchers used a CNN to transfer the style of a given painting to any image. They later created a website called Deep Art that lets anyone do this easily. Since that initial paper, the AI community has started to think about the possibilities for artistry using machine learning. Even Kristen Stewart from Twilight published a paper on artistic style transfer as part of her new movie, Come Swim. >> Can you live with that? >> Yeah, also I'm a werewolf. So let's understand how this style transfer process works by writing our own script in Keras with a TensorFlow backend. We're going to use a base image, which is this extremely attractive photo of me, and a style reference image. Our script will take the style of this image and apply it to the base image. So we're going to feed these images into a neural net by first converting them into the de facto data format for all neural nets, Tensors. The variable function from Keras backend TensorFlow is equivalent to tf.variable. The parameter will be the image converted to an array, then we'll do the same for the style image, and we'll create a combination image where we can later store our end result. We'll use a placeholder to initialize it with a given width and height. Let's see if we successfully loaded our images. Yep, this checks out. Now, we'll want to combine these three images into a single tensor that we can feed into our model. We'll use the concatenate function to do this in just one line. The next step will be to download our pre-trained model called VGG16 that Keras has wrapped for us beautifully. Setting our input to our newly created tensor and the weights to the imagenet weights. Will set include_top=False since we don't want to include the fully connected layer at the top of the network. VGG16 is a 16 layer, convolutional net, created by the Visual Geometry Group at Oxford, that won the ImageNet competition in 2014. The idea here is that a convolutional net pre-trained for image classification on thousands of different images, already knows how to encode the information contained in an image. It's learn filters at each layer that can detect certain generalized features. We're going to use these filters to help us perform style transfer. And the reason we don't need the convolutional blog at the top, is because it's fully connected layers and softmax function help classify images by squashing the dimensionality feature map, and outputting a probability. We're not going to classify it, just transform our input. We'll frame the style transfer task as an optimization problem, where we have some loss function that measures an error value that we want to minimize. Our loss function in this case can be decomposed into two parts, content loss and style loss. We'll initialize the total loss to zero and add each of them to it. First, the content loss. We can think of an image as having both a style component and a content one. We know that the features that a CNN learns are arranged in order of progressively more abstract compositions. Since the higher level features are more abstract, detecting things like faces and the meaning of the universe, actually not that abstract. We can associate them with content, they detect the objects that make up an image. When we run our output image and our reference image through the network respectively, we'll get a set of feature representations for both from a hidden layer that we choose. Then we'll measure the Euclidean distance between them to calculate our loss. Named after the ancient Greek mathematician, Euclid of Alexandria. The idea of distance is very useful in machine learning. We can use it to find rankings, recommendations, similarities. It's a mathematical way of comparing data. The second loss we'll calculate is style loss. This is still a function of our network's hidden layer outputs, but it's slightly more complex. Let's do this. We still pass both images through the net to observe their activations. But instead of comparing the raw activations directly like for content, we'll add an extra step to measure the correlation of the activations. For both of our images we'll take what's called the gram matrix of the activations at a given layer in the network. This will measure which features tend to activate together. This is calculated by taking the inner product of all the activations at a given layer, which are a bunch of vectors, one for each feature. So, this resulting matrix contains the correlations between every pair of feature maps at a given layer. It represents the tendency of features to co-occur in different parts of the image. Once we have this, we can define the style loss as a Euclidean distance between the gram matrices for the reference image and output image. And we'll compute the total style loss as a weighted sum of the style loss at each layer we choose. It turns out that for style, using just a single layer like we did for content loss doesn't get great results. But when using several layers, results improve. [MUSIC] Now that we have our losses, we need to define gradients of the output image with respect to the loss, and then use those gradients to iteratively improve our output image to minimise a loss. So we'll calculate the derivative of our loss with respect to the activation in a given layer to get our gradients and use them to update our output image. Not our weights, like we usually would. Gradients give us a direction on how to update our output image such that the difference between the base image and the style image becomes smaller. We can call our helper classes combination loss function, giving it the model and the output image as parameters. So, we'll combine the loss functions into one, then get the gradient of the output image with regard to the loss, using the gradients function of Keras. Which translates to, tf.gradients under the hood. This gives the symbolic gradient of one tensor with respect to one or more other tensors. Next, we'll run our optimization algorithm called L-BFGS over the pixels of our output image to minimize this loss. Which is very similar to stochastic gradient descent but quicker to converge. We'll feed our minimizer function the gradients we calculated and it'll output the result image. Let's see what this looks like. Dope, I'm going to submit this to GQ. There are mobile apps that do this as well. Prisma lets you pick filters on your mobile device and Artisto even lets you apply filters to video. We're still in the early stages of using machine learning to create art, so there's a lot of opportunity in this space. So to break it down, convolutional neural nets allow us to transfer the style of any given image onto another. To do this, we'll compute loss functions for style and content using outputs from hidden layers we choose. And we can minimize our loss using an optimization scheme similar to stochastic gradient descent called L-BFGS. The winner of the coding challenge from last week is Itai Zlotogorski, he used several features to model Google's stock data and artfully used RMS Prop as his optimization technique, wizard of the week. And the runner up is Andreas Wienzek, loved your plot. The coding challenge for this video is to apply style transfer that combines a base image with two different style images. Post your GitHub link in the comments, and I'll announce the winner next video. Please subscribe for more videos like this. Check out this related video and for now, I've got to minimize my losses, so thanks for watching.\",\n",
       " 'A Q&A Session with Sai Soundararaj and Naren Thiagarajan from FloydHub.',\n",
       " 'Hello, everyone. Welcome to this special little question and answer event we have today. I\\'m here with Sai and Naren from FloydHub and over the last week or so we\\'ve been collecting questions from you and they\\'re here to answer those questions. So to start with I\\'m going to let them introduce themselves. So Sai. Thanks Matt. Hey I\\'m Sai, one of the co-founders at FloydHub. We try to make the engineering aspects of deep learning easier. I started working in the deep learning space a while ago, back in 08 09 when the term deep learning itself wasn\\'t that popular it was just called massive neural networks. GPUs weren\\'t really used and it was considered a really difficult task to identify a mug that was placed on a fairly clean table. And we\\'ve come a long way since then. More recently led the deep learning research efforts at Bing and Microsoft where we did a lot of NLB-- deep learning for NLB. So I\\'m really excited to be here. I think we have a good set of questions and happy to answer them Cool, my name is Naren. I\\'m Sai\\'s co-founder at FloydHub. Before Floyd I was working at a company called Location Labs. We got acquired by Avast last year. I was a software engineer there. I started out five years ago and most recently I was the director of engineering there. Before Location Labs Sai and I went both into Stanford, that\\'s where we actually met. And yeah I have a lot of engineering background I\\'ve been building systems that scales and serves millions of users. And really excited to be building FloydHub. All right, thanks. So let\\'s get started with some questions. Sounds good All right. So the first one is from Ludwig Tremore who says \"Hi. Big fan of Floyd. My question is it realistic for a guy or gal with a programming background to get a career in deep learning without going through academia?\" I think so, absolutely. It depends what kind of career he\\'s talking about. If it\\'s a career in academia itself then I think it\\'s kind of a prerequisite. That\\'s how the system works to have a PhD at least. But if it\\'s in the industry, I think what we look for when we are hiring or a lot of people in the industry look for is just the ability to execute. You know if you have the ability to bring stuff to work and get things to work, I think it\\'s becoming less and less important what\\'s on your resume than compared to what you can actually do. And this applies to deep learning obviously. Yeah. And that\\'s what we\\'re trying to do here at Udacity. Absolutely, yeah. So then he also asks is it better to focus on learning about deep neural networks and trying to keep up with what\\'s happening in the field, or get back to basics and learn about data science in general first? The distinction between all these fields-- data science, machine learning, and deep learning-- is kind of becoming fuddled by the day. So what\\'s worked best for me is to not focus on the distinction. Rather take an end application, work towards that, and do whatever needs to be done, right. It\\'s obviously going to be a certain amount of data science that needs to be done to even get to the machine learning aspect within deep learning. So yeah, I would say focus on building something. Like I said, execute something that\\'s of value and whatever it takes to get that done. Yeah. Makes sense. OK so this next question is from Thomas Johnson. So, he says Andrew Ng has said that the best way to become proficient in deep learning is to take published papers and try to replicate them. In my experience this is a very good method for learning this stuff too. So can either of you recommend some papers to start with? He says he\\'s particularly interested in one d time series. So like natural language processing and speech recognition, that kind of stuff. Yeah. I don\\'t think it\\'s less of my place to recommend a paper and it obviously depends a lot upon their interests. I would say what\\'s more important is implementing some paper-- getting something done, that\\'s way better than not doing anything, right. So the right way to broadly think about this is to choose a paper of your choice. And choose something simple, right. There\\'s a lot of complicated stuff out there you don\\'t want to get de-motivated on your first project. Choose something simple, implement it. And preferably do something that\\'s-- a lot of-- the amazing thing about the deep learning community is that a lot of it is open source. You have a lot of stuff on GitHub. Choose a paper which has a GitHub implementation, you know. Use that as a reference. Implement your own version of this and now you have something to compare yourself against. The choices of the people-- the folks who\\'ve written the paper have made and how they compare to your choice. This is a great learning opportunity. So I would say start something simple like CAD, R, and N if you\\'re interested in NLB. Fairly simple stuff. But do something. It\\'s better than waiting for the best paper to implement. Sure. So then he also asks \"So what do you recommend if--\" Like he has a question about details of the papers. Like where can he go to get help on like building like a wave net or something. I think being part of Udacity that\\'s one of the amazing things about. This is super vibrant slack channel here. So I would say that\\'s the first place you should go. You have amazing instructors, people you can-- people who can help who can help you with. You have the amazing community. Do that, right. And let\\'s say after this I think the thing that\\'s important is the-- you should be open to asking. There\\'s a lot of forums. There\\'s a machine learning subreddit forum which is extremely open to answering questions about any sort of machine learning, deep learning. Use that. There is Hacker News, there\\'s Quora, there\\'s Stack Overflow. And apart from all this, like-- since it\\'s the same person asking the questions. Let\\'s say you are implementing a paper that has a reference implementation. Feel free to use GitHub\\'s issues to ask questions. Oftentimes the authors of these papers hang around there they\\'re really willing to help but you should be willing to ask. Cool. Yeah I\\'ve noticed that a lot too. Like a lot of these implementations are on GitHub so you can get the code and you can talk to the authors. It\\'s really useful. So then this is more about FloydHub specifically. So why did you choose K80 GPUs like as the basis of your GPU computing? So why not like a Titan X or 1080s? Yeah, so why did you choose this as compared to other--? I will defer this to Naren, he\\'s the engineering expert. So we definitely thought about all kinds of GPUs out there in the market. And one of the biggest constraints for us was we want some product to be out there soon. So we want to get validation for our idea. On the other hand, we want to also make it really cost efficient for people to use, in this case, you know, students. One of the things is Nvidia has these restrictions on what kind of GPUs you can use on virtual devices. So for instance, the consumer grade GPUs, you know, that the person asked about. Those things you cannot use in a virtual device. Like you cannot put it in a virtual machine and use it. So with that constraint we wanted to like-- that\\'s why you see all these cloud providers. They all provide K80s because that\\'s what the media wants everybody to use. But having said that we are already thinking about what the next step is. We want to bring down the cost for ourselves and give it to the end user. So, you know, there are things in works and I hope we can release something pretty soon. Yeah I\\'m looking forward to that. Cool. So then this is from Adam Gutierrez. So if an applicant had a Udacity Nanodegree, what other requirements would you look for to hire? Yes the Udacity Nanodegree is great and it\\'s a great platform for you to start proving yourself. You learn a lot of fundamentals. And one of the things that\\'s great here is I see a lot of this stuff being project driven so that gives you the ability to work on real stuff, right. So when we talk about hiring we see less-- we care less about what\\'s on your resume compared to what you\\'ve done, what you\\'re capable of doing. So if you can basically show that-- not just to us, to any employer-- if you can show that that would be awesome. Yeah and feel free to take a project, implement it. Kind of contribute to an open source project. Write a blog there\\'s a scarcity in the amount of information about deep learning out there. So write a blog. You will become super popular and people are going to come to you. Yeah. Yeah I\\'ve definitely noticed that-- about blogs-- is like I\\'m teaching this stuff and trying to find good resources and there\\'s only like one or two blog posts about any particular topic. And so I\\'m just trying to kind of prove yourself to employers. So this is from Ji Loo. They\\'re asking if \"I would like to use the same feature as trained by a neural network for two tasks. May I have two objective functions for a single neural network, or how could I combine them? I think so yes. This is the concept behind-- one of the concepts behind transfer learning. Where you can take like let\\'s say a VGG net for instance-- which was primarily built for image classification-- and then use this for different purposes related to with similar tasks. For instance face recognition for instance. There\\'s a lot of ways to go about that. You can do what you mentioned. You can take a lair, take a network, freeze a lair, and then change just the last few layers. All these are possible. So I think it\\'s definitely a valid approach and the best way to validate is to try it out. Yeah. Yeah I\\'ve seen VGG net used a lot in multiple projects. So you can just find it on GitHub like somebody just uses VGG net as the basis and then builds off that. So I think it\\'s a very good approach. If you could pick one industry to transform or revolutionize with current deep learning techniques which one would you pick? I can give my stab and then you can. I think it\\'s an important philosophical question for all of us for humanity. So I would say medicine you know cancer cell detection, drug discovery, genetic therapy, and increasing lifespan of humans. I think as humans we have an intrinsic motivation to keep our genes in existence on this planet so I think that\\'s something we would probably see a lot of incorporation of technology. One of the problems that\\'s been in the past is there\\'s been a gap between technology in a lot of these other industries and that gap is becoming less and less significant. So, you know, I definitely see a lot of AI in deep learning being applied to this, it\\'s going to be pretty awesome. For me I agree, I think medicine is a huge area that\\'s going to get disrupted. The other area I think it\\'s already happening is transportation. I feel like-- I think we spent like 4 and 1/2 hours on the road because of an accident on 88. It seems like we should be like doing work while we are in the car, right? You know, like sometime cars are going to happen. It\\'s not just like cars, like trucks-- truck drivers spend a lot of time on the road when they should be at home, you know. Spend their evenings with their kids. And a lot of companies are tackling that problem. I can see that in the next 10 to 15 years that industry is going to get changed a lot, you know. It\\'s going to be more efficient, you know, save a lot of lives in the process. So I\\'m really excited about that. Yeah I thought the same thing this morning because I was driving through the same traffic accident. Yeah it was like man it\\'d be really nice if I could just be doing work while I\\'m in my car. That\\'d be great. I actually did. I was on a conference video call while he was driving and the first thing the person asked me-- on the other side asked me was I hope either your car is not moving or you\\'re in a self-driving car. My co-founder is driving, just in the passenger seat. That\\'s awesome. Yeah that\\'s what I\\'m really looking forward to. Like the automobile industry because I know I think Ford said they\\'re going to have an automated car in like 2021 or something and that\\'s only like four years from now. So-- and I bet like all the other car companies are trying to catch up too. So yeah I\\'m really excited about that. Awesome. Yeah. OK so then kind of on the other side of this what\\'s your biggest fear considering the current state of the art deep learning? Like what if it ends up in the wrong hands? I think the fear is a little misplaced. I mean definitely there\\'s the possibility of just not just deep learning any kind of technology being used in the wrong way. I think the fear in people\\'s mind is what if machines start thinking for themselves and we have the next terminator come up and have caused rampage. So I think that\\'s really far ahead from that happening. Probably not in our lifetimes. So there\\'s a difference between intelligence and sentience. And intelligence is what we\\'re building towards right now. The ability for machines to think and feel is so far ahead. And to quote Andrew Ng in this, he said \"Thinking about things like this is like thinking about how to control overpopulation on Mars.\" We\\'re not even there yet so we\\'ll worry about it when it comes. That said we all have-- as practitioners and researchers in this field we have a responsibility to make our work socially good and socially not evil. And efforts like OpenAI are attacking this. They\\'re tackling this. So they\\'re trying to make the whole field of AI more open. Not just in the hands of big corporations. And yeah I think they\\'re doing-- doing some amazing work and as researchers I would urge everyone to be socially responsible for what they do. Excellent. So this is a question from Depunk Virma, so it\\'s kind of along the same lines. So then what other physical AI interventions will we be seeing in our cities like other than-- other than like self-driving cars? Like what other kinds of improvements could we make to our cities using artificial intelligence and deep learning? I think there\\'s going to be a lot-- there is already a lot, right. So one way-- one of the ways to think about this is instead of thinking of end applications think of more basic stuff. For instance there\\'s going to be improvements in computer vision. There are in computer vision. What-- that this is going to lead to things like self-driving cars, drones, surveillance cameras, and a lot of other things whether we like it or not. There\\'s a lot of other physical manifestations. Like, for instance, your phone. They\\'re going to get more and more intelligent. When you speak into your phone, the voice recognition uses AI. The ability to understand this and transform that to an action, that uses AI. And this is only going to keep improving. That said, it\\'s kind of difficult to predict where the market is going to head. You know the internet of things revolution should have happened a few years ago and people were super excited about it. I mean I don\\'t know if it\\'s-- maybe it\\'s still happening but it\\'s not super popular yet. So it might take time for things to manifest. But it is going to manifest in ways that we expect and ways in which we don\\'t expect as well. Yeah. So along those lines I think Alexa-- so Alexa is like kind of getting like normal people into this AI stuff right now right? So it\\'s like really taking off. So it\\'s good to see that people are welcoming AI into their homes. Yeah, I know absolutely yes. And the barrier between AI machines and humans is going to be less and less. So you\\'re going to interact with AI without actually knowing that you\\'re doing it. It\\'s going to be awesome. It\\'s pretty scary when you think about it sometimes. But yeah. Yeah so this question is from Francisco Sosa. So how is FloydHub helping reproducibility in deep learning? So one of the things that when we started building FloydHub was-- you know, Sai being a deep learning researcher-- one of the things that came up was you run a lot of experiments because you\\'re basically trying a whole bunch of parameters. You have to try a lot of different things to figure out what actually works. That means you\\'re running experiments over a period of weeks or sometimes months. So when you go back to something a month ago, you actually want to be able to say how did I get this end model? Like what was the parameters I used, what was the environment? And-- and it\\'s super valuable for someone to be able to read on it and get the same results because there\\'s other people building on top of it. So you just want to make sure that you know that reproducibility is there. So the way we achieve that is by snapshotting the entire experiment. So in GitHub you just snapshot the code, so you know at every version what was your code. But in a [INAUDIBLE] experiment it\\'s beyond just the code. We snapshot the environment that you ran, what are the parameters that you used, what tensorflow version. And we have all those references and what are the outputs you\\'re getting, the models and things like that. So when you go back and kick off the experiment again we can get it to you that all of this is going to be done in exactly the same environment so you\\'re going to get the same results. So yeah so that\\'s how we get reproducibility, yeah. Yeah one feature I really like about FloydHub is being able to upload data-- like on there-- and then you can share it with other people and have it yourself. So I think it\\'s a really nice feature. Yeah we feel like-- especially in deep learning-- the data sets are going be huge. So in a-- in a university setting or like in a Udacity, like there are thousands of students you don\\'t want everybody to wait for the upload to happen. So instead like we could host once everyone can just like access during their experiments. Yeah so like in our second project we\\'re using the CIFAR-10 data set which you guys have preloaded. So it\\'s really easy for our students to just like start up the project notebook and then connect to that data set and just have it there without having to download it. So great bonus. And hopefully in the future if you get to bigger data sets this is going to be even more useful. Definitely. OK so then continuing, so Francisco Sosa also asks are you adding support for tensorflow serving? So and how will that help deploy deep learning models? Yeah so we definitely started looking at tensorflow serving. I mean tensorflow serving is the way you-- you know, when you have a built model, you train a model, and then you want to be able to expose it as an API so you can query it and get the results. Tensorflow serving-- getting it running out of the box is actually incredibly difficult. So one of the approaches that we took was we actually want to make it more generic so you can actually serve any kind of models not just tensorflow or any platform. So let\\'s say you use pytorch then you have a model like how to use that. So the person we are going towards is like building a more generic serving framework where you have a simple test API that you can send any kind of data and you invoke the evaluation function that you have and the hand that is held back. Obviously this is our v1 version of serving but we are definitely going to invest in more into this and make it a more seamless serving experience. This is super useful, especially when you trained a model and it works well, you\\'re super excited and you just want to share it with your friends. You can just like send this API and they can just play around with it. Nice. So along with pytorch I saw that you guys like recently added a bunch of different frameworks to FloydHub. Like what I\\'ll do you have now? We have pretty much all the top frameworks. We have tensorflow theano, torch-- pytorch, cafe, cara chain, MX net. Basically we have everything-- we have more than people are actually using right now. But that\\'s good to have options. Yeah absolutely. Yeah we\\'ve been doing this based on request. It\\'s fairly simple for us to what-- we\\'re agnostic to the frameworks that we\\'ve done. So we can pretty much run anything and pretty-- in the future we might actually make it open so people can add their own frameworks as well. There\\'s going to be a lot of new frameworks coming up, that\\'s my prediction. So we\\'ll be able to support it all. Nice. That\\'s good to hear. So this is from Arjay. You described FloydHub as a platform as a service company. So Heroku for deep learning. But while Heroku is pure PAAS with-- I\\'m not sure what all these acronyms mean-- with IAAS provided by AWS, azure, Google, or user\\'s choice. It appears that FloydHub is like a combination of these two. So do-- yeah so-- is like is it true that it\\'s a combination? Yeah I agree. I think what this person means is that like-- that there is the concept of platform as a service and infrastructure as a service. I think that\\'s what IAAS is. So yes we are actually a combination of both. We picked Heroku for deep learning as our tag line because it\\'s really easy for people to communicate what we do. But it\\'s not entirely what you do, like there\\'s other stuff. One of them is like we manage the infrastructure. When you want to scale your experiments and you want to have-- I just want a bunch of machines I want it on all these experiments in parallel. We take care of like spinning a break and if machines, making sure they have the right platform framework installed, coral drivers and things like that. So all of that happens behind the scenes. So for the end user it\\'s a seamless experience. You just say I want it on my code in a GPU environment, with tensorflow 1.0 and then we make it happen. But it is true, we are like both platform and infrastructure as a service. Nice. So then-- OK so how can you provide competitive GPU compute prices while competing with 800 pound gorillas like AWS and Google? Yeah it\\'s like-- we get this question a lot. So one of the things that we\\'ve started building up right from the beginning is like a lot of technology around how do we share these resources really well. With the given set of instances we have we want to bin pack them close enough that each user just pays for what they use. But at the same time, we also efficiently schedule a lot of jobs in the same instance. And we use-- we\\'ve been constantly improving this. The other ways we are-- we also reduce costs by getting reserve instances and using spot instances. Like wherever we can like cut down the cost, we do all of that in the background. Again, we don\\'t want the end user to worry about it is expensive, I should be turning this off. Instead they should just be worrying about I\\'m just going to run my experiment, it\\'s done I just pay for what I ran. Yeah I really like the model. Like in my use of FloydHub it\\'s really nice. I just load it up and then when I\\'m done shut it down and I just don\\'t have to worry about any of that stuff. That\\'s nice. Sumon [? Chadergee ?] is asking so aipoly vision is an iOS app which uses deep learning for object recognition. So it uses a computational network directly on the phone and works without an internet connection. So can you give an idea like how this app can be developed and how it works? I think phones are becoming more and more popular. They have pretty good GPUs in them, so I wouldn\\'t be surprised if-- that this is happening. That said, there\\'s a couple of things that you can do. One is there are deep learning frameworks specific to running-- optimized to run on iOS and mobile devices. Deep learning kit I think is one of those. The other approaches that a lot of different services take is to have a simpler version your model to run on your phone. Don\\'t have too many layers, don\\'t use the number of multiplications that need to happen. And you know that tends to work well. Again, I don\\'t know exactly what these guys do but if you for instance, if you use Google Now or city when it does the voice recognition. It still works if you\\'re not connected to the internet but the quality is significantly worse because it\\'s using the local model-- much simpler version of it. So-- so I-- but I think we\\'re going to see improvements and we could possibly in the near future have full fledged models running on mobile devices. Yeah because I know-- so tensorflow was built to run on phones and whatever, right? So anywhere like-- anywhere that can be run like tensorflow can do it, right. And so I imagine like there\\'s a lot of programs like these, apps like these, that have a tensorflow model like underneath, right? This is kind of a long question. So in mathematics they say that David Hilbert was the last mathematician who understood all the mathematics of his time. Now it\\'s even impossible to understand all the mathematics of a single branch. So-- so also at some point there were many amateur mathematicians like Fermat who did great things. But, OK, so for deep learning AI in general, do you think it\\'s better to try to be a generalist or to specialize in a single topic? Yeah so there\\'s multiple questions here, OK. So let\\'s do the first one. I read somewhere that there\\'s more humans alive today than has ever existed in the history of mankind. That\\'s kind of fascinating if you think about it, right? And that\\'s kind of how things go. At some point it\\'s going to be impossible to even know what you can learn let alone learning it. And you know technology and all this information is basically growing exponentially. I think we should stop holding onto the notion that we need to know the whole field to get started. And so in that sense I think that\\'s perfectly fine. That\\'s how the whole evaluation of technology and pretty much all kinds of information happens. In terms of being a generalist versus a specialist I think it\\'s up to a person\\'s personal preference. What\\'s worked good for me in the past is to start off as a generalist, because when I start off with a new area I\\'m not really sure what my liking is going to be for the specifics. Start off as a generalist, try out a few things, and if you really like an area become a specialist in that. So that would be what I would do if I was in this-- if I had to stop Yeah. Yeah similar to my like grad school experience where, you know, it\\'s like-- kind of like learning about neuroscience in general and then I found this like one thing I was really interested in. So then I kind of specialized in that part. Yeah so I was probably-- yeah if I was going to be like a deep learning researcher I\\'d probably go along the same routes as that. Yes and I think this advice applies in general to a lot of different things also. Yeah. So then-- so what do you recommend if one aspires to contribute new techniques to the state of the art and not just implement current algorithms? I think a good place to start is to implement current algorithms. Like one of the questions before was how do we take papers and implement them, right? So, yeah that\\'s a great way to start doing this. Take an existing paper that has a reference implementation, implement it. Compare yours with the existing version and see how your choices differ from the choices that others have taken. And you know maybe your choices are much better than them. There you go, you have-- you\\'re advancing the state of art at this point. I think the way to think about it is not about to start today saying, hey, I\\'m going to go with something that advances the state of art right now. But you do something and that accidentally just by means of the effort that you put in does advance the state of art. That\\'s a much easier approach than going to doing it top down. Yeah. Yeah going back to my research. I mean basically the way like all PhD dissertations work is that you kind of-- you\\'re building off of other people\\'s work. And so it\\'s like you do this literature review, you see what they all did, and then you like find like one thing that you can improve, right? So it seems like that\\'d also be a really good way to apply it to this, right, because I mean deep learning today is still very research oriented. So if you just like find something and then tweak it-- like make it better-- and then publish it on archive or something. Absolutely. And also the state of the art in different areas is different, right. For instance CNN\\'s started being used in images and then audio first, then images, and then they started being used in text for instance. So using CNN\\'s to do text sentiment classification was considered state of art at some point and then R and N\\'s came along and you know started doing this. So yeah, so like exactly what you said. Start with do a literature survey. See if there\\'s techniques that can be transferred across domains. And I totally agree with you on this. So we had a question similar to this before. So is a PhD necessary? I don\\'t have a PhD. Yeah so I have a master\\'s in this field that was focused more on AI and robotics with some application in deep-- like I said back then deep learning wasn\\'t really a popular term. But yes I don\\'t think it\\'s really necessary unless you\\'re going into academia at this point. Makes a lot of sense. So this is a question for both of you. So can you describe your journey to launching your business? It\\'s an interesting concept. This is from a student on slack they\\'re called Nagu. I mean it\\'s his first name. So he tried to bootstrap a similar idea with data modeling as a service but failed so he\\'s just interested in like how you guys manage to launch your-- launch FloydHub. So the way this happened was sort of happenstance. So I was working at Microsoft did a lot of deep learning there and personally me and my team there have had a lot of difficulty. Just bridging the gap between data science and then the engineering aspects to get that data science in reality, a lot of infrastructure a lot of engineering work that needs to be done. There was a lack of doing. So a while ago-- this was earlier last year I created a GitHub repository called dl darker that\\'s basically a bunch of instructions to set up all the popular deep learning frameworks. And I created it, didn\\'t think twice about it, went to sleep and then turned out someone posted that on GitHub on Hacker News. And then it became really popular there\\'s a lot of discussion around this and that\\'s kind of the first time I actually gave serious thought into building this into a product. And a few days later I called Naren-- Naren was my classmate at Stanford one of the best engineers I know. So I called him like dude this sounds good do you want to take a shot at this? And when we initially started out, we were not exactly clear on what we were solving so it was a lot of trial and error. And I think one thing you know everyone wanting to get into the entrepreneurial or startup space should have is persistence. So yeah just try out a lot of stuff, get a lot of feedback. We\\'ve had to iterate on our product quite a bit. I mean we\\'re definitely nowhere close to being perfect. We are still iterating on our product actively. It\\'s quite different from what we had like a month-- a month or two ago. Two two months ago. So yeah, do not be discouraged if your idea doesn\\'t take off. This is probably my 15th startup idea. First actual startup but I\\'ve tried a lot in the past that have never taken off. It\\'s been like three months of intense effort doing it just turns out-- either I didn\\'t persist or turns out I was building something that no one needed. So yeah we are excited to be at this space, we\\'re excited to be working with the Udacity, and we\\'ve been getting a lot of feedback from the students. And which-- which is really amazing. And I don\\'t think we\\'ve got so much focused feedback from one group in a single place in such a short time. And yeah, thank you all for all the feedback. And we strive to make things better every day, so we want to streamline your process. We feel deep learning should be easier than it should. As part of being-- let\\'s say the deep learning foundation\\'s nanodegree program, you should focus on the deep learning and not have to worry about setting up AWS. And hope-- we hope we can bridge this gap and make things easier. I\\'ve been really impressed seeing how quickly you are to respond to students in slack and they say oh hey Sai like what\\'s going on with this part and then you guys just like immediately jump in and help them out. I mean I think like the-- in terms of the people you know who was trying our product, I think the students are like really nice about it, right. Like they understand it\\'s a work in progress. And, you know, they saw the first version like three weeks ago and they know it\\'s not perfect but they still give us feedback. These are the things that\\'ll help us improve. And then we fix it and push it back and they like-- they acknowledge it. They\\'re like oh this is awesome you guys push this feature on and like we are happy about it. So it\\'s really good to get that feedback from them. It kind of energizes us like that\\'s awesome people are like seeing what we are doing. So it\\'s really awesome. And we take feedback seriously from users. So we had an investor who is also very technical and uses our product really actively. So he was telling us you seemed to respond way better to me when I ping you as a user of your product than as an investor. That\\'s true, yeah. So we are grateful for all the feedback and hope we can continue improving. OK, I think that\\'s all the time we have for questions today. I\\'d just like to thank you for coming and doing this for us and like for the students. And I\\'m really excited to get our students to start using Floyd more because it seems like-- from my experience and from our student experience-- it just makes deep learning so much easier than like what we\\'re able to use now. Great thanks for having us. It\\'s been an amazing journey working with you guys for the last two or three weeks and we hope to continue this in the future. Thanks so much for having us. It\\'s been a pleasure. Thanks, Matt. Thanks for having us. Thanks. OK. Cheers.',\n",
       " \"In this lesson you'll learn about using TensorBoard to inspect your network. You can view the TensorFlow graph and the distributions of variables in the network. You can also use it to compare multiple training runs with different hyperparameters.\",\n",
       " \"Hi there, it's Mat again!\\n\\nHi there! This week, we're going to be covering TensorBoard. TensorBoard is an app that runs in your browser and allows you to inspect your graphs and network variables, such as weights and biases. It helps with debugging and can be used to help choose the best hyperparameters for your problem.\\nTo start, watch this TensorBoard tutorial from the TensorFlow dev summit. This is a great overview of using TensorBoard. And here is a tutorial to help you get started using TensorBoard. Next up, I'll be showing you how to use TensorBoard for one of the networks we've built in this course.\\nYou can get the notebook files from our public GitHub repo, in the tensorboard folder. Download the files from there, or clone the repo\\ngit clone https://github.com/udacity/deep-learning.git\\nTry out using TensorBoard on your next project!\",\n",
       " \"Hey, there. Welcome back. So this week, we're going to be talking about TensorBoard. So before you get into these videos and these notebooks, be sure you check out the video from the TensorFlow Dev Summit on TensorBoard. So it gives you an overview of what it is and how to use it. However, it goes through the code a little fast, so that's what I'm going to be focusing on in this series of videos. So if you don't know what TensorBoard is yet, it's basically a web app that you use to view things about your TensorFlow network. You can look at the graph, you can look at parameters as you're training it. Well, after you train it. So things like weights and biases and the cost. So basically, it will help you debug, because if you're training and it's just not working, then you can look at what's going on at the weights and bases and the cost after you're done. It's really convenient, and tensor board is just pretty cool. So I'm going to be using the Anna KaRNNa character-wise RNN network to show you how this all works. So the first thing to do is, let's check out how this graph actually looks using TensorBoard board. The way you use TensorBoard in general is that you write out the information about your graph and about your network and the training to some files. These are usually called logs. So all you need to do is just use tf.summary.FileWriter, give it the path to where you want to save these logs, and then, in this case, since you want to look at the graph, we're giving it the graph. And this is basically all you need to view the graph for this network in TensorBoard. So just as a reminder, I'm getting the graph here, so this function build_rnn passing all these hyperparameters. And you could see here is where I build it. So this is the LSTM cells doing multiple layers and running the data through the RNN. And then down here, I'm doing a softmax and getting the cost. So this is where I wrote out the graph. And I wrote to here. So let's see what this looks like in TensorBoard. So TensorBoard comes installed when you install TensorFlow through Pip or Condor or something. So you'll notice I'm in my TensorFlow environment here. And then I just need to do TensorBoard --logdir So this just tells it where your logs are stored. So here I stored logs/1. OK, and it loads up. And this is where we need to go. So you just copy this URL and paste it into your browser. And here is TensorBoard. So you see all these different tabs, scalars, images, audios, graphs, and that's what we're interested in now. And distributions and histograms. I'll show you scalars and distributions and histograms later. You can learn more about images and audio and embeddings from the documentation or from the video. So if you go to Graphs, then this is going to load up the graph for the RNN that we just saved. OK, and there it is, completely zoomed out so you can see everything. So if you click, hold, and drag, then you can move this around. And you can zoom in like so. You zoom out, and so on. So here what we're seeing are these little ovals are our operations. So this one's called sequence output. And this is a concatenation. And this one is called a graph output, and this is a reshape. So you see these names here that correspond to what I wrote when I was building the graph. So here is seq_output. It's a concatenation. And here's graph_output. It's a reshape. So each of these operations here is one of the nodes in the graph that you saw. And when you name it, the name shows up in TensorBoard. So here is graph_output, reshape, and seq_output. And these connecting things here, this shows the size of the tensors that are flowing. This one is flowing to a matrix multiplication. And that matrix multiplication it's come from this softmax weights to the matrix multiplication. And we have saw soft max biases. And that's going to add, and that gives us our predictions. These are also going over here. So here, we are calculating our loss. And there's all this other stuff going on that I don't really know what it is. And then here, we see these sort of operations for calculating the gradients. There is our Adam optimizer RNN. This stuff that I'm not really sure what's going on. We can see here that it's passing to the Adam optimizer. So you can see we have this Adam note up here, and it's taking inputs from all these things. And it's being sent from here. So TensorBoard is a way of connecting these two nodes in your graph without actually having to draw lines everywhere. Because you see this thing has six lines coming in, five lines coming out. And so, like if you're just drawing lines everywhere, it'd get really messy. So instead, it just tells you where it's going, and then highlights it when you hover over it. OK, so this is actually a little confusing as to what do these beta powers belong to? And what's going on with all these slices and stuff? So TensorFlow gives you what are called name scopes that help you group all these operations together, and it makes everything really nice. And I'll show you how to do that next.\",\n",
       " \"Hey. Hi again. So in this video I'm going to be talking about name scopes. So as you saw before, the graph for this RNN was pretty messy and it's kind of hard to understand exactly what was going on. But name scopes allow us to kind of combine multiple operations, multiple nodes into groups and so to make the graph much more legible and helping us figure out what's going on. So to use name scopes you just write with tf.name_scope and then give it a name. So here I have placed a name scope around my inputs. So this is inputs and also the one_hot encoding. And then here's one around targets. So targets, one_hot encoding, and then when I reshaped it. Then here are where I define the RNN in layers. And then this is where I initialize the state in the forward pass through the RNN, through the lstm cells. This is where I am reshaping the output of the RNN, and where I calculate the logits, and the predictions, and the cost. And then this whole part is where I'm training-- so the gradient clipping and the Adam Optimizer. So now if I-- so I write out the graph for TensorBoard. So now if I do this-- and I'm writing it to log/3-- we can see what it looks like. OK, so now that I've used the name scopes and I saved it, this is what it looks like. This is what it looks like name scoped. So you can see all these great lines are tensors flowing everywhere. And they're all going into this train operation, which makes sense because train needs the gradients from all the nodes, right. So that's why it's connected to everything. So to make this a little bit easier to view what I'm going to do is right click on this and then choose Remove from main graph. And now it's over here. And now you see how nice this looks because we have our inputs, we have in our initial state. This is the keep probability. We'll zoom in on this. So this is the keep probability for the dropout. This is where we're defining our current neural networks, so multi rnn cell. So you can kind of open up these groups of operations by clicking the plus sign. Or you can double click on it and it'll do that. Then we kind of go in here and see what's going on. So this is cell 0, cell 1, and multi rnn cell. So remember that I'm using two layers here. So this is the first layer, and this is the second later, and so on. OK, so if you remember when I created the name scope for the inputs, I had the inputs and one_hot encoding. So you can see both of those operations here in this grouping. And this is the forward pass through the recurrent neural network. So you can see the inputs coming in and they get split into sequence steps. And they're squeezed and then they go into the RNN. And then if we open this up, we see each of our sequence steps. So remember that in this network, basically I'm passing in 100 characters, right. And so there's an RNN cell. If you roll out the RNN, there's a cell for every sequence step. So we can see that here are all of our sequence steps. So if I kept scrolling down, you'd see the other 95 cells that are in our unrolled network. Then we can zoom in here further. So cell 0, cell 1. So again, in our cells, we're doing this multi rnn cell, right. And so we have actually two layers. If we look in here, then you should see the dropout. So this is the basic lstm cell and the dropout and then it passes its tensors to the next layer. And then you get the output. And then going through the RNN forward pass, we have the sequence reshape, logits. So we can see what's in here. So, yeah. So it kind of groups together the softmax stuff-- the matrix multiplication, the adding. Then we're going to open up cost. Yeah, and so we see all these slices and these reshapings from before is just part of the cost calculation, cross entropy loss. And finally over here, train. So this is where we grouped together the gradient descent. And apparently, yeah, this beta1, beta2. These are just automatically generated for the Adam Optimizer. So you can see how the name scopes really helped us clean up this graph. And it makes it much more legible and actually kind of what we would see in our head, right, how this should look. So we have our inputs going into the RNN, the hidden layers, and that comes out. And we reshape it, and it goes to the logits. Then the calculate the costs, and get our predictions. And then all this is going over to a training function, which is just getting all the gradients and doing the optimization. So in general, that's what you'll be doing when you're looking at your graphs. So next, I'm going to show you how to look at the parameters and the variables in your network after you're done training it.\",\n",
       " \"Now I'm going to show you how to write out your variables such as weights, and biases, and things like costs to TensorBoard so you can see how these things change while you're training it. And it's really helpful when you're debugging because it's like, ah, I'm training this thing and the loss isn't getting any better. So what's going on with my network? So being able to actually write out these variables and things will help you debug your network and figure out what's going on. So here I'm going to show you where I added just a bit of code to actually export these things. So we do this by using tf.summary. So tf.summary has a few different ways to collect your data. So in this case, I'm using what's called a histogram. So this is useful when you have a bunch of numbers. So like softmax weights, right. So weights is a matrix. And so what you're going to want to look at is, like this histogram, the distribution of these weights. You can do the same with the biases. You can look at the distribution of the biases. So for instance, if you're not actually training-- or if you're running this and then you're not actually updating these weights, something is wrong with the weights, then if you look at the distribution of the weights later, you'll see that it's not changing. The weights aren't changing as you're training it, which they should because you're doing gradient descent or something, right. So this is a good way to check out that your weights are actually being trained as you're training the network. This is basically all you really need to do is just tf.summary.histogram, give it a name, and then pass in the variable that you want to view in TensorBoard. So here I'm also doing it for the predictions. So it is like the output of the softmax. Then down here I'm doing-- I'm exporting the cost. So this is using tf.summary.scalar. So the cost is basically just one number, right. So every time we pass in a batch and then we train it, and we get the cost at the end. So this is the softmax cross entropy. We're doing-- just taking the mean and that's giving us our cost. And so we're just doing the scalar of the cost. So each of these things is basically like another operation and then later, when you're running the session, you can actually get these things. But we have four different ones. And it would be really nice if we could just kind of group them all together. So we can do that using tf.summary.merge_all. So what this is going to do is it's going to give us-- it's going to take all of these histograms in the scalar-- like the cost, the softmax weights, and biases-- all these things and then group it into just one node that we can get when we run the session later. So now I'm going to show you how to actually get these summaries that you've saved. So here I am using two different writers this time. So the train writer. So this is like I'm writing out the data while it's training and I'm saving it in this folder train. And I'm also passing it in the graph. So you can see the graph in TensorBoard, also. Then here I'm also creating a file writer for testing. So this is like for validation data. So this is a good way to-- after you go through a training pass, to compare the loss on the training set and the loss on the test set, or the validation set. So you can kind of see how they compare as they go along. So as I was talking about before-- so you merged all these different summaries into one node that you can get with session.run. So here it is-- model.merged. So as I said before, you can group all of these summaries into one node called merged, in this case. And then when you run the session, you can get it out just like you would with your final state and the cost and everything. So here I'm using model.merged. So session at run, get your merged summaries. And then I'm just saving it to this variable called summary. And then down here, the train writer-- so add summary, give it the summary, and then whatever iteration you're on. So this is just a way that you can keep track of where the summary was added in your training run. And then down here I'm doing the same thing for the validation data. So get the summary by sess.run, get them merged, and add it to the test writer. So in general, this is how you're going to be saving all your summaries and getting summaries for all of your weights, and biases, and things. You basically, when you're building your graph, you add these tf.summary.scalar or histograms. There's a bunch of other functions here you can also use. And then merge them all. And then when you are doing your training, you just kind of grab the merged summaries out of the summary. And then add the summary to your writer, and then it writes it all out. So now I'll show you what this looks like in TensorBoard. So now we're looking at the TensorBoard after we have our variables loaded in. So here we see scalars, which is our cost. And then we have the distributions and histograms for the logits and the predictions. So first let's look at the cost. So this is what we see. We just have this nice little graph here. So one thing I noticed is that sometimes it takes a little bit for TensorBoard to load in all this data. So you might not see both of these lines or all the lines. What I found that helps is just refresh. Refresh this web app. So just reload the page. And then it'll load in the data and so you can see all this. So you can click on this button here to blow it up and see what's going on. And then we have blue is train and red is test. So we can see how it drops off over time. It's really nice. Now we can look at the distributions. So this is for our logits and this axis is the training iterations. And so we can see-- so this is for the softmax biases. And so we see how they start pretty constrained early and then, as we progress in our training, they get broader. Just like the distribution of the biases gets broader. And so you see here this is for the weights and the light area is the maximum. So this is like the maximum weights reaches here and the maximum weights reaches here. And then these are probably like 95 percentiles and things like that. So yeah, if you're looking at this and then basically, like if the network wasn't training, if there was something wrong with it, then you would see these basically just stay the same all through the training iterations. So you can look at this to kind of debug what's going on with your network. And then you get a similar cool view with histograms. So again, we can blow it up. And then you see-- so this is like the early training and then as we go on, we can actually see how the biases are changing in time and the distribution of biases. So this is the distribution. This is the histogram of our biases. So we can see here we have more biases with values in this range and few with values in this range. So most of them are just like slightly negative, right. But I think it's pretty cool because like, early on, we start with these two peaks and they kind of start spreading out and dying off. And then after some time, we just get this one peak over here. It's really cool. So this is the offset in time and if you click on overlay, you can kind of squish it all into this 2D plane instead of having it stretched out in 3D. And this lets you better compare-- so this is the beginning-- so better compare how it looks, like the actual values across training iterations. Because here it's hard to compare this height with this height. But if you do the overlay mode, then you can see what the actual difference is here. So that's basically how you look at these things in TensorBoard. I think the TensorFlow Dev Summit video actually has some other cool examples about how to use this. So definitely check that out. So next I'm going to show you how we can actually use this to choose our hyperparameters. OK, see you soon.\",\n",
       " \"Now I'm going to show you how to use TensorBoard to select hyperparameters. So the idea of how I'm going to do is just run our trainer network with a whole bunch of different hyper parameters like learning rates and number of layers and the number of units in our RNN cells and see which one does the best. So in this case, I'm not really worrying about the validation loss. So the idea behind this RNN was trying to generate new text, and so I don't really care about overfitting because I just want to generate new text from this one data set. So I'm not too concerned about generalizing this model to other data sets. So I just threw out the validation stuff and just bundled up the training into one function where I can pass in the model, the number of EPoX, and the FileWriter, so the FileWriter for writing the summaries and other things to TensorBoard. Otherwise, this looks pretty similar to what we had before. So again, I'm just getting the merged summaries from the graph and sending it to this variable, summary, and then adding it to this FileWriter. So same as what we saw in the previous video. So now down here, this is where I'm running through a bunch of different hybrid primaries to see which one does the best. So I'm giving it three different sizes for lstm, so the number of units in the hidden cells, and the number of layers, and then the learning rates. So I'm just looping through all these things, and then every time I have another training run, I create a different log string based on the hyperparameters that I'm using. So in this case, I have learning rate, the number of layers, and the number of units in our RNN cells. And then I can just pass that log string to tf.summary.filewriter, and this creates a new writer. And then this writer, I can pass to the training method. So in this way, you can actually do runs for all these different hyperparameters and save all the summaries, so things like cost. And then in TensorBoard, you can view all these different runs simultaneously so you can compare them. So now I'm going to show you what this looks like in TensorBoard. So now in TensorBoard, I loaded up all these different runs, and we can see them all compared to each other. So each of these different colors is a different run. And again, this is the number of iterations on the x-axis, and then on the y-axis, it's our cost. And then as you hover over these things, you can see we have the names. And so we can see this purple one is learning rate 0.001 with one layer and one 512 hidden units. So we can see that some of these are doing better than others, so this orange up here. So I mean, you can probably guess that the fewer layers and the fewer units you have, then the worst it's going to be. So the ones up here tend to have few units, and then the ones down here have 512 units, and these ones have 128. So there's a few different things you can look at. So yes, one cool thing about this is you can zoom in by dragging and so you can have a better idea about how these things are comparing down here. And then to zoom out, you double click and it zooms out. You can also add a log scale by clicking in this thing. So it doesn't change this very much, but it can be helpful in some cases. So you see this light-colored lines in the back, so that has to do with smoothing. So this does the smoothing over. So we turn it off, then you get the actual costs but they're random, so it's a little bit easier if you smooth it some, a little bit easier to see. You can do things like relative. So this is actually time instead of iterations. So this is by iteration steps. So we did the same number of steps or each of our runs, but some of the runs are going to take longer because it's just going to take longer to train when we have a whole bunch of units and layers. And this is the wall. So this is the actual wall time. So this is the actual times where we were running the network, when we were training it. So one last thing. So TensorBoard has this really cool feature down here where you can actually filter out your runs. So for example, if you only wanted to see the ones with two layers, you do rl equals 2. So I named all these things with our hyperparameters, so it's only going to get the ones where rl equals 2 is in the name. So each of these is where we have two layers. So in general, I think this is a really good way to filter out your data and just filter it out, look at less data so you can compare things. So in general, basically what we see here is that the lowest costs are the ones with the most units and the most layers. It's what we expect. And if you compare learning rates, this yellow one is learning rate, 0.001, and this is 0.002. So it actually seems like a higher learning rate gets us better cost. So it get us lower cost. And that's interesting. And you won't always find that because a lot of times, having a lower cost, it will take longer to run, but it will also end up with lower cost. So this is, in general, a good way to actually try to select all these hyperparameters because I mean, a lot of times you're just shooting in the dark when you're building your network and choosing the hyperparameters and you run it and you look. But this way you just run with a bunch of different hyperparameters, save all the data, just run it overnight. And then you get up in the morning, look at it with TensorBoard and then you can see which hyperparameters are the best. So that's all I'm going to be covering this week, but my challenge to you is to take another one of the networks you built, such as the image classification network or word2vec and look at the parameters in TensorBoard and try to find the best hyperparameters using this method. Cheers.\",\n",
       " \"Siraj is going to build a music generating neural network trained on jazz songs in Keras. He'll go over the history of algorithmic generation, then he'll walk step by step through the process of how LSTM networks help us generate music.\",\n",
       " \"[Log]  In the sixth century BC, Pythagoras-- yes, that Pythagoras-- proposed a concept called the music of the spheres to describe the proportional movements of celestial bodies like the sun, moon, and planets. This music is not thought of as being literally audible but instead a mathematical concept. Math and music are intrinsically connected. The field of algorithmic composition dates back to the early days of computer science. Translation models take an existing non-musical medium, like a picture, and translate it into a sound. These are usually rule-based. So a rule may be that if it sees a horizontal line in an image, it will interpret that as a constant pitch, whereas a vertical line represents an ascending scale. Evolutionary models are based off of genetic algorithms. Through mutation and natural selection, different solutions evolve towards a suitable composition. Then there's the learning model. By providing it musical data, instead of rules, we can let it learn for itself how to create music. We're fast approaching the point where we'll no longer have to wonder how Mozart or Jimi Hendrix would have composed a piece on a certain topic. We'll just be able to ask their algorithmic counterparts ourselves. Hello, world! It's Siraj, and today we're going to use deep learning to generate some jazz music. The first attempt by anyone to use a computer to compose music was by two American professors at the University of Illinois Urbana-Champaign, Hiller and Issacson. They programmed the university's Iliac computer to compose music. And it generated pitches using random numbers before testing them according to the rules of classical counterpoint. So if a pitch didn't fit a piece, another note was generated. It also relied on probabilities via a Markov chain. It used the past to determine the probabilities of the future. The first piece was completed in 1957 and was called the Iliac Suite for String Quartet. Although it made headlines in Scientific American, the musical establishment was pretty hostile to them. They thought it undermined human creativity and didn't include them in their journals until after Hiller's death. Nowadays, there are a ton of amazing generative programs for composers to aid them when they compose music. Like iTunes? Let's understand this process by building a model to generate jazz pieces using Keras. We'll first examine the music we're going to train our model on. Our input data is going to be a collection of piano pieces by American jazz musician, Pat Metheny, in MIDI format. MIDI, or musical instrument digital interface, is like the digital alphabet for music. It contains a list of messages that tell an electronic device, like a sound card, how to generate a certain sound. So it doesn't store actual sound itself, which lends to a much smaller file. Since these messages are a sequence, we'll use a recurrent network as our sequence learning model. For each MIDI file, we'll extract the stream of nodes for both the melody and the harmony. The harmony's chords accompany the melody's single notes. Then we'll group them all together by measure number. So each measure has its own grouping of chords and this measure chord pair is what we'll call our abstract grammars. We'll vectorize these inputs by converting them into binary matrices so we can feed them into our model. Now we can build our model. This is going to be a double stacked LSTM network. So our computation graph will look like this. The vectorize sequence of notes will be input into the first LSTM cell. Then we'll apply a dropout to help ensure that the model generalized well. And we'll do that process one more time. Then we'll feed the data to our last fully connected layer, labeled Dense. Since every neuron in the previous layer is connecting to every neuron in this layer, it will mix all the learned signals together so our prediction is truly based on the whole input sequence. We'll lastly transform the result with a softmax activation function into an output probability for what is likely the next note in the sequence. When we build our first LSTM layer, by default it will only return the last vector, rather than the entire sequence. So we set return sequences to True so that it returns the entire sequence, which is necessary to be able to stack another LSTM later on. Using two LSTM layers instead of one allows for a more complex feature representation of the input, which means more generalization ability. And thus, a better prediction. Recall that recurrent networks are essentially like a series of feedforward networks that are connected to each other. The output of each and its hidden layer is fed into the next one. And when we back propagate, with each layer, the magnitude of the gradient gets exponentially smaller, which makes the steps also very small, which results in a very slow learning rate of the weights in the lower layers of a deep network. This is the vanishing gradient problem and LSTM recurrent nodes help solve that by preserving the error that can be back propagated through time and layers. Let's look a little closer at this process, but first I got to say, [SINGING] L-S-T-M. Say it again. L-S-T-M. Say it again. When data goes in it forgets the BS. Remembers the good stuff then outputs the rest. L-S-T-M. Say it again. An LSTM cell consists of three gates-- the input, forget, and output, as well as a cell state. The cell state is like a conveyor belt. It just lets memory flow across unchanged, except for a few minor linear interactions. These interactions are the gates. We can add or remove memory from the cell state regulated by them. They optionally let memory through. Each is a sigmoid neural net layer and a multiplication operation. The sigmoid outputs a value between 0 and 1, which describes how much of each component should be let through. We'll represent each of the gates with the following equations, where w is the set of weights at each gate. The way its internal memory changes is similar to piping water through a pipe. So think of memory as water. It flows into a pipe. If we want to change the memory flow, this change is controlled by two valves. The forget valve first. If we shut it, no old memory will be kept. If we keep it open, all old memory will pass through. The other is the new memory valve. New memory comes in through a t-shaped joint and merges with the old memory. And the amount of new memory that comes in is controlled by this valve. The input is in old memory and it passes through the forget valve, which is actually a multiplication operation. The old memory hits the t-shaped joint pipe, which represents a summation operation. New and old memory merge through this operation. In total, this updates the old memory to the new memory. We'll define our loss function as categorical crossentropy. The crossentropy between two probability distributions measure the average number of bits needed to identify an event from a set of possibilities. Since our data is fed in sequences, this measures the difference between the real next note and our predicted next note. We'll minimize this loss function using rmsprop, which is an implementation of stochastic gradient descent. So we'll predict the next note in the sequence over and over again until we have a sequence of generated notes. We'll translate this into MIDI format and write it to a file so we can listen to it. Let's hear what this sounds like. [MUSIC PLAYING] At least it's better than Kenny G. So we're all good. So to break it down, we can generate music using LSTM networks to predict sequences of notes. LSTMs consist of three gates-- the input, forget, and output gate. And we can think of these gates as valves controlling how memory is stored in our network to eliminate vanishing gradients. The winner of the coding challenge from the last video is the Vishal Batchu. Not only did he perform multiple style transfer, but he took it a step further by applying it to video, as well. Wizard of the Week. And the runner up is Michael Pelka. He successfully performed multi-style transfer through a clever matrix operation. The coding challenge for this video is to generate a music clip for a genre that you choose. Remember, it's just a sequence of MIDI messages. Post your github link in the comments, and I'll announce the winner next video. Please subscribe for more programming videos. Check out this related video. And for now, I've got to memorize memory cells. So, thanks for watching.\",\n",
       " \"Hello world. It's Saroj. And today, I'm going to show you the five steps to succeed in any programming interview. I've gone through the interview process dozens of times for a bunch of different tech companies, and I've definitely gotten my fair share of rejections and offers. These are the key lessons I've learned from the game. Interviewing takes work. Don't let anyone tell you it's easy, it's not. No one talks about their rejections, only their offers. It's essentially a full time job. But if you follow these steps, I promise you you will succeed. So let's start with step one, study. Before you even think about applying anywhere, you need to get your technical skills on point. The process of getting a job as a software engineer is the same at almost every big tech company. You do an initial phone screen with a recruiter, where they gauge your interest in the company. And if you pass that, you're asked a series of technical questions that you write answers to on a white board. If you can show the interviewer that you can think through a problem and come up with a solid solution, you'll get an offer. But the only way to do that is to practice. All my friends who ended up working for awesome companies practiced a lot. They stuck to a consistent study schedule. It's not about having some divine intellect, it's about how much time and effort you put into studying. So the question is, what do you need to practice? They're not testing you on syntax. You could learn basic Ruby syntax overnight, if you wanted to. But what can't be taught overnight are computer science fundamentals, and that's what companies test you on. Namely, data structures and algorithms. There is no getting around this. Just ask the guy who invented Homebrew. He was rejected by Google because he didn't know how to invert a binary tree, even though tons of engineers at Google use his tool daily. Start off by taking two courses. I recommend the free Intro to Data Structures course by My Code School, and the Intro to Algorithms course by MIT OpenCourseWare. Both are available on YouTube, and will give you broad knowledge about these topics. Then, you can practice what you've learned on HackerRank and HackerEarth. Both are online platforms that let you earn points for solving problems. It's a lot of fun, actually. And once you've solved dozens of problems on both sites, read Programming Interviews Exposed, then Cracking the Coding Interview. Both are great books that will help you practice specific problems that tend to show up a lot during interviews. >From system design, to questions about time complexity. And when you're done, start conducting some mock interviews with a friend. Have them pick questions and code your answer on a white board, explaining your thought process to them. You can do all this in about two months to three months of studying a couple hours every day. Step two is to find the companies you like. When you start the job hunt, you have a limited amount of time you can dedicate to each company. So you don't want to waste it interviewing for companies that aren't exciting. Keeping track of the interview process across multiple companies can be really stressful, unless you stay organized. Use an Excel spreadsheet to map out the companies on your radar, and where you are in the interview process with each of them. You can find these companies on AngelList, which is my favorite resource. Also, Hacker News has a monthly who's hiring, which is great. This is a somewhat spiritual process. You'll probably go through some existential thoughts, and that's OK. Ultimately, it's about deciding how you can best use your skills to make a positive impact in an area you care about, and then finding companies that align with those values. Once you've got a list of companies, it's time to build your portfolio. Recruiters at popular companies get hundreds of resumes every day, so they don't have time to sift through irrelevant stuff. Make sure every single word on your resume counts and it all fits on one page. Make it dense and descriptive. Remember, the less work they have to do, the more likely they'll be to help you out. Make it easy for them and highlight the most important work you've done. You don't have to fit every single CS related thing you've done on it. Whatever you put down, you should be able to speak intelligently about because all of it is fair game during the interview. Have between two to five versions of your resume, each tailored to what a certain category of companies from your list wants. Personal projects, hackathon projects, contributions to open source libraries. GitHub is where you can put your code where your mouth is. It's the new resume in many respects. Have a personal website and give your best project a web presence. It's professional and visualizations always impress. Step four is to get the interview. The easiest way to do this is to just apply to a company directly through their website. But big company get so many interviewees that it's really easy to get lost in the pile. You want to find the gatekeeper, the technical recruiter. You can use a little bit of keyword magic in Google just like this, and it will give you a list of recruiters and people in positions you're interested in at any company. Send them an email, and make sure your email is short and to the point. That means a quick intro introducing who you are, a link to an easily sharable and relevant project, and an expression of interest to learn more. Which brings us to step five, pass the interview. You want to exude confidence, and if you've studied well and your portfolio is on point, you will. Sometimes the interviewer will be more nervous than you are, and that's OK. Smile, shake hands, let them know that you're another human, and you're here to work through a problem together. They'll ask you to code a problem on a whiteboard, and you want to be able to communicate your thought process well. It's not about the answer as much as it is about the way that you think. That's what they're judging you on. Once you've coded your first solution, pretty much every time they'll ask if there's a more efficient solution. There usually is, and this is where your computer science fundamentals will come in handy. And don't forget to ask questions. They're there to help you. It's a collaborative approach. They're judging your technical skills, but they're also judging if they could see themselves working with you. And if you come prepared, you'll do just fine. There are tons of resources on the web to help you practice your technical skills, and I've linked to everything I talked about here in the description. The interview game takes a lot of effort. The key is to not let your rejections ever, ever, ever, ever get you down. Because with enough practice, you can succeed at any technical interview. Believe in yourself. Stay motivated and you're going to make it. For now, I've got to interview someone for the channel. So, thanks for watching.\",\n",
       " \"Siraj will show you how you can turn an article into a one-sentence summary in Python with the Keras machine learning library. He'll go over word embeddings, encoder-decoder architecture, and the role of attention in learning theory.\",\n",
       " \"(siraj) Hello, world! It's Siraj, and we're going to make an app that reads an article of text and creates a one sentence summary out of it using the power of natural language processing. Language is in many ways the seat of intelligence. It's the original communication protocol that we invented to describe all the incredibly complex processes happening in our neocortex. Do you ever feel like you're getting flooded with an increasing amount of articles and links and videos to choose from? As this data grows, the importance of semantic density does as well. How can you say the most important things in the shortest amount of time? Having a generated summary lets you decide whether you want to deep dive further or not. And the better it gets, the more we'll be able to apply it to more complex language, like that in a scientific paper or even an entire book. The future of NLP is a very bright one. Interestingly enough, one of the earliest use cases for machine summarization was by the Canadian government in the early 90s for a weather system they invented called FoG. Instead of sifting through all the meteorological data they had access to manually, they let FoG read it and generate a weather forecast from it on a recurring basis. It had a set textual template and it would fill in the values for the current weather given the data, something like this. It was just an experiment, but they found that sometimes people actually prefer the computer generated forecasts to the human ones, partly because the generated ones use more consistent terminology. A similar approach has been applied in fields with lots of data that needs human readable summaries, like finance. And in medicine, summarizing a patient's medical data has proven to be a great decision support tool for doctors. Most summarization tools in the past were extractive, they selected an existing subset of words or numbers from some data to create a summary. But you and I do something a little more complex than that. When we summarize, our brain builds an internal semantic representation of what we've just read and from that, we can generate a summary. This is instead an abstractive method and we can do this with deep learning. What can't we do with it? So let's build a tech summariser that can generate a headline from a short article using Keras. We're going to use this collection of news articles as our training data. We'll convert it to pickle format, which essentially means converting it into a raw bytestream. Pickling is a way of converting a Python object into a character stream. So we can easily reconstruct that object in another Python script. Modularity for the win. We're saving the data as a tuple with the heading, description, and keywords. The heading and description are the list of headings and their respective articles in order. And the keywords are akin to tags, but we won't be using those in this example. We're going to first tokenize, or split up the text, into individual words because that's the level we're going to deal with this data in. Our headline will be generated one word at a time. We want some way of representing these words numerically. Bengio coined the term for this called word embeddings back in 2003, but they were first made popular by a team of researchers at Google when they released word2vec, inspired by Boyz II Men. Just kidding. Word2vec is a two layer neural net trained on a big label text corpus. It's a pre-trained model you can download. It takes a word as its input and produces a vector as its output, one vector per word. Creating word vectors lets us analyze words mathematically. So these high dimensional vectors represent words and each dimension encodes a different property, like gender or title. The magnitude along each axis represents the relevance of that property to a word. So we could say king plus man minus woman equals queen. We can also find the similarity between words, which equates to distance. Word2vec offers a predictive approach to creating word vectors, but another approach is count based. And a popular algorithm for that is GloVe, short for global vectors. It first constructs a large co-occurence matrix of words by context. For each word, i.e. row, it will count how frequently it sees it in some context, which is the column. Since the number of context can be large, it factorizes the matrix to get a lower dimensional matrix, which represents words by features. So each row has a feature representation for each word. And they also trained it on a large text corpus. Both perform similarly well, but GloVe trains a little faster so we'll go with that. We'll download the pre-trained GloVe word vectors from this link and save them to disk. Then we'll use them to initialize an embedding matrix with our tokenized vocabulary from our training data. We'll initialize it with random numbers then copy all the GloVe weights of words that show up in our training vocabulary. And for every word outside this embedding matrix, we'll find the closest word inside the matrix by measuring the cosine distance of GloVe vectors. Now we've got this matrix of word embeddings that we could do so many things with. So how are we going to use these word embeddings to create a summary headline for a novel article we feed it? Let's back up for a second. [INAUDIBLE] first introduced a neural architecture called sequence to sequence in 2014. That later inspired the Google Brain team to use it for text summarization successfully. It's called sequence to sequence because we are taking an input sequence and outputting not a single value, but a sequence as well. [SINGING] We gonna encode, then we decode. We gonna encode, then we decode. When I feed it a book, it gets vectorized, and when I decode that, I'm mesmerized. So we use two recurrent networks, one for each sequence. The first is the encoder network. It takes an input sequence and creates an encoded representation of it. The second is the decoder network. We feed it as its input that same encoded representation and it will generate an output sequence by decoding it. There are different ways we can approach this architecture. One approach would be to let our encoder network learn these embeddings from scratch by feeding it our training data. But we're taking a less computationally expensive approach, because we already have learned embeddings from GloVe. When we build our encoder LSTM network, we'll set those pre-trained embeddings as our first layer's weights. The embedding layer is meant to turn input integers into fixed size vectors anyway. We've just given it a huge head start by doing this. And when we train this model, it will just fine tune or improve the accuracy of our embeddings as a supervised classification problem where the input data is our set of vocab words and the labels are their associated headline words. We'll minimize the cross-entropy loss using rmsprop. Now, for our decoder. Our decoder will generate headlines. It will have the same LSTM architecture as our encoder and we'll initialize its weights using our same pre-trained GloVe embeddings. It will take as input the vector representation generated after feeding in the last word of the input text. So it will first generate its own representation using its embedding layer. And the next step is to convert this representation into a word, but there is actually one more step. We need a way to decide what part of the input we need to remember, like names and numbers. We talked about the importance of memory. That's why we use LSTM cells. But another important aspect of learning theory is attention. Basically, what is the most relevant data to memorize? Our decoder will generate a word as its output and that same word will be fed in as input when generating the next word until we have a headline. We use an attention mechanism when outputting each word in the decoder. For each output word, it computes a weight over each of the input words that determines how much attention should be paid to that input word. All the weights sum up to 1 and are used to compute a weighted average of the last hidden layers generated after processing each of the inputted words. We'll take that weighted average and input it into the softmax layer along with the last hidden layer from the current step of the decoder. So let's see what our model generates for this article after training. All right, we've got this headline generated beautifully. And let's do it once more for a different article. Couldn't have said it better myself. So, to break it down, we can use [? retrained ?] word vectors using a model like GloVe easily to avoid having to create them ourselves. To generate an output sequence of words given an input sequence of words, we use a neural encoder decoder architecture. And by adding an attention mechanism to our decoder, it can help it decide what is the most relevant token to focus on when generating new text. The winner of the coding challenge from the last video is Jie Xun See. He wrote an AI composer in 100 lines of code. Last week's challenge was non-trivial and he managed to get a working demo up. So definitely check out his repo. Wizard of the week. The coding challenge for this video is to use a sequence to sequence model with Keras to summarize a piece of text. Post your GitHub link in the comments and I'll announce the winner next video. Please subscribe for more programming videos and for now, I've got to remember to pay attention. So thanks for watching.\",\n",
       " \"In this lesson, you'll learn how to find good initial weights for a neural network. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker.\",\n",
       " \"Hi, it's Mat again!\\n\\nThis week, we'll be covering weight initialization. You'll learn how to find good initial weights for a neural network. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker. You can get the notebook files from our public GitHub repo, in the weight-initialization folder. Download the files from Github, or clone the repo\\ngit clone https://github.com/udacity/deep-learning.git\",\n",
       " \"(matt) Hello there, again. It's Matt. This week we're going to be talking about weight initialization. So in this case, how to choose good initial weights for your neural networks. So this is a really important factor for successfully training your networks. If your weights aren't set right, then the backprop algorithm has a really hard time getting to the minimum of the loss. So we're going to go through this notebook, which we will provide for you on our public GitHub repos so you can work through it too and see how all of this works. So let's get started. So what we're doing here is just making a sort of really simple network for fitting to the MNIST data set. So MNIST dataset, if you don't remember, is like a set of handwritten digits. And this is a really common data set used for testing networks and testing performance because people know how it works, there's a lot of other networks that have been trained on this, so you can kind of compare to those. So this network is built in this helper script that is included with this notebook. So if you want to see how that was built, just look in the helper script and you can see how that was put together. But just like a really simple thing. So this network is basically three layers. So you have your input, and then two hidden layers, then the output. And the output is just a softmax classifier. And each of these hidden layers have relu activations, and then we're using the AdamOptimizer. So what I am going to be showing you in this notebook and these videos is how to set your weights appropriately. And this applies to other networks too, not just this simple one on MNIST. So here is just a nice little diagram of the network we're using. So the first layer has 256 units, the second layer has 128, and since we're classifying digits, we have 10 output units. And we can see how we have defined our layers here. So again, we have the inputs, so this is the layer 1 weights, so from our input images to layer 1. So we have the shape of these images to 256 units, 256 to 128, 256 to 128. And then 128 to the shape of our labels, which is 10. So this is the general layout of our network. Now we're going to discuss how do you actually initialize these weights. So kind of the simplest idea for setting your weights is just set them all to 0 or all to 1. You would think that maybe this is the best solution, but it's actually kind of the worst solution. The problem is if every weight is the same, so say they're all 0 or they're all 1, in that case, all of these units are going to be the same, because you're taking your values from input, you're multiplying everything by 0 and adding up. So all of these are 0, and then all of these are 0, and then all of these are 0. And then you're comparing it, again, like softmax classifier using the cross-entropy to get the cost. And then if you're doing backprop, all the gradients everywhere are pretty much the same because all of your activations are the same. And so then the network, this backprop algorithm has a really hard time trying to figure out how to set the weights, or to update the weights, and calculate the gradients because like all the activations are the same. So then all the gradients are the same and then it just has a really hard time actually calculating the true gradient and getting down this last function. And the same deal if the weights are all 1's. So the weights are all 1's, then, again, you have your inputs all multiplied by 1 and added together. So all these units are, again, the same. So you have the same deal where all the units in these hidden layers are the same and the output's the same, and, again, the backprop algorithm has a hard time actually trying to change the weight so they fit the data. So what I've done here is I made basically two different networks. In one network I've set all the weights to 0. So tf.variables, these are all 0 weights. So these are weights, variable, give them 0's. And then here I made another network that were all the weights are 1's. So all the weights are 1's. Then using these weights of these different networks, you can train in this dataset and then look at the loss after some training, like look at the loss as it is trained. So this is basically a figure showing the loss, the network loss, over the number of batches that have been run through the network. So this is all 1's, the blue is all 1's and the red is 0's. So you can see that it starts out high and the loss is like really large. Usually when you're running networks, you should have losses that are pretty small. Usually around like-- I see them around like 1 or 2. Down here we can actually see the validation accuracy. This is like 11% and this is about 10%, which means that our networks are basically doing, I'm guessing, at chance, because we are choosing one out of 10 classes. So if you're just guessing randomly, then you're going to get about a 10% accuracy. We also see that the losses are really high, especially when everything is 1's. So this is basically just performing really poorly. So again, because our weights are all the same, that means the activations of all of our units in the hidden layers are all the same, and then the backprop algorithm is just having a really hard time trying to minimize the loss. So instead what we can do, instead of setting everything all to 0's or all to 1's, is that we want to have all of our weights with unique numbers. And the best way to just grab a whole bunch of unique numbers is just to sample from some probability distribution. And a good solution to do this is with a uniform distribution. So in the next video, I will show you how we can do this with a uniform distribution.\",\n",
       " \" (speaker) Hello, welcome back. So now I'm going to discuss using the uniform distribution to pick unique numbers for all of our weights. So as we talked about before, using the same number for your weights, whether it's 0 or 1 or whatever number, makes it really hard for backprop to actually set the gradients, like find the gradients and change the weights, so that you can get to the minimum of your cost. So instead what we can do is just pick random numbers that come from a uniform distribution. If you don't know what a uniform distribution is, it's basically a probability distribution where getting a number anywhere within a certain range is equally probable. So typically you'll see these like the default values being from 0 to 1. So in Python and many other programming languages, if you just say, give me a random number, it's going to be generated from 0 to 1. So the idea is for every weight we have, we just sample a number from a uniform distribution, and then we'll get-- if you have 1,000 weights, you just get 1,000 numbers. And they're all going to be different just because since this is a continuous distribution, the probability of getting any one number is practically 0. What that means is that there is a very small probability of ever getting the same number twice if you're randomly pulling from a uniform distribution. So TensorFlow provides a function called random_uniform that will sample from a random uniform distribution for us. So you have the shape, which is basically the number of samples that you want from this distribution, and then the range, so minimum value is 0, max value is none, but it defaults to 1. You can use this random uniform distribution to actually get values for your weights. So here I am visualizing this random uniform distribution using a histogram. So the idea behind a histogram is that you have like buckets, bins, or windows where you just kind of count up the number of values that fall in that window. And then this is displayed here. So each of these kind of blue lines, these blue bars, is like one bin. And then the height here on the y-axis is just the number of values that fall in that bin. So here we are selecting 1,000 numbers, 1,000 random values from a random uniform distribution that goes from negative 3 to positive 3. You can see it displayed here. So you can tell that we get roughly the same amount of values in each of these bins. However, of course, there is going to be some noise up and down because this is random values. Overall, you're going to get approximately two values per bin because we're using 500 bins or 500 buckets, and we have 1,000 values. So overall, you would expect there to be about two values per bin, but of course, there's going to be fewer. There's going be like 1 or 0 in some of the bins and there's going to be more in other bins, just because nature of probability. So now we can see how well our network is able to learn from this data by using random uniform distribution for our weights. So here you can see our weights are being set from the random uniform distribution. And so the default range here is from 0 to 1. And again, we are going to plot the loss for this network as is training over the number of batches that we've ran through the network. So you can see that using this random uniform distribution, we actually get a validation accuracy around 65%, which is much better than what we had before because that was like 10%. So before, with all 0's or all 1's, we were basically not able to train our network at all. We were just randomly guessing. But here, our network actually able to learn from the dataset and we can accurately predict these classes, these digits, about 65% of the time. So by just using this random uniform distribution from 0 to 1, then we're able to actually train our network. And it's really cool. So now that we have seen that choosing random numbers is preferable and it's actually helping us train our network, what range do we actually use? So here we just went with the default range, 0 to 1. But what's the best range to choose? What has been found is that there's just a general rule for setting the size of these weights. A good practice is to set the weights from some negative number to some positive number, so y, where y is 1 over the square root of the number of the inputs to a neuron in that layer. First, let's just consider what happens if we do have our weights being set to negative numbers. So we're going to sample our weights from the random uniform distribution again, but this time we're going to have negative 1 and 1 as our ranges. So now we're going to start initializing our weights to negative numbers as well as positive numbers. So if we basically run this experiment again, so now red is our random uniform, that's only positive numbers for the initialization. And then blue is where we initialized with negative numbers as well as positive numbers. And so we can see that the loss starts really high. And then the blue line, which is our initialization from negative 1 to 1, is the loss is basically so small that you can't even see it on the scale of this run. And you can see here that the evaluation accuracy is almost 90% when we set the weights to initialized with negative numbers. So again, this looks like we're doing even better. So now we know that we need to have random numbers, like unique values for all of our weights, and we need to initialize it with negative numbers as well as positive numbers. Now let's look at how we set this range, like what's the best range to sample these numbers from, like what's too large and what's too small. And I'll show you that in the next video.\",\n",
       " \"Hello. So now we're going to talk about how do we find the range of these random numbers that we're choosing. So is it possible for these numbers to be too small? Is it like what's too large? What's too small? Like how does this actually affect our loss and our network performance? So we're going to try a few different ranges here. So negative 0.1 to 0.1 and then 0.01 and then 0.001. So we're just going to try all these and then see how they compare and then like see which one is too small. OK. So here, I'm plotting out the losses for basically four different runs with different weight initializations. So the red is negative 1 to 1 and blue and green and kind of this cyan color are all down here. And these are like progressively getting smaller, so 0.1, 0.01, 0.001. And again, all these losses here are basically so small you can't even see them on the scale of this red line here. So from this, we can easily see that choosing smaller ranges for our weight initialization is helping train the network. So now we can look at the validation accuracy. So for negative 1 to 1, we have 91%. And then from negative 0.1 to 0.1, we have 97%. So decreasing our range for their weight initializations increased the accuracy a lot. But now we see that as we progressively decrease our range, the accuracy starts getting worse. And the loss gets higher. So it seems like around here, around negative 0.1 to 0.1, this is kind of like the sweet spot. And as we start making the weights too small, then we start decreasing our accuracy and the performance of our network. So now that we know these values are too small, we can compare it with our general rule, where the range y is equal to 1 over the square root of n, where n is the number of units you have. So if we compare to negative 0.1 to 0.1, compare that to the general rule, then we see they are almost exactly the same. And so we found that this range that we've just sort of experimented with is practically the same as our general rule. So you see like this general rule is pretty much what we should be using, most the time. OK. So next, we're going to look at what happens when we're using the normal distribution to sample our initial weights. So the idea is that with a uniform distribution, we have the same chance to pick any number within our range. But with a normal distribution, we're more likely to get numbers near 0. So let's see how this affects our accuracy and our loss.\",\n",
       " \"(instructor) Now I am going to go over using a normal distribution for selecting our weights. So again, TensorFlow provides this function, tf.random_normal, for us. What this does is it selects random numbers from a normal distribution. And again, we give it the shape and parameters, like the mean and the standard deviation, and then it'll just grab these numbers for us. And we can use these random numbers to initialize our weights. Here's what a normal distribution looks like. Again, we're using histograms, so we've [INAUDIBLE] our values. So again, select 1,000 values from a normal distribution and use the histogram to actually view this distribution of our numbers. So you can see that you get this tail, but most of the values are centered around 0 here. Then let's compare the normal distribution against our previous runs with uniform distribution. Here, again, we're looking at the loss over training batches. And this time red is our uniform distribution, which we found before. So in a range from negative 0.1 to 0.1. And then blue is initialising our weights with a normal distribution where the standard deviation is 0.1 So you can see that this is actually lower, the loss is lower for the normal distribution pretty much at every point in training. And just a little bit lower, but it is lower. And there are some places where the normal distribution is doing worse. But in general, like in aggregate, if you look at all these numbers, then on average, the normal distributions can be doing better. And you see, looking at the validation accuracy and the loss, that using the normal distribution to initialize our weights is just slightly better. Again, like this might be due to some random variations. But overall, if you did this a whole bunch of times, you would find that the normal distribution usually does better than a random uniform. So as you see up here, with normal distribution, you actually still get some fairly large numbers that are in these tails. Setting your weights to these large numbers can be detrimental to training your network. So what we would prefer is if could pull these numbers closer to 0. So what we can do is just say, let's just not sample anything from somewhat past a threshold. So we could say our threshold is negative 2 and positive 2, so we're just not going to produce any numbers outside of this range between negative 2 and 2. But it's still going to be normally distributed so that we get most of our values around 0. And this kind of distribution is called a truncated normal distribution. So it's truncated because you're cutting off the tails here. Again, TensorFlow provides this for us, so truncated_normal. You can use this function to initialize your weights using a truncated normal distribution. What this does is it generates numbers from a normal distribution, except where the values have magnitude as more than two standard deviations from the mean. In this case, like if your mean is 0 and your standard deviation is 1, any value that has or any random sample that the value is greater than 2 will just be discarded. And then we'll re-pick it, like re-sample it, from the normal distribution. And we can see what that looks like here. So you can see that again we have this normal distribution centered at 0, but we don't get any large values. And again, we can compare this truncated normal distribution to the normal distribution. So in this plot, our normal distribution is red and our truncated normal is blue. And we can basically see that there's no difference. And if you look at the validation accuracy and loss, again, there are really similar, really close, so there's basically no difference between the two. However, this is mostly because the network that we're using is pretty small. We only have 10 input classes, we're only using like 256 units in our first trend layer. So the deal is if you have a larger network with a bunch more weights, then you're more likely to get these extreme values in your normal distribution. So for a small network, it doesn't really matter if we're using normal or truncated, but we're using large networks with a whole bunch of weights. It's usually better to use a truncated normal so that you don't get a lot of these really large values being sampled from a normal distribution. So finally we can compare where we started, which was using a random uniform distribution from 0 to 1, as compared to our truncated normal distribution. So again, this truncated normal distribution does so well that we can't even see the loss on the scale of our baseline. And we went from about 66% accuracy to 97% just by changing the way we initialized our weights. So you can see it's really important to do this correctly if you want to get the best performance out of your network. All right, I hope this was helpful and that going forward you'll be able to understand how to initialize your weights in the best way possible. Cheers!\",\n",
       " \"Additional Material\\nNew techniques for dealing with weights are discovered every few years. We've provided the most popular papers in this field over the years.\\n\\t•\\tUnderstanding the difficulty of training deep feedforward neural networks\\n\\t•\\tDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\\n\\t•\\tBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\n",
       " \"In this lesson, you'll review the matrix math you'll need to understand to build your neural networks. You'll also explore NumPy, the library you'll use to efficiently deal with matrices in Python.\",\n",
       " 'Deep learning involves a lot of matrix math, and it’s important for you to understand the basics before diving into building your own neural networks. These lessons provide a short refresher on what you need to know for this course, along with some guidance for using the NumPy library to work efficiently with matrices in Python.',\n",
       " \"neural networks do a lot of math with a lot of data but before we can start designing those networks ourselves and making sure all that map is set up correctly we first need to understand how we represent that data specifically we need to think about the shapes data can have for example we might have a single number representing a person's height in centimeters or a list of numbers representing various features of that person like height weight and age or maybe we have an image of that person and represented has a grid with rows and columns of individual pixels and one possible way to represent a single pixel in that image might use three numbers one for each of the red blue and green color channels we describe these different shapes of data in terms of their number of dimensions first we have the smallest simplest shape a single value like 12.4 or negative zero point three these are called scalars we say that scalars have zero dimensions it's kind of impossible to visualize zero dimensions but just go with it that's how scalars are defined so from the examples before a person's height would be a scalar and have a zero dimensional shape then there are lists of values called vectors they are two types row vectors and column vectors they can store the same things but conceptually you would think of a row vector like this one two three ordered horizontally from left to right and a column vector like this one two three ordered vertically from top to bottom the reason to use one type of vector over the other is sometimes just a personal preference but other times the shape actually affects the math you can do either way the vector is said to have one dimension which we call its length so each of these two vectors has a length of three our list from before with a person's height weight and age could be stored as a vector next up we have matrices a matrix is a two-dimensional grid of values we describe the shape of a matrix in terms of its numbers of rows and columns so if you had these two rows of data one two three and four five six that's two rows and three columns so we'd say it's a 2 by 3 matrix if you had a single number for each pixel of an image then you could store those pixel values in a matrix finally there are things called enters so here this term a lot in this course because you'll spend quite a bit of time using a software library called tensorflow and it deals with all its data in the form of tensors the term tensor can refer to any n-dimensional collection of values so a scalar is a zero-dimensional tensor a vector is a one-dimensional tensor matrix is a two-dimensional tensor and anything larger than two dimensions we just call a tensor the problem is higher dimensional data can be difficult to visualize for three dimensions you might picture a stack of matrices but you could just as easily picture a list of matrices or a matrix of vectors and for four dimensions you might think of a matrix where each of its entries is itself a matrix or maybe a list or stack of stacks of matrices or maybe even something else and it just keeps getting more complicated as the number of dimensions goes up the important thing to remember is that even if you have trouble imagining how the data is arranged you can probably still make the math work so back to our earlier examples if we had a separate value for each color channel of each pixel of an image we can store those as a three-dimensional tensor with separate planes for red green and blue values for these lessons will mostly talk in terms of scalars and matrices when we use vectors we'll just think of them as matrices to wear one of their dimensions is sized one for example when considering these lengths three vectors from earlier we would say this row vector is a 1 by 3 matrix and this column vector is a 3 by 1 matrix well also sometimes need to refer to individual elements inside a matrix we do that using its index which is its row then column so if this matrix were named a its values are located at these indices the indices of the locations in the first row are a 1-1 a 12 and a 13 the locations in the second row are at embassies a 2-1 a 22 and a 2 3 and so on so we'd say the value at a 3-2 is 8 one less thing I should mention in math books things you read online even most of the things you read in this class indices usually start at one like I've shown here see how the first row is Row 1 and the first column is column 1 counting for one makes perfect sense but in programming languages its more common for indices to start at zero which looks like this all the indices are just one less than you might expect so in code instead of that eight being at index a 3-2 we'd say it's at index a 2-1 if you've got some experience writing software you're probably already used to this so just keep in mind that most written materials index from one but if you're new to programming it's almost certainly going to take you some time to get used to counting from zero.\",\n",
       " \"Introducing NumPy\\nPython is convenient, but it can also be slow. However, it does allow you to access libraries that execute faster code written in languages like C. NumPy is one such library: it provides fast alternatives to math operations in Python and is designed to work efficiently with groups of numbers - like matrices.\\nNumPy is a large library and we are only going to scratch the surface of it here. If you plan on doing much math with Python, you should definitely spend some time exploring its documentation to learn more.\\nImporting NumPy\\nWhen importing the NumPy library, the convention you'll see used most often – including here – is to name it np, like so:\\nimport numpy as np\\nNow you can use the library by prefixing the names of functions and types with np., which you'll see in the following examples.\\nData Types and Shapes\\nThe most common way to work with numbers in NumPy is through ndarray objects. They are similar to Python lists, but can have any number of dimensions. Also, ndarray supports fast math operations, which is just what we want.\\nSince it can store any number of dimensions, you can use ndarrays to represent any of the data types we covered before: scalars, vectors, matrices, or tensors.\\nScalars\\nScalars in NumPy are a bit more involved than in Python. Instead of Python’s basic types like int, float, etc., NumPy lets you specify signed and unsigned types, as well as different sizes. So instead of Python’s int, you have access to types like uint8, int8, uint16, int16, and so on.\\nThese types are important because every object you make (vectors, matrices, tensors) eventually stores scalars. And when you create a NumPy array, you can specify the type - but every item in the array must have the same type. In this regard, NumPy arrays are more like C arrays than Python lists.\\nIf you want to create a NumPy array that holds a scalar, you do so by passing the value to NumPy's array function, like so:\\ns = np.array(5)\\nYou can still perform math between ndarrays, NumPy scalars, and normal Python scalars, though, as you'll see in the element-wise math lesson.\\nYou can see the shape of your arrays by checking their shape attribute. So if you executed this code:\\ns.shape\\nit would print out the result, an empty pair of parenthesis, (). This indicates that it has zero dimensions.\\nEven though scalars are inside arrays, you still use them like a normal scalar. So you could type:\\nx = s + 3\\nand x would now equal 8. If you were to check the type of x, you'd find it is probably numpy.int64, because its working with NumPy types, not Python types.\\nBy the way, even scalar types support most of the array functions. so you can call x.shape and it would return () because it has zero dimensions, even though it is not an array. If you tried that with a normal Python scalar, you'd get an error.\\nVectors\\nTo create a vector, you'd pass a Python list to the array function, like this:\\nv = np.array([1,2,3])\\nIf you check a vector's shape attribute, it will return a single number representing the vector's one-dimensional length. In the above example, v.shape would return (3,)\\nNow that there is a number, you can see that the shape is a tuple with the sizes of each of the ndarray's dimensions. For scalars it was just an empty tuple, but vectors have one dimension, so the tuple includes a number and a comma. (Python doesn’t understand (3) as a tuple with one item, so it requires the comma. You can read more about tuples here)\\nYou can access an element within the vector using indices, like this:\\nx = v[1]\\nNow x equals 2.\\nNumPy also supports advanced indexing techniques. For example, to access the items from the second element onward, you would say:\\nv[1:]\\nand it would return an array of [2, 3]. NumPy slicing is quite powerful, allowing you to access any combination of items in an ndarray. But it can also be a bit complicated, so you should read up on it in the documentation.\\nMatrices\\nYou create matrices using NumPy's array function, just you did for vectors. However, instead of just passing in a list, you need to supply a list of lists, where each list represents a row. So to create a 3x3 matrix containing the numbers one through nine, you could do this:\\nm = np.array([[1,2,3], [4,5,6], [7,8,9]])\\nChecking its shape attribute would return the tuple (3, 3) to indicate it has two dimensions, each length 3.\\nYou can access elements of matrices just like vectors, but using additional index values. So to find the number 6 in the above matrix, you'd access m[1][2].\\nTensors\\nTensors are just like vectors and matrices, but they can have more dimensions. For example, to create a 3x3x2x1 tensor, you could do the following:\\nt = np.array([[[[1],[2]],[[3],[4]],[[5],[6]]],[[[7],[8]],\\\\\\n    [[9],[10]],[[11],[12]]],[[[13],[14]],[[15],[16]],[[17],[17]]]])\\nAnd t.shape would return (3, 3, 2, 1).\\nYou can access items just like with matrices, but with more indices. So t[2][1][1][0] will return 16.\",\n",
       " \" I'm sure you're already familiar with scalar math it's the normal addition multiplication and so on they've probably been doing for most of your life but sometimes I quit neural networks you might need to perform those sorts of operations on a lot of numbers maybe hundreds thousands even millions of them maybe you need to double them square them or divide them all by the same number if you were writing a program you might decide to make a loop where you iterate over every value and perform the same operation on each one over and over but matrices offer a better alternative if you have all those numbers stored in a matrix you can perform element wise operations on them that just means you'll treat the items in the matrix individually and perform the same operation on each one by the way I say matrix but this notion of element wise math applies to data with any number of dimensions to add two scalars it's easy like two plus three equals five to add a scalar and a matrix it's practically the same thing just add the scalar to each of the items in the matrix so two plus the matrix 1 2 3 4 equals the matrix 3 4 5 6 you can do the same thing with other math operators too here's a real-world example let's say we have a matrix of values that represents the red color channel of an image assuming we're using RGB colors each value will be a single byte ranging from 0 to 255 but let's say we want to normalize these values and convert them all to floating point values in the range from 0 to 1 easy we just divide the matrix element wise by 255 by the way don't worry if you don't know what normalizing means you'll learn more about that later in the course and this doesn't just work with scalars and matrices you can also do element wise math between all the elements within two different matrices for this to work the matrices have to be the same shape for example here we have a pair of two-by-two matrices to add them together we just add pairs of numbers that are at the same indices in each matrix and then we store the result at the same location in the answer matrix so it index 11 we'd have the result of adding the 1 plus the 2 which is three and it index 12 we'd have the result of adding the 3 plus the 4 which is seven and so on not just like before how I said you can do any normal math operations element-wise between scalars and matrices you can also perform any of those operations between corresponding elements of two identically shaped matrices.\",\n",
       " 'Element-wise operations\\nThe Python way\\nSuppose you had a list of numbers, and you wanted to add 5 to every item in the list. Without NumPy, you might do something like this:\\nvalues = [1,2,3,4,5]\\nfor i in range(len(values)):\\n    values[i] += 5\\n\\n# now values holds [6,7,8,9,10]\\nThat makes sense, but it\\'s a lot of code to write and it runs slowly because it\\'s pure Python.\\nNote: Just in case you aren\\'t used to using operators like +=, that just means \"add these two items and then store the result in the left item.\" It is a more succinct way of writing values[i] = values[i] + 5. The code you see in these examples makes use of such operators whenever possible.\\nThe NumPy way\\nIn NumPy, we could do the following:\\nvalues = [1,2,3,4,5]\\nvalues = np.array(values) + 5\\n\\n# now values is an ndarray that holds [6,7,8,9,10]\\nCreating that array may seem odd, but normally you\\'ll be storing your data in ndarrays anyway. So if you already had an ndarray named values, you could have just done:\\nvalues += 5\\nWe should point out, NumPy actually has functions for things like adding, multiplying, etc. But it also supports using the standard math operators. So the following two lines are equivalent:\\nx = np.multiply(some_array, 5)\\nx = some_array * 5\\nWe will usually use the operators instead of the functions because they are more convenient to type and easier to read, but it\\'s really just personal preference.\\nOne more example of operating with scalars and ndarrays. Let\\'s say you have a matrix m and you want to reuse it, but first you need to set all its values to zero. Easy, just multiply by zero and assign the result back to the matrix, like this:\\nm *= 0\\n\\n# now every element in m is zero, no matter how many dimensions it has\\nElement-wise Matrix Operations\\nThe same functions and operators that work with scalars and matrices also work with other dimensions. You just need to make sure that the items you perform the operation on have compatible shapes.\\nLet\\'s say you want to get the squared values of a matrix. That\\'s simply x = m * m (or if you want to assign the value back to m, it\\'s just m *= m\\nThis works because it\\'s an element-wise multiplication between two identically-shaped matrices. (In this case, they are shaped the same because they are actually the same object.)\\nHere\\'s the example from the video:\\na = np.array([[1,3],[5,7]])\\na\\n# displays the following result:\\n# array([[1, 3],\\n#        [5, 7]])\\n\\nb = np.array([[2,4],[6,8]])\\nb\\n# displays the following result:\\n# array([[2, 4],\\n#        [6, 8]])\\n\\na + b\\n# displays the following result\\n#      array([[ 3,  7],\\n#             [11, 15]])\\nAnd if you try working with incompatible shapes, like the other example from the video, you\\'d get an error:\\na = np.array([[1,3],[5,7]])\\na\\n# displays the following result:\\n# array([[1, 3],\\n#        [5, 7]])\\nc = np.array([[2,3,6],[4,5,9],[1,8,7]])\\nc\\n# displays the following result:\\n# array([[2, 3, 6],\\n#        [4, 5, 9],\\n#        [1, 8, 7]])\\n\\na.shape\\n# displays the following result:\\n#  (2, 2)\\n\\nc.shape\\n# displays the following result:\\n#  (3, 3)\\n\\na + c\\n# displays the following error:\\n# ValueError: operands could not be broadcast together with shapes (2,2) (3,3)\\nYou\\'ll learn more about what that \"could not be broadcast together\" means in a later lesson, but for now, just notice that the two shapes are different so we can\\'t perform the element-wise operation.',\n",
       " \"[Log]  earlier we talked about element wise matrix operations and that actually covers a lot of the math involved with neural networks but when it comes to multiplication things can get a little more complicated here we're multiplying a matrix by itself to square its values if you look at the answer it's fairly obvious where each value comes from 1 x 1 equals 1 to x 2 equals 4 3 times 3 equals 9 and so on this element wise multiplication is a valid operation but most of the time when people mention matrix multiplication this is in what they mean instead they're usually talking about a special operation called the matrix product which ends up looking like this now when you look at the answer it's probably not so clear where these numbers are coming from here's another difference we said element wise matrix math required matrices of the same size for example this element wise multiplication works fine but if you try to find the matrix product of those same matrices well you can't it can be confusing and you're pretty much guaranteed to mess it up at least a few times when you start building neural networks but this is actually one of the most common operations you'll use so it's important to understand okay so how do we actually calculate the product of two matrices well before we deal with matrices let's start with just two equal lengths vectors one operation we can perform when these vectors is called a dot product for this class you won't need to know how that products are used but there's a link below the video if you're interested to find the dot product we first multiply the corresponding elements of each vector so the 0 x 2 1 the 2 times the 7 and so on then we add up all those results to get a single number in this case 180 this operation basically lets us take two vectors of any length as long as they're equal and convert those vectors into a single number so how does that help us multiply two matrices it turns out to find the product of two matrices we take a series of dot products between every row in the left matrix and every column and the right matrix that's really important to remember whenever you multiply two matrices you're actually dealing with the rows of the first matrix and the columns of the second Matrix I'll go into more detail about why that's important later but for now let's work through an example to find the product of these two matrices we'll start by taking the dot product of the first row of the left matrix and the first column of the right matrix for now just ignore all these other numbers then we can pretend this row in this column are just two equal lengths vectors now we find their dot product with corresponding elements moving across this row and down this column so just like before we have 20 times the one the two times the seven and so on the dot product between this row and this column becomes one entry in the result matrix you put it at the same row and column indices has the row and column we just used to calculate it so since we used Row one in the left matrix and column one in the right the answer goes here in Row one column one of the new matrix if we add instead used row two in the left matrix and column three in the right the answer would end up here in Row two column three okay all that work was just to find one element in the new matrix to get the whole result we need to find out products for every row in this matrix combined with every column in this one there are three columns in the second Matrix so that means we need three dot products for each row in the first matrix and since there are two rows in the first matrix that means we'll need to find a total of six thought products for this one matrix multiplication we have one so to find the other five we just repeat the process for each row column pair so the dot product between row 1 and column 2 becomes the entry Row 1 column 2 then we finish off this row by finding the dot product between row 1 and column 3 we do the same thing for the next row finding dot products for Row 2 column 1 Row 2 column 2 in row 2 column 3 with all that done we finally have the result of the matrix multiplication we actually wanted that was quite a bit of work the inter matrix only has six numbers in it but to find those six numbers we did 24 multiplications in 18 additions as your matrices grow it gets even worse I guess that's why we do stuff with computers right I mentioned earlier how there are times when taking the product of two matrices won't work in the next video I'll talk more about that and give you some important tips about matrix multiplication.\",\n",
       " \"[Log]  hey welcome back in the last video I showed you these two matrices and I mentioned that we can't calculate their matrix product now that you know how matrix multiplication works it's easier to see why we can remember we were finding a series of dot products between the rows of the left matrix and the columns of the right and what do we need to find a dot product we need two vectors that are the same length that means the rows of the first matrix need to be the same length as the columns of the second Matrix and what's the length of a row it's number of columns and what's the length of a column it's number of rows so that gives us a very important rule for matrix multiplication the number of columns in the left matrix must equal the number of rows in the right matrix you can see it easily when you right after shape side by side remember we always specify the dimensions as rose by columns so when you put them next to each other in the order you plan the multiply them the number of columns for the left matrix ends up here on the inside next to the number of rows for the right matrix here if you want to multiply two matrices these two inner dimensions need to be the same size so we could find the product of this two by three matrix times this three by two matrix because these two threes are the same or we could find the product of this three by two matrix and this two by three matrix because these two twos are the same and it might take us a while but we can even find the product of a 10,000 x 300 matrix and a 300 x 6000 matrix because these two three hundreds are the same just remember when you look at their shape side by side no matter what these outer numbers are you can always multiply two matrices if these numbers on the inside are the same and by the way if those numbers are different like they were in our original example where we would end up with a 3 & a 2 in the middle then you just can't multiply them it doesn't work now let's take a look at these two examples in both cases their inner dimensions match so we can multiply them notice how the numbers and both of these left matrices are actually the same so just arrange into a different shape 2 x 3 vs 3 by 2 and likewise for their matrices on the right but these slight changes in shape have a big effect on the actual results of these multiplications not only are the numbers and the answer is different the sizes of the answers are different we get a 2 by 2 matrix in one case and at 3 by 3 and the other this brings us to our second point matrix multiplication the answer matrix always has the same number of rows as the left matrix and the same number of columns has the right matrix again you can see it easily when you write out their shape side by side this time we're looking at these two numbers on the outside the first one is the number of rows for the first matrix and the last one is the number of columns for the second Matrix these two find the shape of the answer in the same order rose by columns so multiplying this two by three matrix by this three by two matrix gives you this two by two matrix and multiplying these matrices that are 3 by 2 and 2 by 3 gives you this 3 by 3 matrix and multiplying at 10,000 x 300 matrix by a 300 x 6000 matrix would give you a 10,000 x 6000 matrix i want to take a look at one more example here we want to multiply these two matrices we can see from these threes in their shapes that it's safe to multiply them and these outer dimensions tell us will get a 2 by 2 matrix as a result in fact we'll get this two-by-two matrix now let's take those same two matrices but swap their order in the multiplication when we look at their shapes in this order we get 3 by 2 and 2 by 3 those inner to is tell us that we can safely multiply them in this order as well but the outer dimensions tell us the result will be a 3 by 3 matrix calculating the product gives us this result this brings us to our third point about majors multiplication order matters if you have two matrices a and B then the product a B is not the same as the product be a standard multiplication is commutative which means you can change the order of the items in the multiplication and you'll still get the same answer 2 times 5 and five times to both equal 10 but as you just saw matrix multiplication is definitely not commutative sometimes the sizes will only line up one way or the other for example a 4 by 2 matrix and a 3 by 4 matrix can only be multiplied in one order this way works but this way doesn't other times like in the previous example you can multiply them in either order but you'll get different results just to make sure you don't think it's only related to the size consider these two square matrices multiplying them in either order produces the same sized result but notice how the values in each of the answer are different this leads me to the last point I want to make about matrix multiplication it's actually something I mentioned in the previous video whenever you multiply two matrices you're actually dealing with the rows of the first matrix in the columns of the second Matrix but now it should be easier to see why that's important if you're not careful your dot products might combine the wrong elements which would give you the wrong answer even if the multiplication looks like it works I'll going to that some more in the next video so remember when you build your matrices organize your data into rows or columns and then be sure to line up all your math appropriately by the way I know this was a long video so I've listed out the main point you need to remember about matrix multiplication and included those in the class notes below.\",\n",
       " \"NumPy Matrix Multiplication\\nYou've heard a lot about matrix multiplication in the last few videos – now you'll get to see how to do it with NumPy. However, it's important to know that NumPy supports several types of matrix multiplication.\\nElement-wise Multiplication\\nYou saw some element-wise multiplication already. You accomplish that with the multiply function or the * operator. Just to revisit, it would look like this:\\nm = np.array([[1,2,3],[4,5,6]])\\nm\\n# displays the following result:\\n# array([[1, 2, 3],\\n#        [4, 5, 6]])\\n\\nn = m * 0.25\\nn\\n# displays the following result:\\n# array([[ 0.25,  0.5 ,  0.75],\\n#        [ 1.  ,  1.25,  1.5 ]])\\n\\nm * n\\n# displays the following result:\\n# array([[ 0.25,  1.  ,  2.25],\\n#        [ 4.  ,  6.25,  9.  ]])\\n\\nnp.multiply(m, n)   # equivalent to m * n\\n# displays the following result:\\n# array([[ 0.25,  1.  ,  2.25],\\n#        [ 4.  ,  6.25,  9.  ]])\\nMatrix Product\\nTo find the matrix product, you use NumPy's matmul function.\\nIf your have compatible shapes, the it's as simple as this:\\na = np.array([[1,2,3,4],[5,6,7,8]])\\na\\n# displays the following result:\\n# array([[1, 2, 3, 4],\\n#        [5, 6, 7, 8]])\\na.shape\\n# displays the following result:\\n# (2, 4)\\n\\nb = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\\nb\\n# displays the following result:\\n# array([[ 1,  2,  3],\\n#        [ 4,  5,  6],\\n#        [ 7,  8,  9],\\n#        [10, 11, 12]])\\nb.shape\\n# displays the following result:\\n# (4, 3)\\n\\nc = np.matmul(a, b)\\nc\\n# displays the following result:\\n# array([[ 70,  80,  90],\\n#        [158, 184, 210]])\\nc.shape\\n# displays the following result:\\n# (2, 3)\\nIf your matrices have incompatible shapes, you'll get an error, like the following:\\nnp.matmul(b, a)\\n# displays the following error:\\n# ValueError: shapes (4,3) and (2,4) not aligned: 3 (dim 1) != 2 (dim 0)\\nNumPy's dot function\\nYou may sometimes see NumPy's dot function in places where you would expect a matmul. It turns out that the results of dot and matmul are the same if the matrices are two dimensional.\\nSo these two results are equivalent:\\na = np.array([[1,2],[3,4]])\\na\\n# displays the following result:\\n# array([[1, 2],\\n#        [3, 4]])\\n\\nnp.dot(a,a)\\n# displays the following result:\\n# array([[ 7, 10],\\n#        [15, 22]])\\n\\na.dot(a)  # you can call `dot` directly on the `ndarray`\\n# displays the following result:\\n# array([[ 7, 10],\\n#        [15, 22]])\\n\\nnp.matmul(a,a)\\n# array([[ 7, 10],\\n#        [15, 22]])\\nWhile these functions return the same results for two dimensional data, you should be careful about which you choose when working with other data shapes. You can read more about the differences, and find links to other NumPy functions, in the matmul and dot documentation.\",\n",
       " \"now that you know about matrix shapes and how they affect the math you can do there's something important you need to know about the matrix transpose the transpose of a matrix is a matrix with the same values as the original but it has the rows and columns switched so the transpose of the square matrix is this matrix you can see how the values in Row one of the original matrix one two three are now the values of column one in the transpose and the same thing happened for the second and third rows in the original becoming a second and third columns in the transpose it works for non square matrices to the transpose of this is this again you can see how the rows of the original become the columns in the transpose we really are just swapping the rows and columns but there are two important features of a transpose first if the original matrix wasn't a square the transpose has a new shape with the original dimension swapped so the transpose of a four by two matrix is a two by four matrix this can be helpful at times for example if you need to perform a matrix multiplication but the shapes of your matrices are incompatible the second important thing is that the data and the transpose isn't arranged the way it was in the original matrix if the original matrix represents rows of data the transpose will represent columns and if the original is arranged as columns the transpose will be arranged as rose just in case that sounds too abstract here's an example of what I mean when I talk about data arranged as rows or columns imagine we had a matrix that stored the heights weights and ages of three people we could choose to store each person's data has a single row or a single column if we choose rose then each column ends up representing a specific feature for all the people like height weight or age however if we storage prisons data has a column then each row contains all the values for a specific feature but now imagine we look at the transpose what used to be rows are now columns and vice versa I've mentioned this multiple times now but if you're multiplying two matrices you're dealing with the rose and the left matrix and the columns and the right this is why this discussion is so important if you have a math problem working with all the features for one person it's probably different than working with a single feature for all the people and most likely only one of those was actually what you are looking for this is going come up when you build your neural networks now writing the times when some matrix operation won't work because the shapes don't line up correctly but then you'll notice it would work if you throw in a transpose like maybe you're trying to multiply these 3 by 2 and 4 by 2 matrices in the matrix multiplication lessons you learn that won't work because their dimensions are incompatible remember these two inside numbers need to be the same but here we have a2 and a4 but if you were to take the transpose of the four by two matrix you'd have a two by four matrix and then you could multiply the 3 by 2 and 2 by 4 matrices or let's go back and consider another option you could take the transpose of the three by two matrix to get a 2 by 3 matrix then swap their orders and multiply the 4 by 2 and 2 by 3 matrices mathematically both of those solutions work but the question is when you're in that situation should you actually do either of those things and the answer is sometimes we just need to know when let's go back to trying to multiply these two matrices a three by two and a four by two we've already shown we can make this multiplication work two different ways using transposes but let's see when it is or isn't a good idea if both of these matrices contain data arranged in rows then either solution will be fine using the transpose of the second Matrix gives us rows times columns and that's exactly what we want from matrix multiplication the answer will be this three by four matrix the second option where we take the transpose of the first matrix and then swap their orders also gives us rows times columns this time the answer will be this 4 by 3 matrix so that's great if both of the original matrices store the data has rows it's safe to use a transpose by the way you may not have noticed but the answers to each of those two multiplications are actually transposes of each other so doing this either way gives you the same numbers just with different shapes that means you can choose either option based on the dimensions you want in your result okay that worked but what if the original matrices have their data arranged as columns in that case neither option works let's see why not if we use the transpose of the second Matrix we end up combining the wrong elements with our dot products see if we look at this row in this column we end up multiplying the one and the which are each from the first data items in the original matrix but then we'd add that to the product of the two and the three which are each values in the second data items so each dot product you take while multiplying these two matrices ends up combining values from every data item rather than combining elements of related items like we probably intended this is like what I described earlier where maybe now we're looking at the heights of all the people instead of looking at all the data for just one person okay so taking the transpose of the four by two matrix won't give us the answer we expect but what about the second option here we use the transpose of the first matrix and then swap them but it gives us the same problem data arrange has columns on the left and as rows on the right how about if the original three by two matrix had been arranged as rows and the four by two matrix has been columns that actually makes sense since we were trying to do this multiplication in the first place unfortunately neither solution works here either the first gives us rows times rose and the second gives us columns times columns what about the final possibility where the original three by two matrix have been arranged as columns and the four by two matrix had been rose that gives us columns times rows or columns times columns either way it's not giving you the answer you were looking for so it looks like the only time you can safely use a transpose in a matrix multiplication is it's the data in both of your original matrices is arranged as rose now I'm not actually trying to say you always need to store data has rows sometimes it'll make more sense as columns but just remember if you run into a situation where it looks like using a transpose will help make some math work and you will definitely run into the situation then you need to stop and really think about what's in each of your matrices make sure you know which rows and columns need to interact with each other and then only use transposes if they don't mess any of that up.\",\n",
       " \"Transpose\\nGetting the transpose of a matrix is really easy in NumPy. Simply access its T attribute. There is also a transpose() function which returns the same thing, but you’ll rarely see that used anywhere because typing T is so much easier. :)\\nFor example:\\nm = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\\nm\\n# displays the following result:\\n# array([[ 1,  2,  3,  4],\\n#        [ 5,  6,  7,  8],\\n#        [ 9, 10, 11, 12]])\\n\\nm.T\\n# displays the following result:\\n# array([[ 1,  5,  9],\\n#        [ 2,  6, 10],\\n#        [ 3,  7, 11],\\n#        [ 4,  8, 12]])\\nNumPy does this without actually moving any data in memory - it simply changes the way it indexes the original matrix - so it’s quite efficient.\\nHowever, that also means you need to be careful with how you modify objects, because they are sharing the same data. For example, with the same matrix m from above, let's make a new variable m_t that stores m's transpose. Then look what happens if we modify a value in m_t:\\nm_t = m.T\\nm_t[3][1] = 200\\nm_t\\n# displays the following result:\\n# array([[ 1,   5, 9],\\n#        [ 2,   6, 10],\\n#        [ 3,   7, 11],\\n#        [ 4, 200, 12]])\\n\\nm\\n# displays the following result:\\n# array([[ 1,  2,  3,   4],\\n#        [ 5,  6,  7, 200],\\n#        [ 9, 10, 11,  12]])\\nNotice how it modified both the transpose and the original matrix, too! That's because they are sharing the same copy of data. So remember to consider the transpose just as a different view of your matrix, rather than a different matrix entirely.\\nA real use case\\nI don't want to get into too many details about neural networks because you haven't covered them yet, but there is one place you will almost certainly end up using a transpose, or at least thinking about it.\\nLet's say you have the following two matrices, called inputs and weights,\\ninputs = np.array([[-0.27,  0.45,  0.64, 0.31]])\\ninputs\\n# displays the following result:\\n# array([[-0.27,  0.45,  0.64,  0.31]])\\n\\ninputs.shape\\n# displays the following result:\\n# (1, 4)\\n\\nweights = np.array([[0.02, 0.001, -0.03, 0.036], \\\\\\n    [0.04, -0.003, 0.025, 0.009], [0.012, -0.045, 0.28, -0.067]])\\n\\nweights\\n# displays the following result:\\n# array([[ 0.02 ,  0.001, -0.03 ,  0.036],\\n#        [ 0.04 , -0.003,  0.025,  0.009],\\n#        [ 0.012, -0.045,  0.28 , -0.067]])\\n\\nweights.shape\\n# displays the following result:\\n# (3, 4)\\nI won't go into what they're for because you'll learn about them later, but you're going to end up wanting to find the matrix product of these two matrices.\\nIf you try it like they are now, you get an error:\\nnp.matmul(inputs, weights)\\n# displays the following error:\\n# ValueError: shapes (1,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)\\nIf you did the matrix multiplication lesson, then you've seen this error before. It's complaining of incompatible shapes because the number of columns in the left matrix, 4, does not equal the number of rows in the right matrix, 3.\\nSo that doesn't work, but notice if you take the transpose of the weights matrix, it will:\\nnp.matmul(inputs, weights.T)\\n# displays the following result:\\n# array([[-0.01299,  0.00664,  0.13494]])\\nIt also works if you take the transpose of inputs instead and swap their order, like we showed in the video:\\nnp.matmul(weights, inputs.T)\\n# displays the following result:\\n# array([[-0.01299],# \\n#        [ 0.00664],\\n#        [ 0.13494]])\\nThe two answers are transposes of each other, so which multiplication you use really just depends on the shape you want for the output.\",\n",
       " \"NumPy Exam\\nThis is just a short programming quiz that asks you use a few NumPy features. It is meant to give you a little practice if you don't have NumPy experience. \\u2028\\u2028# Use the numpy library\\nimport numpy as np\\n\\n\\n######################################################\\n#\\n#      MESSAGE TO STUDENTS:\\n#\\n#  This file contains a solution to the coding quiz. Feel free\\n#  to look at it when you are stuck, but try to solve the\\n#   problem on your own first.\\n#\\n######################################################\\n\\n\\ndef prepare_inputs(inputs):\\n    # TODO: create a 2-dimensional ndarray from the given 1-dimensional list;\\n    #       assign it to new_inputs\\n    input_array = np.array([inputs])\\n    \\n    # TODO: find the minimum value in input_array and subtract that\\n    #       value from all the elements of input_array. Store the\\n    #       result in inputs_minus_min\\n    # We can use NumPy's min function and element-wise division\\n    inputs_minus_min = input_array - np.min(input_array)\\n\\n    # TODO: find the maximum value in inputs_minus_min and divide\\n    #       all of the values in inputs_minus_min by the maximum value.\\n    #       Store the results in inputs_div_max.\\n    # We can use NumPy's max function and element-wise division\\n    inputs_div_max = inputs_minus_min / np.max(inputs_minus_min)\\n\\n    return input_array, inputs_minus_min, inputs_div_max\\n    \\n\\ndef multiply_inputs(m1, m2):\\n    # Check the shapes of the matrices m1 and m2. \\n    # m1 and m2 will be ndarray objects.\\n    #\\n    # Return False if the shapes cannot be used for matrix\\n    # multiplication. You may not use a transpose\\n    if m1.shape[0] != m2.shape[1] and m1.shape[1] != m2.shape[0]:     \\n        return False\\n\\n    # Have not returned False, so calculate the matrix product\\n    # of m1 and m2 and return it. Do not use a transpose,\\n    #       but you swap their order if necessary\\n    if m1.shape[1] == m2.shape[0]:\\n        return np.matmul(m1, m2)        \\n    else:\\n        return np.matmul(m2, m1)        \\n\\n\\ndef find_mean(values):\\n    # Return the average of the values in the given Python list\\n    # NumPy has a lot of helpful methods like this.\\n    return np.mean(values)\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in inputs if c not in punctuation])\n",
    "\n",
    "all_text = ' '.join(inputs)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39974, 218877, 46)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(all_text), len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Recurrent neural networks are able to learn from sequences of data. In this lesson, you'll learn the concepts behind recurrent networks and see how a character-wise recurrent network is implemented in TensorFlow. One of the coolest deep learning results from last year was the Google Translate update. They've been a leader in machine learning for a while, but implementing a deep neural network for translation brought the service nearly to the level of human translators. With translation, the correct word to use depends on the context, and all the other words in the sentence, and even in the paragraph. Much of the information contained in language is in the sequence of the words. So far, we've been working with what are called feed forward networks. The input is fed into the network and it propagates forward through the hidden layers to the output layer. In feed forward networks, there is no sense of order in the inputs. Here's a simple idea then, let's build order into our network. First, we'll split up the data into parts. The text, this can be words or individual characters like I've done here with the word steep. Going forward, I'm going to borrow an example from Andrej Karpathy because it's great. Our goal here is to predict the next character in the word steep. To keep it simple, assume our entire alphabet consist of S, T, E, and P. Let's start with the normal feed forward network. We'll pass in the character S and our desired output is T. We pass in a T, we should get out E. Now, we pass in that E. In this sentence E is followed by another E or a P. The network, as shown here, doesn't have enough information to determine which character to predict. To solve this problem, we'll need to include information about the sequence of characters. We can do this by routing the hidden layer output from the previous step back into the hidden layer. That box there means value from the previous sequence, or time step. Now, when the network sees an E, it knows it saw an S and\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recurrent',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'data.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'lesson,',\n",
       " \"you'll\",\n",
       " 'learn',\n",
       " 'the',\n",
       " 'concepts',\n",
       " 'behind',\n",
       " 'recurrent',\n",
       " 'networks',\n",
       " 'and',\n",
       " 'see',\n",
       " 'how',\n",
       " 'a',\n",
       " 'character-wise',\n",
       " 'recurrent',\n",
       " 'network',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'TensorFlow.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coolest',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'results',\n",
       " 'from',\n",
       " 'last',\n",
       " 'year',\n",
       " 'was',\n",
       " 'the',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'update.',\n",
       " \"They've\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'leader',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while,',\n",
       " 'but',\n",
       " 'implementing',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'for',\n",
       " 'translation',\n",
       " 'brought',\n",
       " 'the',\n",
       " 'service',\n",
       " 'nearly',\n",
       " 'to',\n",
       " 'the',\n",
       " 'level',\n",
       " 'of',\n",
       " 'human',\n",
       " 'translators.',\n",
       " 'With',\n",
       " 'translation,',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'word',\n",
       " 'to',\n",
       " 'use',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'the',\n",
       " 'context,',\n",
       " 'and',\n",
       " 'all',\n",
       " 'the',\n",
       " 'other',\n",
       " 'words',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sentence,',\n",
       " 'and',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the',\n",
       " 'paragraph.',\n",
       " 'Much']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "inputs_ints = []\n",
    "for each in inputs:\n",
    "    inputs_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_array = [i for i, label in enumerate(labels)]\n",
    "labels_np = np.array(labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, create an array features that contains the data we'll pass to the network. The data should come from review_ints, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128]. For reviews longer than 200, use on the first 200 words as the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out that inputs with 0 length\n",
    "inputs_ints = [each for each in inputs_ints if len(each) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(inputs), seq_len), dtype=int)\n",
    "for i, row in enumerate(inputs_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1031,   10, 1484],\n",
       "       [ 561,    3,    1, ...,    3, 2349, 1611],\n",
       "       [1178,   12,  564, ...,    3,  518,   26],\n",
       "       ..., \n",
       "       [  84,   11,    7, ...,  341,   30,    7],\n",
       "       [4158, 3044,    1, ...,  890,  950,   43],\n",
       "       [ 217, 5199,   90, ...,    1,   99, 1459]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(36, 200) \n",
      "Validation set: \t(5, 200) \n",
      "Test set: \t\t(5, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac= 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
